# Sobreajuste, Reamostragem e Valida√ß√£o dos Resultados

Na √∫ltima aula, vimos que a transi√ß√£o da classifica√ß√£o tradicional com vari√°veis num√©ricas para a classifica√ß√£o de textos exige primeiro **converter a linguagem em n√∫meros**, e foi a√≠ que aprofundamos os modelos de linguagem *n-gram*. Discutimos como unigramas, bigramas e trigramas capturam diferentes n√≠veis de contexto e por que a suposi√ß√£o de Markov e a Regra da Cadeia permitem estimar probabilidades de sequ√™ncias mesmo em corpus limitados. Essa base probabil√≠stica ficou clara quando mostramos que **classificadores como a regress√£o log√≠stica**, apresentados esta semana, continuam seguindo o mesmo princ√≠pio: atribuir pesos √†s features‚Äîneste caso, contagens ou frequ√™ncias de *n-grams*‚Äîpara modelar $Pr(Y\! =\! 1\mid X)$ por meio da fun√ß√£o sigmoide, ou do softmax quando temos mais de duas classes. Finalmente, amarramos essas ideias ao processo de treinamento: definimos a perda em cross-entropy, vimos como o gradiente descende essa fun√ß√£o e por que hiperpar√¢metros como a learning rate influenciam a converg√™ncia do modelo. Em suma, o encontro uniu teoria de modelos de linguagem e pr√°tica de classifica√ß√£o supervisionada, mostrando que **entender *n-grams* √© o primeiro passo para aplicar algoritmos como regress√£o log√≠stica a tarefas de sentimento, spam ou t√≥picos**. 

Na aula de hoje, iremos discutir um problema que n√£o √© √∫nico ao texto em Aprendizado de M√°quina: o Sobreajuste (ou ***overfitting***), e como identific√°-lo e evit√°-lo. O problema decorre do fato de que muitos m√©todos estat√≠sticos/de Aprendizado de M√°quina s√£o facilmente adapt√°veis e capazes de modelar rela√ß√µes complexa (Mesmo antes da difus√£o do *Deep Learning*!). No entanto, podem acabar enfatizando demais caracter√≠sticas dos dados de treinamento que n√£o est√£o no teste (ou no mundo), o que vai impactar sua capacidade de generaliza√ß√£o para outros casos[^1]. Por isso, precisamos garantir que nossos modelos s√£o confi√°veis e podem ser usados para previs√£o. Sem essa confian√ßa, os modelos s√£o in√∫teis.


## Sobreajuste (*Overfitting*)

Existem t√©cnicas de aprendizado estat√≠stico que s√£o capazes de "aprender" t√£o bem as caracter√≠sticas de um banco de treinamento que podem fazer previs√µes corretas para todas as observa√ß√µes. Esse tipo de modelo √© conhecido como um modelo sobreajustado, ou *over-fit*, e provavelmente n√£o vai se sair muito bem em novas amostras. Vamos imaginar uma tarefa de classifica√ß√£o bem simples em que h√° somente dois preditores: A e B, e um modelo deve classific√°-la entre a classe azul e a classe vermelha (simula√ß√£o).

```{figure} ../aula8/images/kkfig.4.2.png
---
width: 100%
name: figkkclass
align: center
---
Exemplo de um banco de treinamento com duas classes e preditores. Os pain√©is mostram dois modelos de classifica√ß√£o e seus limites de classifica√ß√£o. Fonte Kuhn and Kjell (p.63, {cite}`kuhn2018applied`.)
```

Como mostra A {numref}`Figura {number} <figkkclass>`, o modelo 1 seria um modelo sobreajustado, pois tenta definir os limites para a classifica√ß√£o muito pr√≥ximos tamb√©m aos casos at√≠picos. Isto √©, os casos azuis que est√£o para valores dos preditores maiores do que 0.5 (ambos preditores). Com esse sobreajuste, com certeza o modelo apresentaria √≥timos resultados no banco de treinamento, mas performaria muito mal em novas observa√ß√µes, justamente por tentar se aproximar muito de como as observa√ß√µes se comportaram no treinamento. O segundo modelo, apesar de admitir mais erro durante o treinamento, provavelmente apresentaria melhores resultados em observa√ß√µes n√£o vistas e, portanto, melhor capacidade de generaliza√ß√£o. Esse √© o ponto principal do problema: Queremos conseguir identificar quando nosso modelo est√° sobreajustado, e qual modelo ter√° melhores resultados em novas observa√ß√µes. N√≥s vimos isso acontecer anteriormente na quinta aula, quando avaliamos o resultado de treinamento e teste de alguns modelos com base na sua flexibilidade, na {numref}`Figura {number} <flexteste>`:

```{figure} ../aula5/images/fig2.9.png
---
width: 100%
name: flexteste
align: center
---
√Ä esquerda: dados simulados a partir de f, mostrados em preto. Tr√™s estimativas de f s√£o exibidas: a linha de regress√£o linear (curva laranja) e dois ajustes por splines de suaviza√ß√£o (curvas azul e verde).

√Ä direita: Erro Quadr√°tico M√©dio de treinamento (curva cinza), EQM de teste (curva vermelha) e EQM m√≠nimo poss√≠vel de teste entre todos os m√©todos (linha tracejada). Os quadrados representam os EQMs de treinamento e de teste para os tr√™s ajustes mostrados no painel da esquerda. Fonte: James et al. ({cite}`james2023introduction`., p. 29).
```

Nesse caso, temos um modelo sobreajustado, que √© representado na linha verde: Ele segue bem de perto a varia√ß√£o estoc√°stica de cada observa√ß√£o, e consegue √≥timos resultados no treinamento (quadrado verde, linha cinza). No entanto, vemos que seu resultado √© ruim para o banco de teste (linha vermelha), s√≥ n√£o sendo pior do que a regress√£o linear, que nesse caso foi um modelo subajustado (*under-fit*). Nas nossas aplica√ß√µes, sempre almejamos o modelo azul (corretamente ajustado), e, apesar de n√£o sabermos o verdadeiro erro m√≠nimo poss√≠vel, podemos comparar diversos modelos e suas estimativas com m√©todos de reamostragem, escolhendo o melhor modelo dentre os testados. 


Vamos tentar imaginar como isso ocorreria com texto em um exemplo simples. Considere um mini‚Äêconjunto de tweets sobre um filme. Temos o seguinte banco de Treino fict√≠tico para uma classifica√ß√£o de sentimento (6 frases, r√≥tulos entre par√™nteses):

1. ‚ÄúAmei muito este filme!‚Äù (+)
2. ‚ÄúFilme incr√≠vel, recomendo!‚Äù (+)
3. ‚ÄúObra-prima total.‚Äù (+)
4. ‚ÄúOdiei esse filme demais.‚Äù (‚àí)
5. ‚ÄúP√©ssimo enredo, detestei.‚Äù (‚àí)
6. ‚ÄúTerr√≠vel do come√ßo ao fim.‚Äù (‚àí)

Para transformar texto em n√∫meros aplicamos um modelo *bag-of-words* de **trigramas**: cada sequ√™ncia de tr√™s palavras vira uma feature, gerando dezenas de colunas muito espec√≠ficas (‚Äúamei muito este‚Äù, ‚Äúesse filme demais‚Äù etc.). Treinamos uma **regress√£o log√≠stica** sem regulariza√ß√£o; como h√° exatamente um trigrama exclusivo para cada frase, o algoritmo pode facilmente encontrar pesos que se encaixam perfeitamente e atingir 100% de acur√°cia no conjunto de treino. No entanto, imaginemos o seguinte banco de teste:

Teste (2 frases nunca vistas):

7. ‚ÄúAdorei esse filme.‚Äù (+)
8. ‚ÄúHorr√≠vel, n√£o gostei.‚Äù (‚àí)

Nenhum trigama do teste aparece no treino, logo o modelo poder√° atribuir probabilidades aleat√≥rias e classificar ambas como negativas ou positivas, resultando em apenas 50% de acerto (ou errar ambas). O sistema **memoriza ru√≠do em vez de aprender padr√µes gerais**, o que √© uma evid√™ncia de sobreajuste. Poder√≠amos trocar para unigramas ou aplicar regulariza√ß√£o para reduzir a complexidade e recuperar a capacidade de generaliza√ß√£o. Mas precisamos saber identificar quando estamos com sobreajuste.


## Reamostragem e Valida√ß√£o Cruzada

```{video} https://www.youtube.com/embed/fSytzGwwBVw?si=1si6a5JK_COrLkHa
```

---

Justamente para que sejamos capazes de identificar quando estamos na situa√ß√£o de sobreajuste que surgiram os m√©todos de Reamostragem. **M√©todos de Reamostragem**, ou *Resampling*, s√£o ferramentas indispens√°veis na estat√≠stica e no aprendizado de m√°quina, e consistem em sortear amostras repetidas de um determinado conjunto de dados de treinamento, reajustando o modelo em cada nova amostra e obtendo informa√ß√µes adicionais sobre seu ajuste e incerteza. 

M√©todos de reamostragem podem ser utilizados para estimar o erro de um modelo ou para selecionar o n√≠vel de flexibilidade (como vimos na {numref}`Figura {number} <flexteste>`). O processo de avaliar a performance de um modelo √© conhecido como **Valida√ß√£o do Modelo**, e o processo de escolher hiperpar√¢metros e/ou flexibilidade do modelo √© conhecido como **Sele√ß√£o do modelo** (ou *hyperparameter tuning*).

Durante o curso, discutimos muito o valor de erro do modelo no banco de teste. o Erro de Teste √© a m√©dia dos erros que resultam das previs√µes do modelo em uma nova observa√ß√£o, que n√£o foi vista durante o treinamento. No entanto, a distin√ß√£o entre banco de treino e teste pode ser um pouco ilus√≥ria: Na verdade, n√£o temos um banco anotado separado s√≥ para teste, mas um banco de treinamento que foi repartido para treinamento e teste. Na aus√™ncia do banco de teste ideal, usamos m√©todos de valida√ß√£o para verificar a capacidade de generaliza√ß√£o de um modelo.

## Abordagem do Conjunto de Valida√ß√£o/Teste


```{video} https://www.youtube.com/embed/ngrOYWgJjb4?si=V1M2QpZVbXRwqcyl
```


---

Ao longo do curso, j√° usamos um m√©todo de valida√ß√£o. Dividimos um banco de treinamento em duas partes: treino e teste. O banco de teste tamb√©m pode ser chamado de banco de valida√ß√£o. Com esse m√©todo, treinamos o modelo no banco de treinamento e avaliamos sua performance no banco de teste. A {numref}`Figura {number} <treinoteste>` ilustra a divis√£o neste m√©todo.


```{figure} ../aula8/images/islfig5.1.png
---
width: 100%
name: treinoteste
align: center
---
Divis√£o entre o banco de treino (azul) e teste/valida√ß√£o (bege). Fonte: James et al. ({cite}`james2023introduction`., p. 203)
```

Uma das principais desvantagens desse m√©todo sozinho √© que, ao repetirmos o processo de separa√ß√£o entre treino e teste, obteremos resultados bem diferentes. A figura {numref}`Figura {number} <testevar>` mostra a vari√¢ncia das estimativas de erro obtida com diferentes divis√µes entre treino e teste. Fica claro que a aleatoriedade contida nessa divis√£o pode afetar bastante nossa estimativa: n√£o temos como saber quando estaremos na linha amarela (menor MSE) ou na linha laranja (maior MSE). Portanto, precisaremos de outros m√©todos para termos mais confian√ßa em nossos modelos.


```{figure} ../aula8/images/islfig.5.2.png
---
width: 100%
name: testevar
align: center
---
Uma √∫nica divis√£o entre treino e teste (Esquerda) e m√∫ltiplos resultados em v√°rias divis√µes (Direita). Fonte: James et al. ({cite}`james2023introduction`., p. 204)
```

Em resumo, o m√©todo de valida√ß√£o s√≥ bom com o banco de teste tem duas desvantagens principais: 1) A estimativa de erro ser√° √∫nica, e pode variar bastante dependendo de quais observa√ß√µes entraram no banco de teste. 2) O modelo treinar√° com menos observa√ß√µes (80 ou 90%), gerando resultados que podem supersestimar o erro de teste.


## Abordagem *Leave-one-out* (LOOCV)

O m√©todo "Deixe um de fora" (***Leave-one-out***), ou "Exclua um", consiste em dividir o banco tamb√©m em duas partes. No entanto, ao inv√©s de deixar 10%, ou 20%, para teste, uma √∫nica observa√ß√£o ($x_1,y_1$) √© usada como valida√ß√£o, e o resto √© usado para o treinamento. O m√©todo √© ent√£o repetido com $(x_2,y_2), (x_3,y_3) ..., (x_n,y_n)$ de fora. Ou seja, repetimos o processo para cada observa√ß√£o no banco de dados, cada vez deixando uma √∫nica observa√ß√£o de fora. Com isso, teremos *n* estimativas de erro. No caso da regress√£o, obteremos *n* MSEs.

$$CV_{(n)} = \dfrac{1}{n} \sum_{i=1}^{n} \text{MSE}_{i}$$


A {numref}`Figura {number} <LOOCV>` ilustra como funciona esse processo.

```{figure} ../aula8/images/islfig.5.3.png
---
width: 100%
name: LOOCV
align: center
---
Ilutra√ß√£o do m√©todo *Leave-one-out* (LOOCV). Fonte: James et al. ({cite}`james2023introduction`., p. 205)
```

As vantagens desse m√©todo com rela√ß√£o ao primeiro s√£o as seguintes: 1) ele tem menos vi√©s, gerando menor superestima√ß√£o do erro no teste. 2) N√£o √© aleat√≥rio, tendo resultados est√°veis, j√° que sempre seguir√° a mesma ordem dentro do banco de dados. No entanto, possui uma desvantagem clara: O seu gasto computacional e de tempo √© muito alto. Para um banco de *N* observa√ß√µes, teremos que treinar o modelo *N* vezes, e isso √© impratic√°vel com bancos grande e com modelos que j√° exigem mais, como √© o caso dos modelos de Aprendizado Profundo. Portanto, na pr√°tica, se usa um m√©todo que √© um caso especial do *LOOCV*: o ***K-fold Cross-validation***.

## *K-fold Cross-Validation*



```{video} https://www.youtube.com/embed/rSGzUy13F_0?si=LYRw0bYA7rfJdio8
```

---

Uma alternativa menos computacionalmente exigente ao *LOOCV* √© o ***K-fold***. Nessa estrat√©gia, definimos em quantas reparti√ß√µes, ou "dobras" (da√≠ o *fold*), divid√≠remos o banco de treino. Com base nessas $k$ dobras, iniciamos o treinamento com o primeiro *fold* sendo usado para valida√ß√£o e o resto para treino, repetindo isso $k$ vezes. Obtendo, por tanto, $k$ m√©tricas de erro. Nossa f√≥rmula para o MSE do *LOOCV* vira, ent√£o:

$$CV_{(k)} = \dfrac{1}{k} \sum_{i=1}^{k} \text{MSE}_{i}$$

A {numref}`Figura {number} <KFOLD>` ilustra como funciona esse processo.


```{figure} ../aula8/images/islfig.5.5.png
---
width: 100%
name: KFOLD
align: center
---
Ilutra√ß√£o do m√©todo *K-fold*. Fonte: James et al. ({cite}`james2023introduction`., p. 207)
```

N√£o s√≥ o m√©todo *k-fold* √© menos custoso computacionalmente, ele tamb√©m possui vantagens de vi√©s com rela√ß√£o ao *LOOCV*: NA valida√ß√£o cruzada, ‚Äúvi√©s‚Äù √© o erro introduzido porque cada modelo √© treinado com um subconjunto dos dados, n√£o com o conjunto inteiro.

No LOOCV (Leave-One-Out) cada split treina com n ‚Äì 1 observa√ß√µes, praticamente todo o conjunto. Isso torna a estimativa de erro muito pouco enviesada, mas cada fold avalia o modelo num √∫nico ponto, o que gera alta vari√¢ncia.

No k-fold com k pequeno (por exemplo k = 5 ou 10) cada modelo treina com apenas (k ‚Äì 1)/k do conjunto; como perde um pouco mais de dados para teste, o erro m√©dio tende a ser levemente mais alto (maior vi√©s) do que no LOOCV.

A ‚Äúvantagem‚Äù √© justamente esse pequeno aumento de vi√©s: ele suaviza a avalia√ß√£o e, combinado com o uso repetido de partes maiores do conjunto de teste, reduz drasticamente a vari√¢ncia da estimativa. Na pr√°tica, esse equil√≠brio entre ‚Äúum pouco mais de vi√©s‚Äù e ‚Äúmuito menos vari√¢ncia‚Äù faz com que o k-fold produza uma previs√£o de desempenho mais est√°vel e geralmente mais pr√≥xima do erro real em dados novos. 


Na {numref}`Figura {number} <KFOLDvsLOOCV>` est√£o as estimativas de valida√ß√£o cruzada e as taxas reais de erro de teste obtidas ao aplicar splines de suaviza√ß√£o aos conjuntos de dados simulados mostrados nas Figuras 2.9‚Äì2.11 do Cap√≠tulo 2. O MSE de teste verdadeiro √© exibido em azul. As linhas preta tracejada e laranja cont√≠nua representam, respectivamente, as estimativas do LOOCV e da valida√ß√£o cruzada com 10 dobras. Nos tr√™s gr√°ficos, as duas estimativas de valida√ß√£o cruzada s√£o muito semelhantes. No painel da direita, o MSE de teste verdadeiro e as curvas de valida√ß√£o cruzada s√£o quase id√™nticos. No painel central, os dois conjuntos de curvas coincidem nos menores graus de flexibilidade, mas as curvas de valida√ß√£o cruzada superestimam o MSE do conjunto de teste para graus mais altos de flexibilidade. No painel da esquerda, as curvas de valida√ß√£o cruzada apresentam o formato geral correto, por√©m subestimam o MSE de teste verdadeiro.

```{figure} ../aula8/images/islfig5.6.png
---
width: 100%
name: KFOLDvsLOOCV
align: center
---
Erro quadr√°tico m√©dio (MSE) de teste verdadeiro e estimado para os conjuntos de dados simulados nas Figuras 2.9 (esquerda), 2.10 (centro) e 2.11 (direita). O MSE de teste verdadeiro √© mostrado em azul, a estimativa do LOOCV aparece como uma linha tracejada preta e a estimativa da valida√ß√£o cruzada com 10 dobras em laranja. As cruzes indicam o m√≠nimo de cada uma das curvas de MSE. Fonte: James et al. ({cite}`james2023introduction`., p. 209)

```



```{admonition} üí¨ Com a palavra, os autores:
:class: quote
"Ao realizar valida√ß√£o cruzada k-fold ‚Äî por exemplo, com k = 5 ou k = 10 ‚Äî obt√©m-se um n√≠vel intermedi√°rio de vi√©s, porque cada conjunto de treinamento cont√©m aproximadamente (k ‚àí 1)n/k observa√ß√µes: menos do que no LOOCV, mas muito mais do que na estrat√©gia de conjunto de valida√ß√£o √∫nico. Portanto, sob a √≥tica de redu√ß√£o de vi√©s, o LOOCV √© prefer√≠vel ao k-fold. Contudo, vi√©s n√£o √© a √∫nica preocupa√ß√£o; tamb√©m importa a vari√¢ncia do procedimento. O LOOCV apresenta vari√¢ncia mais alta do que o k-fold com k < n. Por qu√™? No LOOCV, calculamos a m√©dia das sa√≠das de n modelos ajustados sobre conjuntos quase id√™nticos, gerando resultados altamente correlacionados. J√° no k-fold com k < n, fazemos a m√©dia das sa√≠das de k modelos treinados em conjuntos que se sobrep√µem menos, resultando em correla√ß√µes menores. Como a m√©dia de vari√°veis muito correlacionadas tem vari√¢ncia maior que a m√©dia de vari√°veis menos correlacionadas, a estimativa do erro de teste via LOOCV tende a exibir vari√¢ncia mais alta que a obtida pelo k-fold. Em suma, h√° um trade-off vi√©s-vari√¢ncia na escolha de k: valores como k = 5 ou k = 10 s√£o usados com frequ√™ncia, pois fornecem estimativas do erro de teste que n√£o sofrem nem de vi√©s excessivo nem de vari√¢ncia muito alta."
({cite}`james2023introduction`., p. 209, tradu√ß√£o nossa)
```

## Conclus√£o




## Notas

[^1]: A capacidade de generaliza√ß√£o √© a habilidade de um modelo de aprendizado de m√°quina manter alta precis√£o quando recebe dados que nunca fizeram parte do treinamento, porque aprendeu os padr√µes subjacentes em vez de memorizar casos espec√≠ficos. Quando o modelo √© complexo demais para a quantidade ou a qualidade dos dados, ele tende ao *overfitting*. Ou seja, acerta o treinamento, mas falha nos exemplos novos; E se for simples demais, ocorre *underfitting*, pois n√£o captura nem mesmo os padr√µes b√°sicos.