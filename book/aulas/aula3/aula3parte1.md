
# *Bag-of-Words* e o modelo Multinomial

## Cap√≠tulo 5 - *Bag-of-words*

Na √∫ltima aula, vimos o que √© um c√≥rpus e os princ√≠pios que o pesquisador deve seguir na constru√ß√£o ou utiliza√ß√£o de um c√≥rpus para tarefas de aprendizado supervisionado. Depois de construir um c√≥rpus de documentos, o pesquisador deve decidir como ir√° representar o texto de forma num√©rica. Uma das formas mais comuns, e utilizadas nos modelos mais cl√°ssicos de aprendizado estat√≠stico, √© a *bag-of-words*, ou saco de palavras. Essa representa√ß√£o ser√° a base dos modelos iniciais do curso que veremos a partir da sexta aula, como o modelo log√≠stico/multinomial. A ideia principal por tr√°s desse modelo √© a de representar cada documento pelo n√∫mero de vezes que cada palavra aparece nele. Considere as seguintes frases (ou documentos):

1. O cachorro ama o osso;
2. O Osso ama o cachorro;

Ignorando o artigo "O", ter√≠amos a seguinte matriz *Document-feature*, ou o seguinte BOW (*Bag-of-words*):


<div align="center">

| Documento | Cachorro | ama | Osso |
|-----------|------|-------|----------|
| Doc1      | 1    | 1     | 1        |
| Doc2      | 1    | 1     | 1        |

</div>

Essa matriz √© chamada assim por que cada linha (ou observa√ß√£o) cont√©m um documento/senten√ßa/frase e cada coluna cont√©m a palavra em considera√ß√£o. Por exemplo, a coluna "Cachorro" mostra quantas vezes o termo Cachorro aparece em cada documento. Neste caso, os dois documentos possuem as mesmas palavras, e seriam tratados como "iguais". Isso se deve ao fato de que, nesse tipo de representa√ß√£o, a ordem das palavras e o contexto n√£o s√£o considerados; por isso, embora as duas frases expressem rela√ß√µes diferentes, a Bag-of-Words as representa de forma id√™ntica, pois cont√™m as mesmas palavras com as mesmas contagens. Mais adiante no curso, ser√£o apresentadas outras representa√ß√µes que capturam contexto, como n-gramas, embeddings e modelos contextuais. No entanto, apesar da sua simplicidade e redu√ß√£o do contexto dos documentos, a BOW pode ser extraordinariemente eficiente a depender do seu objetivo e tarefa. Grimmer et al. estabelecem a seguinte "receita" para se trabalhar com a representa√ß√£o BOW:

1. Escolher a unidade de an√°lise;
2. Tokenizar o texto;
3. Reduzir complexidade;
4. Criar a matriz *document-feature*, ou documentos-termos.


### Escolher a unidade de an√°lise

Depois de garantir a digitaliza√ß√£o dos documentos, ou colet√°-los, o pesquisador deve decidir como vai dividi-los em unidades de an√°lise menores. No nosso exemplo, n√£o precisamos dividir os documentos em unidades menores pois eles j√° est√£o em formato reduzido, por serem *tweets* de at√© 280 caract√©res. No entanto, em muitos casos os pesquisadores estar√£o interessados em textos mais longos, como declara√ß√µes, discursos, manifestos, not√≠cias, artigos. Em outros casos, o objetivo pode ser o de juntar textos menores em documentos maiores (e.g. juntar os tweets de um dia de determinado usu√°rio). Independente da situa√ß√£o, o pesquisador deve estar consciente da decis√£o e escolher aquela que seja melhor para atender o seu objetivo de pesquisa. A unidade de an√°lise pode afetar a pergunta de pesquisa, a efici√™ncia dos modelos e a pr√≥pria coleta de dados. A unidade de an√°lise tamb√©m √© conhecida por *documento*, podendo ser o par√°grafo de uma not√≠cia, uma p√°gina inteira, ou apenas um *tweet*.



### "Tokenizar"

"Tokenizar", ou transformar em *tokens*, vem depois da escolha da unidade de an√°lise. Cada documento ser√° quebrado em suas partes individuais, as palavras. Esse √© o primeiro passo para a cria√ß√£o do saco de palavras (BOW). O exemplo mais comum √© o do tokenizar em n-grams de 1. A frase "Diga n√£o √† vacina√ß√£o obrigat√≥ria!" pode ser quebrada da seguinte forma:


<div align="center">

["Diga","n√£o","√†","vacina√ß√£o","obrigat√≥ria", "!"]

</div>


Seguindo a mesma frase e a mesma etapa de tokeniza√ß√£o em palavras, podemos formar bi-grams, que s√£o pares de palavras consecutivas no texto. Bi-grams ajudam a capturar um pouco de contexto local que o unigram (n=1) n√£o representa, como express√µes fixas, nega√ß√µes e rela√ß√µes imediatas entre termos.

<div align="center">

["Diga_n√£o", "n√£o_√†", "√†_vacina√ß√£o", "vacina√ß√£o_obrigat√≥ria", "obrigat√≥ria_!"]

</div>

Se o objetivo √© capturar resist√™ncia √† vacina√ß√£o, ou hesita√ß√£o vacinal, talvez quebrar o texto em bigramas seja mais informativo do que quebr√°-lo apenas em suas palavras individuais. Com isso, se d√° mais contexto ao modelo de aprendizado de m√°quina, ao custo de adicionar mais combina√ß√µes raras que podem n√£o aparecer tanto em seu banco de dados. üí¨ "Usar ordens maiores de n-gramas pode aumentar substancialmente o n√∫mero de tipos √∫nicos, mas pode ajudar nossa an√°lise textual ao reter mais informa√ß√µes" ({cite}`grimmer2022text`, p. 99, tradu√ß√£o nossa). Novamente, isso n√£o √© uma escolha trivial: Assim como todos os passos e princ√≠pios discutidos nas √∫ltimas aulas, a forma de processar e representar numericamente o texto altera substancialmente os resultados. O pesquisador deve tentar sempre estar consciente dessas escolhas e relat√°-las aos leitores quando necess√°rio. 


```{admonition} üí¨ Com a palavra, os autores:
:class: quote
"Alternativamente, pesquisadores podem achar √∫til manter uma lista de certos bigramas e trigramas que eles antecipam ser √∫teis para sua an√°lise. Por exemplo, Rule, Cointet e Bearman (2015) usam algumas express√µes de m√∫ltiplas palavras, como national security e local government, em sua an√°lise dos discursos SOTU. Vamos indicar isso visualmente colocando um sublinhado entre duas palavras que est√£o sendo tratadas como um n-grama. Quando a lista de n-gramas a ser extra√≠da √© pequena, isso resolve o problema do vocabul√°rio grande, mas exige uma lista de n-gramas."
({cite}`grimmer2022text`, p. 100, tradu√ß√£o nossa)
```

### Reduzir a complexidade

O que Grimmer et al. chamam de "Reduzir a complexidade" √© conhecido de forma mais corriqueira no jarg√£o de aprendizado de m√°quina como "pr√©-processamento" do texto. A principal fun√ß√£o do pr√©-processamento √© o de reduzir os custos computacionais ao diminuir o texto, eliminando palavras muito recorrentes (e, portanto, pouco informativas), reduzir elas √† raiz, etc.

#### 1 - Converter o texto em min√∫sculo

Converter o texto em min√∫sculo, especialmente em BOW, √© garantir que a mesma palavra em min√∫sculo e em mai√∫sculo sejam consideradas iguais. Por exemplo, nas frases:

1. "A decis√£o da Igreja";
2. "A igreja do bairro"

Temos a palavra igreja com duas defini√ß√µes bem diferentes: "Igreja" se referindo √† Igreja cat√≥lica como um todo, no n√≠vel da institui√ß√£o religiosa. E "igreja" se refere ao pr√©dio, um local f√≠sico. Na matriz de documentos, ter√≠amos algo mais ou menos assim (ignorando os artigos e preposi√ß√µes):


<div align="center">


| Documento | decis√£o | Igreja | igreja | bairro |
|-----------|------|-------|----------| --------- |
| Doc1      | 1    | 1     | 0        | 0         |
| Doc2      | 0    | 0     | 1        | 1         |

</div>


Note que surgem duas colunas distintas (‚ÄúIgreja‚Äù e ‚Äúigreja‚Äù) para a mesma forma lexical, inflando o vocabul√°rio. Aplicando *lowercasing* antes da vetoriza√ß√£o, consolidamos as formas:

<div align="center">

| Documento | decis√£o | igreja | bairro |
|-----------|------|-------|---------|
| Doc1      | 1    | 1     | 0         |
| Doc2      | 0    | 0     | 1         |

</div>

Ao inv√©s de quatro colunas, t√™m-se tr√™s. H√° casos em que manter a capitaliza√ß√£o √© √∫til (p.ex., NER, distin√ß√£o de nomes pr√≥prios como ‚ÄúPorto‚Äù vs. ‚Äúporto‚Äù), mas, em pipelines cl√°ssicos de BOW, a normaliza√ß√£o para min√∫sculas costuma ser preferida para reduzir ru√≠do e dimensionalidade. A decis√£o deve considerar o objetivo da tarefa: se a distin√ß√£o entre nomes pr√≥prios e comuns for relevante, pode ser melhor preservar a capitaliza√ß√£o ou criar uma feature espec√≠fica para ‚Äúcapitalizado‚Äù.


#### 2 - Remover pontua√ß√£o

Na maior parte dos casos, a pontua√ß√£o espec√≠fica (e.g. v√≠rgulas, pontos finais, acentos, etc.) n√£o vai adicionar informa√ß√£o relevante ao modelo. Voltando ao exemplo anterior:

<div align="center">

["Diga","n√£o","√†","vacina√ß√£o","obrigat√≥ria", "!"]

</div>

Podemos ter:

<div align="center">

["Diga","nao","a","vacina√ßao","obrigatoria"]

</div>

Esse processo reduz a quantidade de colunas e termos, facilitando o treinamento do modelo de aprendizado de m√°quina. Novamente, a decis√£o de manter certas formas, pontua√ß√µes e termos deve estar alinhada ao objetivo da tarefa.



#### 3 - Remover *stop words*

Podemos estar interessados tamb√©m em remover certas palavras que, de t√£o comuns, n√£o adicionam muita informa√ß√£o ao modelo. Exemplos de palavras "*stop words*" pode ser artigos (a, os, as, os), preposi√ß√µes (de,do,desde,em) ou outras palavras que o pesquisador, ao se defrontar com o texto, considere irrelevantes para a diferencia√ß√£o dos documentos. Com esse passo adicional, a frase acima se torna:

<div align="center">

["Diga","nao","vacina√ßao","obrigatoria"]

</div>

O pesquisador pode utilizar listas comuns de *stop words* em v√°rias l√≠nguas, ou criar sua pr√≥pria lista.


```{admonition} üí¨ Com a palavra, os autores:
:class: quote
"Remover stopwords frequentemente elimina uma fra√ß√£o extremamente alta dos tokens do c√≥rpus enquanto remove muito poucos tipos, o que ‚Äî se essas palavras carregam pouco ou nenhum significado ‚Äî pode resultar em enormes economias computacionais. Isso reflete o fato emp√≠rico de que, na maioria das l√≠nguas, poucas palavras s√£o extremamente comuns. A ideia √© que, embora esses tokens respondam por uma grande fra√ß√£o das palavras, eles representam apenas uma pequena fra√ß√£o do significado. Listas de stopwords em muitas l√≠nguas est√£o dispon√≠veis em pacotes de software que fornecem ferramentas de an√°lise de texto."
({cite}`grimmer2022text`, p. 102, tradu√ß√£o nossa)
```


#### 4 - Reduzir √† raiz

Um outro passo √© o de reduzir as palavras √†s suas ra√≠zes, diminuindo os termos √∫nicos nos documentos. ‚ÄúReduzir √† raiz‚Äù √© um passo de pr√©-processamento que transforma palavras em uma forma b√°sica para diminuir varia√ß√µes superficiais e, assim, reduzir a esparsidade do vocabul√°rio na matriz documento-termo. Na pr√°tica, usa-se:

- **Stemming**: corta sufixos com regras heur√≠sticas para obter um radical aproximado. √â r√°pido, mas pode gerar formas n√£o dicionarizadas (ex.: ‚Äúestudando‚Äù, ‚Äúestudaram‚Äù, ‚Äúestudar√£o‚Äù ‚Üí ‚Äúestud‚Äù).

- **Lematiza√ß√£o**: mapeia cada palavra para o seu lema (forma can√¥nica), considerando informa√ß√£o morfol√≥gica/gramatical. √â mais precisa, por√©m mais custosa (ex.: ‚Äúestudando‚Äù, ‚Äúestudaram‚Äù, ‚Äúestudar√£o‚Äù ‚Üí ‚Äúestudar‚Äù). 

Voltando ao exemplo da vacina√ß√£o obrigat√≥ria, ter√≠amos, com o *stemming*:

<div align="center">

["dig","nao","vacina","obrig"]

</div>

Com a lematiza√ß√£o, ficaria:

<div align="center">

["dizer","n√£o","vacina√ß√£o","obrigat√≥rio"]


</div>


#### 5 - Eliminar Palavras muito frequentes ou muito raras

Como fica claro, o √∫ltimo passo √© o de remover palavras que s√£o muito frequentes ou muito raras. Essas palavras, devido √†s suas frequ√™ncias anormais, acrescentam pouca informa√ß√£o ao modelo, gerando barulho desnecess√°rio. Portanto, o pesquisador pode optar por filtrar palavras que seriam "*outliers*" nos documentos.


### Construir a matriz *document-feature*

Depois de todos esses passos, o final √© o de transformar o texto em dados num√©ricos. O resultado final √© uma matriz de documentos-termos, chamada de *W*, com N textos e um vocabul√°rio de tamanho (Matriz N x J). No geral, essa matriz √© muito esparsa, isto √©, cont√©m muitos zeros em muitas colunas para muitos documentos. Por isso √© necess√°rio tokenizar, eliminar certos termos, etc. 


### Quando n√£o pr√©-processar?

Nem sempre vale aplicar todo o pr√©-processamento ‚Äúno autom√°tico‚Äù. Em tarefas que dependem de nuances de forma ou contexto ‚Äî como reconhecimento de entidades (nomes pr√≥prios, marcas, locais), an√°lise estil√≠stica, detec√ß√£o de ironia/sarcasmo, distin√ß√µes sem√¢nticas sens√≠veis √† capitaliza√ß√£o (‚ÄúPorto‚Äù vs. ‚Äúporto‚Äù), ou quando a pontua√ß√£o e emojis carregam sinal (sentimento, √™nfase) ‚Äî remover pontua√ß√£o, fazer lowercasing agressivo, tirar stopwords ou reduzir √† raiz pode apagar informa√ß√£o √∫til. Al√©m disso, em dom√≠nios especializados (jur√≠dico, biom√©dico), a forma exata e a morfologia costumam importar; j√° em textos curtos (como t√≠tulos e tweets), cada token tende a ter mais peso, e filtros pesados podem ‚Äúesvaziar‚Äù o sinal. Por isso, √© recomend√°vel: alinhar cada transforma√ß√£o ao objetivo da tarefa; testar incrementalmente o impacto de cada etapa; manter vers√µes reproduz√≠veis do pipeline; ajustar listas de stopwords ao dom√≠nio; preferir lematiza√ß√£o a stemming quando a precis√£o lexical for crucial; registrar decis√µes e m√©tricas; e, quando fizer sentido, combinar representa√ß√µes (p.ex., manter capitaliza√ß√£o como feature adicional, preservar certos sinais de pontua√ß√£o/emoji ou incluir n-grams) em vez de eliminar informa√ß√£o de forma irrevers√≠vel.






