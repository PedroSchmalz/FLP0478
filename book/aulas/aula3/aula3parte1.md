
# *Bag-of-Words* e o modelo Multinomial

## Cap√≠tulo 5 - *Bag-of-words*

Na √∫ltima aula, vimos o que √© um c√≥rpus e os princ√≠pios que o pesquisador deve seguir na constru√ß√£o ou utiliza√ß√£o de um c√≥rpus para tarefas de aprendizado supervisionado. Depois de construir um c√≥rpus de documentos, o pesquisador deve decidir como ir√° representar o texto de forma num√©rica. Uma das formas mais comuns, e utilizadas nos modelos mais cl√°ssicos de aprendizado estat√≠stico, √© a *bag-of-words*, ou saco de palavras. Essa representa√ß√£o ser√° a base dos modelos iniciais do curso que veremos a partir da sexta aula, como o modelo log√≠stico/multinomial. A ideia principal por tr√°s desse modelo √© a de representar cada documento pelo n√∫mero de vezes que cada palavra aparece nele. Considere as seguintes frases (ou documentos):

1. O cachorro ama o osso;
2. O Osso ama o cachorro;

Ignorando o artigo "O", ter√≠amos a seguinte matriz *Document-feature*, ou o seguinte BOW (*Bag-of-words*):


<div align="center">

| Documento | Cachorro | ama | Osso |
|-----------|------|-------|----------|
| Doc1      | 1    | 1     | 1        |
| Doc2      | 1    | 1     | 1        |

</div>

Essa matriz √© chamada assim por que cada linha (ou observa√ß√£o) cont√©m um documento/senten√ßa/frase e cada coluna cont√©m a palavra em considera√ß√£o. Por exemplo, a coluna "Cachorro" mostra quantas vezes o termo Cachorro aparece em cada documento. Neste caso, os dois documentos possuem as mesmas palavras, e seriam tratados como "iguais". Isso se deve ao fato de que, nesse tipo de representa√ß√£o, a ordem das palavras e o contexto n√£o s√£o considerados; por isso, embora as duas frases expressem rela√ß√µes diferentes, a Bag-of-Words as representa de forma id√™ntica, pois cont√™m as mesmas palavras com as mesmas contagens. Mais adiante no curso, ser√£o apresentadas outras representa√ß√µes que capturam contexto, como n-gramas, embeddings e modelos contextuais. No entanto, apesar da sua simplicidade e redu√ß√£o do contexto dos documentos, a BOW pode ser extraordinariemente eficiente a depender do seu objetivo e tarefa. Grimmer et al. estabelecem a seguinte "receita" para se trabalhar com a representa√ß√£o BOW:

1. Escolher a unidade de an√°lise;
2. Tokenizar o texto;
3. Reduzir complexidade;
4. Criar a matriz *document-feature*, ou documentos-termos.


### Escolher a unidade de an√°lise

Depois de garantir a digitaliza√ß√£o dos documentos, ou colet√°-los, o pesquisador deve decidir como vai dividi-los em unidades de an√°lise menores. No nosso exemplo, n√£o precisamos dividir os documentos em unidades menores pois eles j√° est√£o em formato reduzido, por serem *tweets* de at√© 280 caract√©res. No entanto, em muitos casos os pesquisadores estar√£o interessados em textos mais longos, como declara√ß√µes, discursos, manifestos, not√≠cias, artigos. Em outros casos, o objetivo pode ser o de juntar textos menores em documentos maiores (e.g. juntar os tweets de um dia de determinado usu√°rio). Independente da situa√ß√£o, o pesquisador deve estar consciente da decis√£o e escolher aquela que seja melhor para atender o seu objetivo de pesquisa. A unidade de an√°lise pode afetar a pergunta de pesquisa, a efici√™ncia dos modelos e a pr√≥pria coleta de dados. A unidade de an√°lise tamb√©m √© conhecida por *documento*, podendo ser o par√°grafo de uma not√≠cia, uma p√°gina inteira, ou apenas um *tweet*.



### "Tokenizar"

"Tokenizar", ou transformar em *tokens*, vem depois da escolha da unidade de an√°lise. Cada documento ser√° quebrado em suas partes individuais, as palavras. Esse √© o primeiro passo para a cria√ß√£o do saco de palavras (BOW). O exemplo mais comum √© o do tokenizar em n-grams de 1. A frase "Diga n√£o √† vacina√ß√£o obrigat√≥ria!" pode ser quebrada da seguinte forma:


<div align="center">

["Diga","n√£o","√†","vacina√ß√£o","obrigat√≥ria", "!"]

</div>


Seguindo a mesma frase e a mesma etapa de tokeniza√ß√£o em palavras, podemos formar bi-grams, que s√£o pares de palavras consecutivas no texto. Bi-grams ajudam a capturar um pouco de contexto local que o unigram (n=1) n√£o representa, como express√µes fixas, nega√ß√µes e rela√ß√µes imediatas entre termos.

<div align="center">

["Diga_n√£o", "n√£o_√†", "√†_vacina√ß√£o", "vacina√ß√£o_obrigat√≥ria", "obrigat√≥ria_!"]

</div>

Se o objetivo √© capturar resist√™ncia √† vacina√ß√£o, ou hesita√ß√£o vacinal, talvez quebrar o texto em bigramas seja mais informativo do que quebr√°-lo apenas em suas palavras individuais. Com isso, se d√° mais contexto ao modelo de aprendizado de m√°quina, ao custo de adicionar mais combina√ß√µes raras que podem n√£o aparecer tanto em seu banco de dados. üí¨ "Usar ordens maiores de n-gramas pode aumentar substancialmente o n√∫mero de tipos √∫nicos, mas pode ajudar nossa an√°lise textual ao reter mais informa√ß√µes" ({cite}`grimmer2022text`, p. 99, tradu√ß√£o nossa). Novamente, isso n√£o √© uma escolha trivial: Assim como todos os passos e princ√≠pios discutidos nas √∫ltimas aulas, a forma de processar e representar numericamente o texto altera substancialmente os resultados. O pesquisador deve tentar sempre estar consciente dessas escolhas e relat√°-las aos leitores quando necess√°rio. 


```{admonition} üí¨ Com a palavra, os autores:
:class: quote
"Alternativamente, pesquisadores podem achar √∫til manter uma lista de certos bigramas e trigramas que eles antecipam ser √∫teis para sua an√°lise. Por exemplo, Rule, Cointet e Bearman (2015) usam algumas express√µes de m√∫ltiplas palavras, como national security e local government, em sua an√°lise dos discursos SOTU. Vamos indicar isso visualmente colocando um sublinhado entre duas palavras que est√£o sendo tratadas como um n-grama. Quando a lista de n-gramas a ser extra√≠da √© pequena, isso resolve o problema do vocabul√°rio grande, mas exige uma lista de n-gramas."
({cite}`grimmer2022text`, p. 100, tradu√ß√£o nossa)
```

### Reduzir a complexidade

O que Grimmer et al. chamam de "Reduzir a complexidade" √© conhecido de forma mais corriqueira no jarg√£o de aprendizado de m√°quina como "pr√©-processamento" do texto. A principal fun√ß√£o do pr√©-processamento √© o de reduzir os custos computacionais ao diminuir o texto, eliminando palavras muito recorrentes (e, portanto, pouco informativas), reduzir elas √† raiz, etc.

#### 1 - Converter o texto em min√∫sculo

Converter o texto em min√∫sculo, especialmente em BOW, √© garantir que a mesma palavra em min√∫sculo e em mai√∫sculo sejam consideradas iguais. Por exemplo, nas frases:

1. "A decis√£o da Igreja";
2. "A igreja do bairro"

Temos a palavra igreja com duas defini√ß√µes bem diferentes: "Igreja" se referindo √† Igreja cat√≥lica como um todo, no n√≠vel da institui√ß√£o religiosa. E "igreja" se refere ao pr√©dio, um local f√≠sico. Na matriz de documentos, ter√≠amos algo mais ou menos assim (ignorando os artigos e preposi√ß√µes):


<div align="center">


| Documento | decis√£o | Igreja | igreja | bairro |
|-----------|------|-------|----------| --------- |
| Doc1      | 1    | 1     | 0        | 0         |
| Doc2      | 0    | 0     | 1        | 1         |

</div>


Note que surgem duas colunas distintas (‚ÄúIgreja‚Äù e ‚Äúigreja‚Äù) para a mesma forma lexical, inflando o vocabul√°rio. Aplicando *lowercasing* antes da vetoriza√ß√£o, consolidamos as formas:

<div align="center">

| Documento | decis√£o | igreja | bairro |
|-----------|------|-------|---------|
| Doc1      | 1    | 1     | 0         |
| Doc2      | 0    | 1     | 1         |

</div>

Ao inv√©s de quatro colunas, t√™m-se tr√™s. H√° casos em que manter a capitaliza√ß√£o √© √∫til (p.ex., NER, distin√ß√£o de nomes pr√≥prios como ‚ÄúPorto‚Äù vs. ‚Äúporto‚Äù), mas, em pipelines cl√°ssicos de BOW, a normaliza√ß√£o para min√∫sculas costuma ser preferida para reduzir ru√≠do e dimensionalidade. A decis√£o deve considerar o objetivo da tarefa: se a distin√ß√£o entre nomes pr√≥prios e comuns for relevante, pode ser melhor preservar a capitaliza√ß√£o ou criar uma feature espec√≠fica para ‚Äúcapitalizado‚Äù.


#### 2 - Remover pontua√ß√£o

Na maior parte dos casos, a pontua√ß√£o espec√≠fica (e.g. v√≠rgulas, pontos finais, acentos, etc.) n√£o vai adicionar informa√ß√£o relevante ao modelo. Voltando ao exemplo anterior:

<div align="center">

["Diga","n√£o","√†","vacina√ß√£o","obrigat√≥ria", "!"]

</div>

Podemos ter:

<div align="center">

["Diga","nao","a","vacina√ßao","obrigatoria"]

</div>

Esse processo reduz a quantidade de colunas e termos, facilitando o treinamento do modelo de aprendizado de m√°quina. Novamente, a decis√£o de manter certas formas, pontua√ß√µes e termos deve estar alinhada ao objetivo da tarefa.



#### 3 - Remover *stop words*

Podemos estar interessados tamb√©m em remover certas palavras que, de t√£o comuns, n√£o adicionam muita informa√ß√£o ao modelo. Exemplos de palavras "*stop words*" pode ser artigos (a, os, as, os), preposi√ß√µes (de,do,desde,em) ou outras palavras que o pesquisador, ao se defrontar com o texto, considere irrelevantes para a diferencia√ß√£o dos documentos. Com esse passo adicional, a frase acima se torna:

<div align="center">

["Diga","nao","vacina√ßao","obrigatoria"]

</div>

O pesquisador pode utilizar listas comuns de *stop words* em v√°rias l√≠nguas, ou criar sua pr√≥pria lista.


```{admonition} üí¨ Com a palavra, os autores:
:class: quote
"Remover stopwords frequentemente elimina uma fra√ß√£o extremamente alta dos tokens do c√≥rpus enquanto remove muito poucos tipos, o que ‚Äî se essas palavras carregam pouco ou nenhum significado ‚Äî pode resultar em enormes economias computacionais. Isso reflete o fato emp√≠rico de que, na maioria das l√≠nguas, poucas palavras s√£o extremamente comuns. A ideia √© que, embora esses tokens respondam por uma grande fra√ß√£o das palavras, eles representam apenas uma pequena fra√ß√£o do significado. Listas de stopwords em muitas l√≠nguas est√£o dispon√≠veis em pacotes de software que fornecem ferramentas de an√°lise de texto."
({cite}`grimmer2022text`, p. 102, tradu√ß√£o nossa)
```


#### 4 - Reduzir √† raiz

Um outro passo √© o de reduzir as palavras √†s suas ra√≠zes, diminuindo os termos √∫nicos nos documentos. ‚ÄúReduzir √† raiz‚Äù √© um passo de pr√©-processamento que transforma palavras em uma forma b√°sica para diminuir varia√ß√µes superficiais e, assim, reduzir a esparsidade do vocabul√°rio na matriz documento-termo. Na pr√°tica, usa-se:

- **Stemming**: corta sufixos com regras heur√≠sticas para obter um radical aproximado. √â r√°pido, mas pode gerar formas n√£o dicionarizadas (ex.: ‚Äúestudando‚Äù, ‚Äúestudaram‚Äù, ‚Äúestudar√£o‚Äù ‚Üí ‚Äúestud‚Äù).

- **Lematiza√ß√£o**: mapeia cada palavra para o seu lema (forma can√¥nica), considerando informa√ß√£o morfol√≥gica/gramatical. √â mais precisa, por√©m mais custosa (ex.: ‚Äúestudando‚Äù, ‚Äúestudaram‚Äù, ‚Äúestudar√£o‚Äù ‚Üí ‚Äúestudar‚Äù). 

Voltando ao exemplo da vacina√ß√£o obrigat√≥ria, ter√≠amos, com o *stemming*:

<div align="center">

["dig","nao","vacina","obrig"]

</div>

Com a lematiza√ß√£o, ficaria:

<div align="center">

["dizer","n√£o","vacina√ß√£o","obrigat√≥rio"]


</div>


#### 5 - Eliminar Palavras muito frequentes ou muito raras

Como fica claro, o √∫ltimo passo √© o de remover palavras que s√£o muito frequentes ou muito raras. Essas palavras, devido √†s suas frequ√™ncias anormais, acrescentam pouca informa√ß√£o ao modelo, gerando barulho desnecess√°rio. Portanto, o pesquisador pode optar por filtrar palavras que seriam "*outliers*" nos documentos.


### Construir a matriz *document-feature*

Depois de todos esses passos, o final √© o de transformar o texto em dados num√©ricos. O resultado final √© uma matriz de documentos-termos, chamada de *W*, com N textos e um vocabul√°rio de tamanho J (Matriz N x J). No geral, essa matriz √© muito esparsa, isto √©, cont√©m muitos zeros em muitas colunas para muitos documentos. Por isso √© necess√°rio tokenizar, eliminar certos termos, etc. 


### Quando n√£o pr√©-processar?

Nem sempre vale aplicar todo o pr√©-processamento ‚Äúno autom√°tico‚Äù. Em tarefas que dependem de nuances de forma ou contexto ‚Äî como reconhecimento de entidades (nomes pr√≥prios, marcas, locais), an√°lise estil√≠stica, detec√ß√£o de ironia/sarcasmo, distin√ß√µes sem√¢nticas sens√≠veis √† capitaliza√ß√£o (‚ÄúPorto‚Äù vs. ‚Äúporto‚Äù), ou quando a pontua√ß√£o e emojis carregam sinal (sentimento, √™nfase) ‚Äî remover pontua√ß√£o, fazer lowercasing agressivo, tirar stopwords ou reduzir √† raiz pode apagar informa√ß√£o √∫til. Al√©m disso, em dom√≠nios especializados (jur√≠dico, biom√©dico), a forma exata e a morfologia costumam importar; j√° em textos curtos (como t√≠tulos e tweets), cada token tende a ter mais peso, e filtros pesados podem ‚Äúesvaziar‚Äù o sinal. Por isso, √© recomend√°vel: alinhar cada transforma√ß√£o ao objetivo da tarefa; testar incrementalmente o impacto de cada etapa; manter vers√µes reproduz√≠veis do pipeline; ajustar listas de stopwords ao dom√≠nio; preferir lematiza√ß√£o a stemming quando a precis√£o lexical for crucial; registrar decis√µes e m√©tricas; e, quando fizer sentido, combinar representa√ß√µes (p.ex., manter capitaliza√ß√£o como feature adicional, preservar certos sinais de pontua√ß√£o/emoji ou incluir n-grams) em vez de eliminar informa√ß√£o de forma irrevers√≠vel.



## Cap√≠tulo 6 - O Modelo Multinomial

At√© agora, vimos como transformar um texto em n√∫meros usando *bag‚Äëof‚Äëwords* (BOW).  
Mas uma pergunta importante surge: **de onde v√™m esses n√∫meros?**  
Ou, em termos de estat√≠stica: *qual a "hist√≥ria" por tr√°s das contagens de palavras?*

Neste cap√≠tulo, Grimmer et al. discutem a **distribui√ß√£o multinomial**, que √© o modelo probabil√≠stico mais simples e direto para textos representados como BOW.  
Esse modelo permite **atribuir probabilidades a documentos** e pensar em hip√≥teses, como por exemplo: *quem √© o autor de um texto?*  

Aqui √© necess√°rio distinguir entre modelos probabil√≠sticos e modelos algor√≠tmicos, que s√£o duas formas diferentes de ver a mesma coisa. Modelos probab√≠listicos olham para como os dados foram gerados usando probabilidade. Quando as premissas sobre o processo de gera√ß√£o dos dados (Ou *Data Generating Proces*, *DGP*) s√£o corretas, os modelos probabil√≠sticos oferecem uma afirma√ß√£o clara sobre as premissas no funcionamento ideal do modelo. Tamb√©m s√£o f√°ceis de extrapolar para novas situa√ß√µes e apresentam formas claras para otimiza√ß√£o e quantifica√ß√£o da incerteza.

```{admonition} üí¨ Com a palavra, os autores:
:class: quote
"Essas bases probabil√≠sticas contrastam com abordagens algor√≠tmicas, que especificam uma s√©rie de passos, geralmente na forma de uma fun√ß√£o objetivo a ser otimizada. √â melhor pensar nisso como uma linguagem diferente para descrever algo semelhante. Por exemplo, muitos modelos, como a regress√£o linear, podem ser descritos de uma perspectiva probabil√≠stica (um modelo linear normal) ou de uma perspectiva algor√≠tmica (minimizar a soma dos erros quadr√°ticos)."
({cite}`grimmer2022text`, p. 112, tradu√ß√£o nossa)
```

Em estat√≠stica e em ci√™ncia de dados, *Data Generating Process (DGP)*, ou processo gerador de dados, √© a narrativa formal ‚Äî probabil√≠stica ou causal ‚Äî que especifica como os dados observados poderiam ter sido produzidos. Em outras palavras, √© um conjunto de suposi√ß√µes expl√≠citas sobre:

* Quais vari√°veis existem (observadas e latentes).

* Como elas se relacionam (determin√≠stica e/ou estocasticamente).

* Quais distribui√ß√µes de probabilidade regem os componentes aleat√≥rios.

* Como o ‚Äúru√≠do‚Äù entra no sistema e quais s√£o suas propriedades (m√©dia, vari√¢ncia, independ√™ncia/heterocedasticidade, etc.).

* Qual √© a ordem de gera√ß√£o (quem vem antes: par√¢metros, covari√°veis, resultados).

A utilidade do DGP √© dupla. Primeiro, ele torna transparentes as suposi√ß√µes do modelo: quando se sabe exatamente que hist√≥ria est√° sendo contada, sabe-se tamb√©m quando as conclus√µes s√£o v√°lidas. Segundo, um DGP bem especificado habilita a escolha de estimadores apropriados, a deriva√ß√£o de propriedades (consist√™ncia, vi√©s, vari√¢ncia), a quantifica√ß√£o de incerteza e a valida√ß√£o do modelo (diagn√≥sticos e testes de ader√™ncia).

Em modelos probabil√≠sticos, o DGP √© escrito como uma sequ√™ncia de sorteios de distribui√ß√µes (por exemplo, par√¢metros s√£o sorteados de um prior; dados s√£o sorteados de uma verossimilhan√ßa condicional nos par√¢metros). Isso permite infer√™ncia Bayesiana, estima√ß√£o por m√°xima verossimilhan√ßa e an√°lise de incerteza bem fundamentada. Em abordagens algor√≠tmicas, muitas vezes n√£o se escreve explicitamente um DGP; define-se um objetivo de otimiza√ß√£o (loss) e um procedimento de ajuste. Mesmo assim, pode-se interpretar esses procedimentos como aproximando um DGP impl√≠cito (por exemplo, regress√£o linear com MSE corresponde a erros normais i.i.d.).


### Revis√£o ‚Äî Distribui√ß√µes, DGP e Regress√£o Linear

Modelar estatisticamente √© **contar uma hist√≥ria** sobre como os dados surgem. Chamamos essa hist√≥ria de **processo gerador de dados (DGP)**.  
No caso mais simples:

1. **Distribui√ß√£o** escolhida ‚Äì Normal, Bernoulli, Multinomial, etc.  
2. **Par√¢metros** que controlam m√©dia, vari√¢ncia, covari√¢ncias.  
3. **Regras de amostragem** (independ√™ncia, tamanho da amostra).  

Quando alinhamos essas pe√ßas, obtemos previs√µes (m√©dias, vari√¢ncias) que podem ser comparadas √† observa√ß√£o.

---

#### Regress√£o Linear Simples


$$
\underbrace{Y_i}_{\text{quantidade observada}}
=\;\beta_0+\beta_1 X_i \;+\;\underbrace{\varepsilon_i}_{\text{ru√≠do}}
,\qquad
\varepsilon_i\sim\mathcal{N}\bigl(0,\sigma^2\bigr)
$$

* **Distribui√ß√£o** dos erros: Normal(0, œÉ¬≤).  
* **Par√¢metros desconhecidos**: $\beta_0, \beta_1, \sigma^2$.
* **Vari√°veis explicativas**: $X_i$  
* **Suposi√ß√£o de independ√™ncia** entre $\varepsilon_i$.  

Isso gera um conjunto de $Y_i$ cujas **condicionais** $Y|X$ seguem uma normal centrada em $\beta_0+\beta_1X$. Por que assumir erro Normal?

**A. Lembrete-rel√¢mpago: Distribui√ß√£o Normal**  
‚Ä¢ Formato ‚Äúsino‚Äù: sim√©trica em torno da m√©dia $\mu$.  
‚Ä¢ Desvio-padr√£o $\sigma$ controla a ‚Äúlargura‚Äù (68 % dos valores em $\mu \pm \sigma$).  
‚Ä¢ √önica distribui√ß√£o cont√≠nua totalmente descrita por **m√©dia** e **vari√¢ncia**.

**B. Teorema Central do Limite (TCL) em 2 linhas**  
Se somarmos (ou tirarmos a m√©dia de) muitos efeitos independentes com vari√¢ncia finita, o resultado tende a ser Normal ‚Äî **mesmo que cada efeito individual n√£o seja Normal**.  
Consequ√™ncia pr√°tica: o termo de erro $\varepsilon$ de um modelo costuma ser bem aproximado por Normal, porque ele agrega in√∫meras pequenas influ√™ncias n√£o controladas.


#### Conex√£o com o Multinomial (Cap. 6 de Grimmer et al.)

No cap√≠tulo de texto como dado:

* **Distribui√ß√£o de contagem** escolhida: **Multinomial**.  
* **Par√¢metro**: vetor $\mu$ no simplex (probabilidades das palavras).  
* **Dados**: contagens $W_{ij}$.  
* **DGP**: para cada documento i  
  $$
  W_i \sim \text{Multinomial}(M_i,\mu).
  $$

A l√≥gica √© id√™ntica √† regress√£o:

1. Escolher a distribui√ß√£o coerente com a natureza do dado (contagem ‚Üí Multinomial).  
2. Declarar suposi√ß√µes (independ√™ncia de tokens, mesmo $\mu$ em todos os docs ou dentro de grupos).  
3. Estimar $\mu$ pela m√°xima verossimilhan√ßa ($\hat\mu_j=W_{\cdot j}/\sum_j W_{\cdot j}$).  
4. Avaliar ajuste e, se necess√°rio, **regularizar** com um prior Dirichlet (equivalente a adicionar pseudo-contagens).

---

#### Papel das suposi√ß√µes ‚Äî paralelos √∫teis

| Componente            | Regress√£o linear               | Multinomial de texto          |
|-----------------------|--------------------------------|-------------------------------|
| Vari√°vel de interesse | Y cont√≠nuo                     | Vetor de contagens            |
| Distribui√ß√£o do erro  | Normal                         | Multinomial                   |
| Independ√™ncia         | Observa√ß√µes \(i\)              | Tokens dentro de doc          |
| Heterocedasticidade?  | Viola œÉ¬≤ constante             | Viola $\mu$ comum           |
| Regulariza√ß√£o         | Erros-padr√£o robustos, Bayes ridge | Dirichlet prior (\(\alpha\)) |

Quebrar qualquer suposi√ß√£o exige **diagn√≥stico** (gr√°ficos de res√≠duos, compara√ß√£o emp√≠rica/vistas te√≥ricas) e possivelmente **refinar o DGP** (transformar vari√°veis, hierarquizar par√¢metros, robustez).



### Distribui√ß√£o Multinomial

Imagine que temos s√≥ **tr√™s palavras poss√≠veis no vocabul√°rio**: gato, cachorro e peixe.  
Cada documento √© um "saco de palavras" com algumas dessas palavras.  

Se o documento tiver **apenas uma palavra**, podemos represent√°-lo assim:

- gato ‚Üí $(1, 0, 0)$  
- cachorro ‚Üí $(0, 1, 0)$  
- peixe ‚Üí $(0, 0, 1)$  

Isso √© chamado de representa√ß√£o *one‚Äëhot encoding* (s√≥ um valor igual a 1, o resto √© 0). Tamb√©m √© conhecido como dummy variable, com cada coluna tendo valores bin√°rios (zero e um) para cada palavra.

---

#### Liga√ß√£o com a probabilidade

Podemos imaginar um **sorteio de palavras**: para cada posi√ß√£o do documento, escolhemos uma palavra de acordo com certas probabilidades.  

Por exemplo, suponha que temos:

$$
\mu = (0.5,\, 0.25,\, 0.25)
$$

Isso significa que:
- metade das vezes sai **gato**,
- em 25% das vezes sai **cachorro**,
- e em 25% das vezes sai **peixe**.

---

#### Quando o documento tem mais de uma palavra

Agora, em vez de escolher uma palavra, escolhemos v√°rias (por exemplo $M=10$ palavras).  
O documento √© uma **cole√ß√£o de sorteios**. O modelo que descreve isso √© a **distribui√ß√£o multinomial**.

Formalmente, se $W_{ij}$ √© a contagem da palavra $j$ no documento $i$, dizemos:

$$
W_i \sim \text{Multinomial}(M_i, \mu).
$$

E a f√≥rmula da probabilidade desse documento √©:

$$
p(W_i \mid \mu) = \frac{M_i!}{\prod_{j=1}^J W_{ij}!}
\prod_{j=1}^J \mu_j^{W_{ij}}.
$$

üí° N√£o se assuste:  
- a fra√ß√£o com fatoriais ($M_i! / \prod W_{ij} !$) s√≥ diz **quantas formas diferentes h√° de reorganizar as palavras dentro do documento**;  
- o produto $\prod \mu_j^{W_{ij}}$ s√≥ diz: "qual √© a probabilidade de ter exatamente tantas ocorr√™ncias de cada palavra".

---

#### Exemplo intuitivo

Diga que $\mu = (0.5, 0.25, 0.25)$, ou seja, metade das vezes d√° **gato**.  

Agora criamos um documento com 3 palavras: (peixe, gato, peixe).  
Isso equivale ao vetor:

$$
W = (1, 0, 2).
$$

A probabilidade desse documento √©:

$$
p(W \mid \mu) = \frac{3!}{1!\,0!\,2!}(0.5)^1 (0.25)^0 (0.25)^2 = 0.09375.
$$

Ou seja, **9,4% de chance** de observar esse documento, dado o modelo.  

---

#### O que essa distribui√ß√£o garante?

- **M√©dia**  
  \[
    \mathbb{E}[W_{ij}] \;=\; M_i \,\mu_j
  \]  
  A contagem esperada do termo \(j\) no documento \(i\) √© a sua probabilidade \(\mu_j\) multiplicada pelo tamanho do documento \(M_i\).

- **Vari√¢ncia**  
  \[
    \operatorname{Var}(W_{ij}) \;=\; M_i \,\mu_j \,(1-\mu_j)
  \]  
  A incerteza √© maior quando \(\mu_j \approx 0{,}5\) e diminui quando a palavra √© muito rara ou muito comum.

- **Covari√¢ncia**  
  \[
    \operatorname{Cov}\!\bigl(W_{ij},\,W_{ij'}\bigr) \;=\; -\,M_i \,\mu_j \,\mu_{j'}
  \]  
  Como o total de tokens √© fixo (\(M_i\)), um aumento na contagem de \(j\) implica redu√ß√£o esperada em \(j'\) ‚Äî efeito de ‚Äúsoma constante‚Äù.


---

#### Um Modelo de Linguagem B√°sico

Um **modelo de linguagem** √© qualquer modelo que atribui probabilidades a textos.  
No caso multinomial, pensamos o texto como "gerado" a partir de uma **caixa de probabilidades $\mu$**.

---

#### Aplica√ß√£o famosa ‚Äì *Federalist Papers*

Mosteller e Wallace (1963) tentaram descobrir quem escreveu alguns ensaios com autoria disputada.

Considere o vocabul√°rio bem pequeno: {**by**, **man**, **upon**}.  
Contagens observadas:

| Autor     | by  | man | upon |
|-----------|-----|-----|------|
| Hamilton  | 859 | 102 | 374  |
| Jay       | 82  |   0 |   1  |
| Madison   | 474 |  17 |   7  |
| Disputado | 15  |   2 |   0  |

---

#### Estimando $\mu$

Para cada autor, calculamos a *fra√ß√£o de uso* de cada palavra.  
Por exemplo, Hamilton:

$$
\hat{\mu}_H = \left(\tfrac{859}{1335}, \tfrac{102}{1335}, \tfrac{374}{1335}\right)
= (0.64,\, 0.08,\, 0.28).
$$

Fazendo o mesmo para os outros autores, temos:

- Jay: $(0.99, 0, 0.01)$  
- Madison: $(0.95, 0.035, 0.015)$

---

#### Testando o documento disputado

Documento disputado = $(15, 2, 0)$.  

Calculamos a probabilidade de ele surgir segundo cada autor.  
O resultado indica que **Madison** √© o autor mais prov√°vel.

---

#### Por que precisamos de *Smoothing*?

Note como Jay tem probabilidade **zero** de usar "man".  
Mas ser√° que isso √© verdade? N√£o, s√≥ temos poucos textos de Jay. Ele *poderia* ter usado "man", s√≥ n√£o apareceu.  

Se aplicarmos as f√≥rmulas ‚Äúsecas‚Äù, ent√£o qualquer vez que "man" apare√ßa, Jay √© automaticamente imposs√≠vel.  
Isso √© perigoso!

---

#### Solu√ß√£o: Suaviza√ß√£o de Laplace (add-one)

Em vez de assumir que zero √© zero, adicionamos **1 a todas as contagens**:

$$
\tilde{W}_{ij} = W_{ij} + 1.
$$

Assim, toda palavra tem chance n√£o-nula.  
√â como imaginar que vimos cada palavra pelo menos uma vez.

---

#### O Dirichlet como Regulariza√ß√£o

Podemos ser ainda mais elegantes: em vez de *for√ßar um add‚Äëone manual*,  
colocamos um **prior probabil√≠stico** sobre $\mu$: a **Distribui√ß√£o de Dirichlet**.

---

#### O que √© uma Dirichlet?

Uma distribui√ß√£o que gera vetores de probabilidades $\mu$ (n√£o negativos, que somam 1).  
Ou seja, √© perfeita para modelar "propor√ß√µes de palavras".

Se:

$$
\mu_k \sim \text{Dirichlet}(\alpha),
$$

ent√£o o vetor $\mu_k$ j√° vem com uma no√ß√£o de "suaviza√ß√£o".  
Os par√¢metros $\alpha_j$ funcionam como **pseudo-contagens**.

Exemplo:  
- Se $\alpha = (1,1,1)$, √© como se tiv√©ssemos visto *1 ocorr√™ncia fict√≠cia de cada palavra*.  
- Se $\alpha = (10,10,10)$, todos os $\mu$ ficar√£o pr√≥ximos de $(1/3,1/3,1/3)$.

---

#### Propriedades

- Esperan√ßa:  
  $$ \mathbb{E}[\mu_j] = \frac{\alpha_j}{\sum \alpha} $$
- Vari√¢ncia:  
  valores grandes de $\alpha$ ‚Üí distribui√ß√µes concentradas;  
  valores pequenos ‚Üí maior variabilidade.

---

### 6.5 Conclus√£o

- A **distribui√ß√£o multinomial** representa **contagens de palavras**, assumindo sorteios independentes.  
- Ela explica como calcular probabilidades de documentos e possibilita an√°lises de autoria, classifica√ß√£o etc.  
- O problema dos zeros √© resolvido com **suaviza√ß√£o (Laplace)** ou com um **prior Dirichlet**.  
- Esse √© um dos pontos de partida para modelos de t√≥picos, classifica√ß√£o supervisionada e outras t√©cnicas avan√ßadas.

---

Resumindo: o modelo multinomial √© como **imaginar que um autor tem um saquinho de palavras com certas probabilidades, e cada documento √© produzido sorteando delas v√°rias vezes**.







