# Aprendizado Supervisionado


Na √∫ltima aula vimos a representa√ß√£o textual *Bag-of-words*, que divide o texto com base nos seus componentes b√°sicos, as palavras. Vimos tamb√©m como um modelo de aprendizado supervisionado b√°sico, a Regress√£o Multinomial, pode utilizar essa representa√ß√£o como *features* no treinamento, utilizando a frequ√™ncia das palavras em cada documento para tentar prever a classe √† qual aquele documento pertence. Hoje, discutiremos alguns conceitos que foram discutidos de maneira breve na se√ß√£o de "Vis√£o Geral do Curso". Definiremos o que √© o Aprendizado de M√°quina, o Aprendizado Supervisionado, e os principais conceitos relacionados √† essas tarefas.

## Aprendizado de M√°quina

O Aprendizado de M√°quina √© uma tecnologia dentro do campo de Intelig√™ncia Artificial que permite que computadores aprendam e fa√ßam predi√ß√µes sem programa√ß√£o expl√≠cita. Intelig√™ncia Artificial √© a "Intelig√™ncia apresentada por artefatos (e.g. M√°quinas), em oposi√ß√£o √† intelig√™ncia natural (IN) apresentada por animais, como os humanos. [...] De maneira geral, definimos intelig√™ncia como a habilidade de perceber um ambiente, analis√°-lo e tomar ac√µes/decis√µes que maximizam a chance de atingir determinado objetivo" ({cite}`cerulli2023fundamentals`., p. 5). Como mostra a {numref}`Figura {number} <AIML>`, Aprendizado de M√°quina √© uma sub√°rea da Intelig√™ncia artificial e, como definimos na primeira linha, permitem que a m√°quina tome ac√µes/decis√µes com base em um conjunto de dados/experi√™ncias pr√©vias, e n√£o em programa√ß√£o expl√≠cita. Aprendizado de M√°quina e Aprendizado Estat√≠stico s√£o utilizados de maneira intercambi√°vel na literatura. Dentro da √°rea de aprendizado supervisionado, temos a sub√°rea de Aprendizado Profundo (ou *Deep Learning*), em que a caracter√≠stica definidora dos modelos s√£o de que possuir√£o diversas camadas neurais (veremos o que √© isso futuramente).


```{figure} ../aula4/images/AIML.png
---
width: 100%
name: AIML
align: center
---
 Rela√ß√£o Entre Intelig√™ncia Artificial, Aprendizado de M√°quina e *Deep Learning*. Fonte: [Somos Tera](https://blog.somostera.com/data-science/deep-learning-vs-machine-learning)
```


A {numref}`Figura {number} <classicdiv>` mostra como a literatura faz a divis√£o cl√°ssica do Aprendizado de M√°quina. Temos aplica√ß√µes supervisionadas, em que um conjunto de valores $Y$ (*targets*) s√£o preditos com base em um conjunto de vari√°veis explicativas (ou *features*). Existem dois tipos de aplica√ß√µes supervisionadas: As com *targets* de valores cont√≠nuos (Regress√£o) e as de valores categ√≥ricos (Classifica√ß√£o). No decorrer do curso, focaremos em aplica√ß√µes de Classifica√ß√£o. No entanto, existem tamb√©m aplica√ß√µes n√£o supervisionadas, como as de *Clustering*, que buscam encontrar padr√µes nos dados (e.g. Classifica√ß√£o de T√≥picos, Divis√£o em grupos) sem que o humano/pesquisador forne√ßa r√≥tulos ou valores alvo. H√° ainda uma terceira categoria, a dos m√©todos semi-supervisionados, que combinam um pequeno conjunto de dados rotulados com muitos dados n√£o rotulados para melhorar o desempenho dos modelos. Por fim, existe o Aprendizado por Refor√ßo (*Reinforcement Learning*), em que um agente interage com um ambiente e aprende, por tentativa e erro, a escolher a√ß√µes que maximizem a recompensa acumulada ao longo do tempo. Aqui est√£o alguns exemplos t√≠picos de cada fam√≠lia de aplica√ß√µes:

- Classifica√ß√£o (supervisionado) ‚Äì filtragem de e-mails spam √ó n√£o spam.

- Regress√£o (supervisionado) ‚Äì previs√£o de sal√°rio a partir de experi√™ncia profissional e localiza√ß√£o.

- Clustering (n√£o supervisionado) ‚Äì segmenta√ß√£o de clientes em grupos com padr√µes de compra semelhantes.

- Semi-supervisionado ‚Äì treinamento de um classificador de imagens m√©dicas usando poucas tomografias rotuladas e milhares sem r√≥tulo.

- Reinforcement Learning ‚Äì agentes que aprendem a jogar Go ou a controlar bra√ßos rob√≥ticos por meio de recompensas de desempenho.



```{figure} ../aula4/images/classicdivision.jpg
---
width: 100%
name: classicdiv
align: center
---
 Divis√£o Cl√°ssica do Aprendizado de M√°quina. Fonte: [Ribeiro e Gomes](https://www.researchgate.net/publication/374010223_On_the_Use_of_Machine_Learning_for_Damage_Assessment_in_Composite_Structures_A_Review) (2023) {cite}`ribeiro2023machinelearning`.
```


O Paradigma central do aprendizado supervisionado se articula na ideia de traduzir uma tarefa cognitiva em um problema estat√≠stico (Cerulli, p. 7). O aprendizado estat√≠stico come√ßa com a coleta de informa√ß√µes do passado armazenados em um objeto $D$, o **banco de dados**. Um banco de dados √© uma cole√ß√£o de informa√ß√µes sobre $N$ casos, nos quais observamos um resultado (ou *outcome*) $y$, e um conjunto $p$ de preditores (ou vari√°veis explicativas) $ X = (X_i, ..., Xp) $:


$$
D_i := {(y_i, \mathbf{x_i}), i = 1,\dots, N}
$$

Grosso modo, $D_i$ √© igual ao conjunto dos pares ordenados $(y_i,x_i)$ com i indo de 1 a $N$, o tamanho do banco de dados. Na aplica√ß√£o de PLN que estamos trabalhando ao longo do curso, $D_i$ √© o banco de dados contendo todas as publica√ß√µes dos pol√≠ticos no *X*, os pares ordenados $(y_i,x_i)$ representam a nossa classifica√ß√£o para cada publica√ß√£o(Sentimento ou Posicionamento), em $y_i$, e a representa√ß√£o do nosso texto em $x_i$. A tarefa principal do aprendizado de m√°quina √© mapear, usando o banco de dados, os preditores $x_i$ para cada resultado $y_i$. Com isso, temos o seguinte algoritmo (ou fun√ß√£o) geral:

$$
(x_1,...,x_p) \xrightarrow{\,f\,} y 
$$


## Fun√ß√£o Erro


Para mapear isso da melhor maneira, precisamos de outra fun√ß√£o: A **fun√ß√£o erro**. A **fun√ß√£o erro**, de forma muito geral e superficial, √© um mapeamento  

$$
L : (y, \hat{y}) \;\longrightarrow\; \mathbb{R}_{\ge 0}
$$

que devolve um escalar n√£o-negativo indicando o quanto a predi√ß√£o $\hat{y}=f(\mathbf{x})$ diverge do valor verdadeiro $y$. Traduzindo, o modelo de aprendizado de m√°quina utilizar√° a fun√ß√£o erro para entender o qu√£o distante ele est√° do melhor resultado. Nas sucessivas itera√ß√µes, ele vai tentar minimizar o erro, consequentemente minimizando a fun√ß√£o de custo. Uma fun√ß√£o de erro comum na regress√£o linear √© o MSE (*Mean Squared Error*):

$$
MSE (X) = E[(y- f(x)¬≤| x)]
$$

Cada tarefa de aprendizado de m√°quina ter√° sua fun√ß√£o de erro espec√≠fica (at√© a n√£o supervisionada). N√£o √© necess√°rio memorizar todas as fun√ß√µes erro/custo, e alguns modelos (como os de aprendizado profundo) usam fun√ß√µes erro pr√≥prias. O importante √© entender o que s√£o e que o objetivo do modelo, numa aplica√ß√£o deste tipo, √© o de reduzir o erro. O melhor modelo, na concep√ß√£o cl√°ssica do Aprendizado de m√°quina, √© aquele que consegue o melhor resultado na aproxima√ß√£o de $E(y|x)$. Ou seja, o que consegue o melhor resultado (Acur√°cia, Precis√£o, etc.) utilizando as *features* para tentar prever o *target*.



```{admonition} üí¨ Com a palavra, os autores:
:class: quote
"Em Aprendizado de M√°quina, prever a vari√°vel-alvo √© t√£o central que podemos definir a √°rea como um conjunto de estrat√©gias de modelagem (param√©tricas ou n√£o param√©tricas) cujo objetivo √© obter uma aproxima√ß√£o confi√°vel de $E(y‚à£x)$, tomando a acur√°cia de predi√ß√£o como princ√≠pio orientador. Assim, alguns m√©todos podem ser considerados superiores a outros desde que a predi√ß√£o seja o √∫nico prop√≥sito da an√°lise. A estimativa estat√≠stica de $E(y‚à£x)$ est√° sujeita a dois tipos de erro poss√≠veis: (1) erro amostral e (2) erro de especifica√ß√£o."
({cite}`cerulli2023fundamentals`, p. 15, tradu√ß√£o nossa)
```

## Classifica√ß√£o Supervisionada em PLN

A tarefa de classifica√ß√£o √© uma das aplica√ß√µes mais comuns do aprendizado supervisionado, especialmente em Processamento de Linguagem Natural (PLN). O objetivo central da classifica√ß√£o √© atribuir um r√≥tulo ou categoria a cada exemplo do conjunto de dados, com base em suas caracter√≠sticas (features). No contexto do curso, isso significa, por exemplo, identificar o sentimento ou posicionamento de uma publica√ß√£o de pol√≠tico a partir do texto (representado numericamente) de uma publica√ß√£o no *X*. A classifica√ß√£o supervisionada usa um conjunto de documentos rotulados em categorias para criar um modelo estat√≠stico que relaciona as palavras nos documentos aos r√≥tulos, e aplica este modelo para conseguir r√≥tulos compar√°veis para outros documentos n√£o rotulados. Nesse tipo de aplica√ß√£o, o pesquisador √© respons√°vel por providenciar tr√™s tipos de informa√ß√£o: as categorias/r√≥tulos para os documentos de treinamento; a representa√ß√£o do texto (*BOW* ou outra), e a classe de modelos que vai ser utilizada no treinamento. Na tarefa de classifica√ß√£o, o modelo de aprendizado de m√°quina tamb√©m √© referido como *Classificador*.

No projeto *Mapping Political Elites COVID-19 Vaccine Tweets in Brazil*, o objetivo √© o de anotar publica√ß√µes no X (antigo *Twitter*) de pol√≠ticos brasileiros sobre as vacinas de COVID-19 no per√≠odo da pandemia. As categorias de anota√ß√£o s√£o: Se as publica√ß√µes s√£o relevantes com rela√ß√£o √†s vacinas de COVID-19 (**Relev√¢ncia**), se possuem sentimentos negativos, positivos ou indeterminados (**An√°lise de Sentimento**); e se posicionam de forma contr√°ria, favor√°vel ou indeterminada com rela√ß√£o √†s vacinas (**Detec√ß√£o de Posicionamento**). Mais detalhes sobre o processo de anota√ß√£o, coleta, e os dados dispon√≠veis est√£o no [Github](https://github.com/NUPRAM/CoViD-Pol) do NUPRAM (N√∫cleo de Pol√≠ticas, Redes Sociais e Aprendizado de M√°quina). Esse √© um projeto no formato cl√°ssico de uma tarefa de classifica√ß√£o em PLN: Temos tr√™s categorias para cada publica√ß√£o (Relev√¢ncia, Sentimento e Posicionamento); temos a representa√ß√£o do texto (*Embeddings* do BERTimbau, que veremos no final do curso); e temos a classe de modelos (O BERTimbau, um modelo de aprendizado profundo para o portugu√™s brasileiro). Segundo Grimmer et al., Um processo de anota√ß√£o tem quatro passos:


### 1. Criar o banco de Treinamento

Criar um banco de treinamento (ou usar um j√° existente) √© o primeiro passo em toda aplica√ß√£o de Aprendizado de M√°quina. Para isso, √© necess√°rio codifica√ß√£o/anota√ß√£o por feita por humanos. A codifica√ß√£o humana de textos existe h√° muito tempo para a organiza√ß√£o e quantifica√ß√£o de textos (e.g. g√™neros textuais, g√™neros musicais, categorias de livros). Assim que um pesquisador define categorias, a codifica√ß√£o humana √© o processo de colocar manualmente esses documentos em categorias. Esse processo √© uma combina√ß√£o de um *codebook*, o treinamento de anotadores, e os processos internos espec√≠ficos de cada anotador. Com base em Neuendorf (2016), Grimmer et al. estabelecem as seguintes caracter√≠sticas de um bom banco de treinamento:

* Objetividade-Intersubjetividade: A categoria mensurada √© objetiva, e seu entendimento n√£o √© restrito √† uma √∫nica pessoa (ou grupo de pessoas);
* Desenho *a priori*: O banco de dados deve ser classificado com base em um *codebook*;
* Confiabilidade: Diferentes conjuntos de anotadores humanos deveriam ser capazes de atingir mais ou menos a mesma classifica√ß√£o no mesmo conjunto de dados;
* Validade: A m√©trica deve estar alinhada com o conceito de interesse;
* Generalizabilidade: O banco de treinamento ser√° baseado em uma amostra de um conjunto maior de documentos. A anota√ß√£o feita nessa amostra deve ser generaliz√°vel para o resto dos documentos;
* Replicabilidade: Outros pesquisadores e anotadores deve ser capazes de replicar a anota√ß√£o no mesmo conjunto de dados (ou aplicar em outros conjuntos de documentos).


#### a) Criando um codebook

Um *codebook* √© um documento que define de forma clara e detalhada as categorias, crit√©rios e regras que devem ser seguidos durante o processo de anota√ß√£o dos dados. Ele serve como guia para os anotadores humanos, garantindo que todos compreendam e apliquem os conceitos de maneira consistente e objetiva. O codebook descreve exemplos, contraexemplos e situa√ß√µes amb√≠guas, ajudando a reduzir interpreta√ß√µes subjetivas e aumentando a confiabilidade e a replicabilidade da classifica√ß√£o. Em projetos de aprendizado supervisionado, um codebook bem elaborado √© fundamental para assegurar que o banco de treinamento reflita fielmente o conceito de interesse e possa ser utilizado por outros pesquisadores/anotadores. Vamos pegar o exemplo da nossa classifica√ß√£o de Relev√¢ncia das publica√ß√µes de pol√≠ticos. Essas foram as regras: Posts cujo conte√∫do se referia a vacinas e √† vacina√ß√£o contra a COVID-19 receberam valor 1, enquanto posts que apenas continham palavras-chave, mas n√£o tratavam de vacinas/vacina√ß√£o contra a COVID-19, receberam valor 0. Posts classificados como **relevantes** inclu√≠am:

* Cita√ß√£o direta de vacinas/vacina√ß√£o contra a COVID-19;

* Refer√™ncia indireta a vacinas e vacina√ß√£o contra a COVID-19 (por exemplo, discuss√£o sobre outras vacinas e/ou campanhas de vacina√ß√£o);

* Termos espec√≠ficos ‚Äî ou que possam ser inferidos como tais ‚Äî relativos √†s vacinas contra a COVID-19, como ‚Äúsegunda e terceira doses/aplica√ß√µes‚Äù, ‚Äúdoses de refor√ßo‚Äù;

* Considerando o per√≠odo deste estudo, tweets que mencionem vacinas e/ou vacina√ß√£o no Brasil ou em outros pa√≠ses, mesmo que n√£o especifiquem COVID-19;

* Men√ß√£o ao trabalho relacionado a vacinas de institui√ß√µes (por exemplo, Fiocruz, Butantan), cientistas (por exemplo, Peter Hotez) ou pol√≠ticos, ou ainda opini√µes e comportamentos pr√≥-vacina ou anti-vacina (por exemplo, Osmar Terra, CPICOVID19), mesmo em hashtags (#);

* Men√ß√£o ao trabalho relacionado a vacinas de laborat√≥rios, ind√∫strias ou organiza√ß√µes respons√°veis pela produ√ß√£o ou desenvolvimento de vacinas, como Fiocruz, Butantan, Covaxin, AstraZeneca, Oxford etc.;

* Campanhas de vacina√ß√£o e comunicados de utilidade p√∫blica mencionando faixas et√°rias espec√≠ficas e limitadas (por exemplo, ‚ÄúVacina√ß√£o para 37‚Äì39 anos come√ßa amanh√£‚Äù), pois tais an√∫ncios eram quase exclusivamente para campanhas de vacina√ß√£o contra a COVID-19;

* Mensagens que discutam terapias e tratamentos para infec√ß√£o por COVID-19 no contexto da pr√≥pria COVID-19;

* Men√ß√µes √† imuniza√ß√£o obtida por vacinas ou por contamina√ß√£o com o v√≠rus da COVID-19 (por exemplo, imuniza√ß√£o natural, imuniza√ß√£o de rebanho etc.); ou

* Tweets que incluam termos como ‚Äúnegacionista‚Äù, ‚Äúnegacionismo‚Äù e equivalentes, que se refiram direta ou indiretamente √†s posi√ß√µes da elite pol√≠tica sobre vacinas.

Os dois exemplos abaixo s√£o de *tweets* reais considerados relevantes na nossa anota√ß√£o:

```{admonition} üê¶ Tweet
:class: tweet
**@capitaoassum√ß√£o**: Vacina Para todos!
13:00 ¬∑ 14 mar. 2021
```

```{admonition} üê¶ Tweet
:class: tweet
**@celsorussomano**: Cuidado com a ditadura que querem nos impor com a vacina da COVID-19. Estamos falando de uma vacina experimental, e todos nos corremos o risco de sermos cobaias. Isso √© um desrespeito com a vida dos paulistanos, e n√£o podemos aceitar que siga adiante!
12:34 ¬∑ 12 fev. 2021
```



Posts considerados **n√£o relevantes** inclu√≠am mensagens que se referiam a:

* Vacina√ß√£o em animais;

* ‚ÄúVacina‚Äù usada como met√°fora para outro tema (por exemplo, transpar√™ncia como vacina contra a corrup√ß√£o);

* Mensagem sobre outro assunto, mas contendo uma hashtag de vacina da COVID-19;

* Aus√™ncia de men√ß√£o a vacina√ß√£o, vacinas, laborat√≥rios ou qualquer palavra apresentada acima como relevante;

* Mensagens em idioma estrangeiro que seriam relevantes se estivessem em portugu√™s; ou

* Mensagens que discutam terapias e tratamentos para infec√ß√£o por COVID-19, mas cujo contexto n√£o esteja relacionado √† COVID-19.


Abaixo est√£o dois exemplos tamb√©m reais de tweets *Irrelevantes*:

```{admonition} üê¶ Tweet
:class: tweet
**@anisiomaiapb**: Alguns partidos ainda funcionam como monarquias onde as decis√µes s√£o tomadas entre pai e filho ou entre marido e esposa. N√£o passam de projetos familiares. Ainda bem que o PT est√° vacinado contra isto e continuamos militando num projeto plural, democr√°tico e participativo.
```



```{admonition} üê¶ Tweet
:class: tweet
**@JoaoCampos**: O @governope decretou, a partir de hoje, a obrigatoriedade do uso de m√°scara para quem trabalha em estabelecimentos comercias durante a pandemia. Mas, se puder, saia de casa sempre de m√°scara, ela √© a √∫nica vacina que temos contra o coronav√≠rus.
```

A segunda publica√ß√£o, apesar de ser durante a pandemia e referenciar as m√°scaras, n√£o se refere √†s vacinas de COVID-19, mas usa as m√°scaras como analogia √† vacina. Portanto, √© considerada Irrelevante. 

Esses s√£o exemplos de uma √∫nica tarefa de codifica√ß√£o dentro do nosso projeto, e a mais "direta". Mesmo nessa tarefa simples, precisamos definir muitas regras para desambiguar situa√ß√µes at√≠picas e garantir a replicabilidade e confiabilidade do nosso conjunto de treinamento. Veremos, futuramente, que a classifica√ß√£o de Posicionamento e Sentimento s√£o mais complexas e geraram menor concord√¢ncia entre anotadores, al√©m de problemas de desbalanceamento dos dados.


#### b) Escolher e Gerenciar anotadores

Seguindo o desenvolvimento de regras detalhadas para cada categoria de relev√¢ncia, posicionamento e sentimento, foi criado um manual de codifica√ß√£o para treinar a equipe de anotadores; al√©m disso, literatura sobre hesita√ß√£o vacinal foi compartilhada a fim de aprimorar a compreens√£o da complexidade das atitudes e emo√ß√µes em rela√ß√£o √† vacina√ß√£o. A equipe de pesquisa tamb√©m recebeu treinamento sobre as diferen√ßas de contexto dos tr√™s anos analisados (por exemplo, 2020 como ano sem vacinas; 2021, in√≠cio da vacina√ß√£o de adultos e adolescentes; e 2022, in√≠cio da imuniza√ß√£o de crian√ßas e beb√™s contra a SARS-CoV-2).

A anota√ß√£o do corpus foi realizada em 61 rodadas. Em cada rodada, uma amostra aleat√≥ria de 200 publica√ß√µes era classificada usando um banco de dados criado especificamente para o projeto, no qual os posts eram anonimizados, removendo informa√ß√µes sobre autor, data ou imagens associadas √† mensagem. O processo de classifica√ß√£o teve duas etapas: 

Relev√¢ncia ‚Äî Tr√™s anotadores classificavam cada post como relevante ou n√£o relevante. Em seguida, conflitos eram revisados por tr√™s pesquisadores seniores.

Posicionamento e sentimento ‚Äî Depois de resolvidos os conflitos de relev√¢ncia, os posts considerados relevantes eram novamente anotados, agora quanto ao posicionamento e ao tipo de sentimento, de forma independente por tr√™s anotadores. Conflitos nessas duas categorias tamb√©m eram revistos pelos supervisores de pesquisa.


#### c) Selecionar Documentos

O processo de amostragem teve duas fases: Primeiro, definiu-se que o grupo pol√≠tico a ser estudado era os candidatos que estavam concorrendo √†s elei√ß√µes municipais nas capitais em 2020, para o cargo de prefeito. Depois, coletou-se todas as publica√ß√µes feitas por esses indiv√≠duos nos anos 2020, 2021 e 2022. Para a anota√ß√£o do c√≥rpus final, foram amostradas aleatoriamente 9.045 publica√ß√µes, que ficaram divididas em 61 rodadas de aproximadamente 200 publica√ß√µes. 


#### d) Checar Confiabilidade

A confiabilidade da anota√ß√£o de um c√≥rpus pode ser checada de algumas maneiras. A primeira √© se consultar regularmente com os anotadores para verificar se n√£o h√° contextos que n√£o foram levados em conta, se eles conseguiram entender o processo de anota√ß√£o, e se eles concordam entre si. Existem m√©tricas que permitem mensurar o quanto os anotadores concordam em cada fase da anota√ß√£o, comparando os r√≥tulos dados por cada um em determinado documento. A {numref}`Figura {number} <krippendorf>` mostra o quanto a concord√¢ncia entre anotadores variou ao longo das tr√™s tarefas (Relev√¢ncia, Posicionamento e Sentimento). [Veja mais](https://pt.wikipedia.org/wiki/Alfa_de_Krippendorff) sobre o Alfa de Krippendorf.


```{figure} ../aula4/images/Krippendorff_Alpha.png
---
width: 100%
name: krippendorf
align: center
---
Alfa de Krippendorf para as tr√™s tarefas de anota√ß√£o. Fonte: Barberia et al., 2025 ({cite}`barberia2025its`)
```

A relev√¢ncia apresentou o maior acordo geral entre os anotadores, com alfa m√©dio de 0,94 e varia√ß√£o m√≠nima entre as rodadas. Essa tarefa se beneficiou de um conjunto de dados mais balanceado, menos conflitos entre os anotadores e um n√∫mero total maior de observa√ß√µes. Os crit√©rios de sentimento e posicionamento obtiveram acordo moderado. O alfa m√©dio foi 0,67 para Sentimento e 0,70 para Posicionamento. Contudo, ambas as tarefas mostraram grande variabilidade ao longo das rodadas. Essa oscila√ß√£o pode ser atribu√≠da ao desequil√≠brio entre classes nessas duas classifica√ß√µes, o que gerou maior discord√¢ncia nos conte√∫dos analisados em cada rodada, sobretudo nas categorias minorit√°rias (desfavor√°vel e indefinido).


### 2. Escolher o Modelo de Aprendizado de M√°quina

Ap√≥s a cria√ß√£o de um banco de treinamento confi√°vel e bem anotado, o pr√≥ximo passo √© selecionar o modelo de aprendizado de m√°quina mais adequado para a tarefa. A escolha do modelo depende de diversos fatores, como o tipo de problema (classifica√ß√£o, regress√£o, etc.), o tamanho e a qualidade dos dados, o n√∫mero de categorias, e o objetivo da an√°lise.

No contexto de Processamento de Linguagem Natural (PLN), modelos cl√°ssicos como Regress√£o Log√≠stica, Naive Bayes e √Årvores de Decis√£o s√£o frequentemente utilizados para tarefas de classifica√ß√£o de texto, especialmente quando se trabalha com representa√ß√µes simples como *Bag-of-Words*. Para problemas mais complexos ou conjuntos de dados maiores, modelos de aprendizado profundo, como redes neurais e transformadores (por exemplo, BERTimbau para portugu√™s), podem oferecer ganhos significativos de desempenho.

√â importante considerar tamb√©m a interpretabilidade do modelo, o tempo de treinamento, e a facilidade de ajuste dos hiperpar√¢metros (discutiremos isso novamente na pr√≥xima aula). Muitas vezes, recomenda-se come√ßar com modelos mais simples e, conforme necess√°rio, avan√ßar para modelos mais sofisticados. Independentemente da escolha, o modelo deve ser treinado utilizando o banco de dados anotado, buscando aprender padr√µes que permitam prever corretamente os r√≥tulos dos novos exemplos.

Por fim, a sele√ß√£o do modelo deve ser acompanhada de uma avalia√ß√£o rigorosa de sua performance, utilizando m√©tricas apropriadas e valida√ß√£o em dados n√£o vistos, para garantir que o classificador seja confi√°vel.

### 3. Checar a Performance

A qualidade de um classificador √© avaliada por m√©tricas como **acur√°cia**, **precis√£o**, **recall** e **F1-score**, que medem o qu√£o bem o modelo consegue distinguir entre as diferentes classes ([Veja Mais](https://developers.google.com/machine-learning/crash-course/classification/accuracy-precision-recall?hl=pt-br)). Al√©m disso, √© fundamental validar o desempenho do modelo em dados n√£o vistos durante o treinamento, garantindo que ele n√£o apenas memorize os exemplos, mas realmente aprenda padr√µes √∫teis e generalize para outros conjuntos de documentos relacionados.

A **acur√°cia** √© uma das m√©tricas mais utilizadas para avaliar o desempenho de um classificador. Ela indica a propor√ß√£o de previs√µes corretas feitas pelo modelo em rela√ß√£o ao total de exemplos avaliados. Em outras palavras, a acur√°cia mostra o quanto o modelo acertou ao classificar os documentos ou exemplos em suas respectivas categorias.

A f√≥rmula da acur√°cia √©:

$$
\text{Acur√°cia} = \frac{\text{N√∫mero de previs√µes corretas}}{\text{N√∫mero total de exemplos}}
$$

Por exemplo, se um modelo classificou corretamente 80 de 100 exemplos, sua acur√°cia ser√° 0,8 (ou 80%). Embora seja uma m√©trica intuitiva e f√°cil de interpretar, √© importante lembrar que ela pode ser enganosa em situa√ß√µes de classes desbalanceadas, onde outras m√©tricas como precis√£o, recall e F1-score tamb√©m devem ser consideradas.

A **precis√£o** (ou *precision*) √© outra m√©trica importante para avaliar o desempenho de um classificador, especialmente em situa√ß√µes onde o custo de falsos positivos √© alto. A precis√£o indica, dentre todas as previs√µes positivas feitas pelo modelo, qual propor√ß√£o realmente corresponde a exemplos positivos. Em outras palavras, ela mostra o quanto o modelo foi assertivo ao identificar exemplos de uma determinada classe.

A f√≥rmula da precis√£o √©:

$$
\text{Precis√£o} = \frac{\text{N√∫mero de verdadeiros positivos}}{\text{N√∫mero total de exemplos classificados como positivos}}
$$

Por exemplo, se o modelo classificou 50 exemplos como positivos, mas apenas 40 deles eram realmente positivos, a precis√£o ser√° $40/50 = 0,8$ (ou 80%). A precis√£o √© especialmente relevante quando queremos evitar que o modelo fa√ßa muitas previs√µes positivas incorretas, como em tarefas de detec√ß√£o de fraudes ou diagn√≥sticos m√©dicos.

O **recall** (ou sensibilidade) √© uma m√©trica que indica a capacidade do classificador de identificar corretamente todos os exemplos positivos presentes no conjunto de dados. Em outras palavras, o recall mostra a propor√ß√£o de exemplos positivos que foram corretamente classificados como positivos pelo modelo.

A f√≥rmula do recall √©:

$$
\text{Recall} = \frac{\text{N√∫mero de verdadeiros positivos}}{\text{N√∫mero de verdadeiros positivos} + \text{N√∫mero de falsos negativos}}
$$

Ou seja, √© a propor√ß√£o de exemplos positivos corretamente identificados pelo modelo, considerando todos os exemplos que realmente pertencem √† classe positiva (verdadeiros positivos + falsos negativos).

O **F1-score** √© uma m√©trica que combina precis√£o e recall em um √∫nico valor, representando a m√©dia harm√¥nica entre as duas. O F1-score √© √∫til para avaliar o desempenho do modelo quando h√° um equil√≠brio desejado entre precis√£o e recall, especialmente em conjuntos de dados desbalanceados.

A f√≥rmula do F1-score √©:

$$
\text{F1-score} = 2 \times \frac{\text{Precis√£o} \times \text{Recall}}{\text{Precis√£o} + \text{Recall}}
$$

O F1-score varia de 0 a 1, sendo 1 o valor ideal, indicando que o modelo tem alta precis√£o e alto recall ao mesmo tempo.

### 4. Aplicar no Banco de Teste

Ap√≥s treinar e validar o modelo de aprendizado supervisionado, o passo final √© aplicar o classificador no banco de teste. O banco de teste consiste em exemplos que n√£o foram utilizados durante o treinamento do modelo, permitindo avaliar sua capacidade de generaliza√ß√£o para dados novos e reais. Ao aplicar o modelo no banco de teste, obtemos previs√µes para cada exemplo e podemos calcular as m√©tricas de desempenho (acur√°cia, precis√£o, recall, F1-score) de forma mais confi√°vel. Esse processo garante que o modelo n√£o apenas aprendeu padr√µes espec√≠ficos do conjunto de treinamento, mas tamb√©m √© capaz de realizar classifica√ß√µes corretas em situa√ß√µes in√©ditas, tornando-se √∫til para aplica√ß√µes pr√°ticas e futuras

## Conclus√£o

O aprendizado supervisionado √© uma abordagem fundamental para an√°lise de textos e classifica√ß√£o de documentos em Processamento de Linguagem Natural. Ao longo do processo, √© essencial construir um banco de treinamento confi√°vel, com regras claras de anota√ß√£o e valida√ß√£o, garantindo objetividade, replicabilidade e generalizabilidade dos resultados. A escolha do modelo de aprendizado de m√°quina deve considerar o tipo de problema, a qualidade dos dados e o objetivo da an√°lise, equilibrando simplicidade, interpretabilidade e desempenho. A avalia√ß√£o rigorosa do classificador, por meio de m√©tricas como acur√°cia, precis√£o, recall e F1-score, assegura que o modelo seja capaz de generalizar para novos dados e produzir resultados √∫teis em aplica√ß√µes reais. Por fim, aplicar o modelo em um banco de teste √© indispens√°vel para validar sua capacidade de classifica√ß√£o em situa√ß√µes in√©ditas, consolidando o papel do aprendizado supervisionado como ferramenta poderosa para extrair conhecimento e apoiar decis√µes baseadas em grandes




















