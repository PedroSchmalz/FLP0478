# O Problema de Classifica√ß√£o

Na √∫ltima aula, aprofundamos os conceitos fundamentais do aprendizado supervisionado, diferenciando os objetivos de infer√™ncia e predi√ß√£o, e discutindo como construir bancos de dados de treinamento confi√°veis para aplica√ß√µes em PLN. Exploramos o papel dos m√©todos param√©tricos e n√£o-param√©tricos, os principais trade-offs entre flexibilidade e interpretabilidade, e a import√¢ncia de equilibrar vi√©s e vari√¢ncia para obter modelos robustos e generaliz√°veis. Tamb√©m revisamos m√©tricas essenciais para avalia√ß√£o de classificadores e destacamos a necessidade de testar os modelos em dados novos para garantir sua utilidade pr√°tica. Por fim, apresentamos um protocolo padr√£o para conduzir pesquisas rigorosas e transparentes em aprendizado de m√°quina supervisionado.

Na aula de hoje, iremos discutir o problema espec√≠fico de classifica√ß√£o, e alguns dos modelos mais b√°sicos utilizados para esta tarefa. O problema de classifica√ß√£o surge quando temos uma vari√°vel categ√≥rica como nossa vari√°vel resposta $y$. Ou seja, n√£o queremos prever um valor num√©rico cont√≠nuo (e.g. valor de uma casa), mas uma classe (favor√°vel, desfavor√°vel, incerto). Alguns dos classificadores[^1] mais comuns s√£o: Regress√£o Log√≠stica, *Linear Discriminant Analysis* (*LDA*), *Quadratic Discriminant Analysis*, *Naive Bayes* e *K Nearest Neighbors* (KNN). No cap√≠tulo 2, James et al. ({cite}`james2023introduction`.) discutem o KNN, e no cap√≠tulo 4 focam nos outros citados acima.


## Por que n√£o Regress√£o Linear?


Uma quest√£o que pode surgir √© a de por que n√£o usar a regress√£o linear para classifica√ß√£o se podemos colocar as categorias como n√∫meros? Vamos supor o seguinte caso de classifica√ß√£o em tr√™s diagn√≥sticos:

$$
Y =
\begin{cases}
  1 & \text{se AVC;} \\\\
  2 & \text{se Overdose;} \\\\
  3 & \text{se Crise Epil√©ptica.}
\end{cases}
$$

Al√©m de gerar um ordenamento entre os casos (Crise Epil√©ptica ser "maior" que AVC), estabelece que a dist√¢ncia entre um AVC e a overdose √© a mesma que entre uma overdose e uma crise epil√©ptica. Ainda por cima, alterar a ordem dessa categoriza√ß√£o geraria estimativas com significados e dimens√µes muito diferentes, tornando o modelo de regress√£o linear inst√°vel e pouco confi√°vel. A situa√ß√£o melhora um pouco quando temos apenas dois resultados poss√≠veis:


$$
Y =
\begin{cases}
  0 & \text{se AVC;} \\\\
  1 & \text{se Overdose;} \\\\
\end{cases}
$$

Mesmo se alter√°ssemos os valores, os resultados se manteriam. No entanto, poder√≠amos obter valores estimados para al√©m dos limites 0 e 1, al√©m de obter poucas estimativas para casos mais perto dos valores m√°ximos e m√≠nimos, como mostra a seguinte figura:


```{figure} ../aula6/images/fig4.2.a.png
---
width: 100%
name: reglinclass
align: center
---
Classifica√ß√£o no banco "Default" utilizando uma regress√£o linear. Fonte: James et al. ({cite}`james2023introduction`., p. 139)
```

A {numref}`Figura {number} <reglinclass>` mostra que a regress√£o linear (linha azul) concentra a maior parte dos valores estimados de Y (Probabilidade de *Default*) bem perto de zero. Portanto, pouqu√≠ssimos indiv√≠duos seriam classificados como inadimplentes (ou devedores). Al√©m disso, encontramos probabilidades negativas (o que √© imposs√≠vel) perto de valores de *balance* (Saldo do cart√£o de cr√©dito) menores que 500. No laborat√≥rio de hoje exploraremos um pouco mais desse banco de dados apresentado pelos autores, e tentaremos classificar os adimplentes e inadimplentes utilizando os diversos modelos discutidos no cap√≠tulo.

```{admonition} üí¨ Com a palavra, os autores:
:class: quote
"Para resumir, existem pelo menos duas raz√µes para n√£o realizar classifica√ß√£o utilizando um m√©todo de regress√£o [linear]: (a) um m√©todo de regress√£o n√£o pode acomodar uma resposta qualitativa com mais de duas classes; (b) um m√©todo de regress√£o n√£o fornecer√° estimativas significativas de Pr(Y | X), mesmo com apenas duas classes. Assim, √© prefer√≠vel usar um m√©todo de classifica√ß√£o que seja realmente adequado para valores de resposta qualitativa."
({cite}`james2023introduction`., p. 138, tradu√ß√£o nossa)
```

## A Regress√£o Log√≠stica

Quando temos um resultado bin√°rio (Sim ou n√£o, 0 ou 1), podemos utilizar a regress√£o log√≠stica para modelar a probabilidade de que $Y_i$ pertence a determinada categoria.

$$
Pr(Y_i = 1 | X)
$$

Traduzindo, queremos a probabilidade ($Pr$) de que $Y_i$ perten√ßa a categoria 1 dado ($|$) os valores das vari√°veis preditoras associadas √†quela observa√ß√£o ($X$). No caso do banco de inadimplentes (*Default*), podemos querer saber a probabilidade de que um indiv√≠duo vai ser inadimplente dada suas caracter√≠sticas preditoras (Se √© estudante ou n√£o, renda, d√≠vidas anteriores, etc.). No caso da regress√£o log√≠stica simples (de um √∫nico preditor), podemos pensar somente com rela√ß√£o ao saldo (*Balance*) do cart√£o do indiv√≠duo:


$$
Pr(Inadimplente = Sim | Saldo)
$$

Estimando a mesma rela√ß√£o apresentada na {numref}`Figura {number} <reglinclass>` com um modelo de regress√£o log√≠stica, obtemos o seguinte resultado:

```{figure} ../aula6/images/fig4.2.b.png
---
width: 100%
name: reglogclass
align: center
---
Classifica√ß√£o no banco "Default" utilizando uma regress√£o log√≠stica. Fonte: James et al. ({cite}`james2023introduction`., p. 139)
```

A {numref}`Figura {number} <reglogclass>` mostra que temos uma rela√ß√£o muito mais "limpa" entre o saldo de cart√£o de cr√©dito e os valores estimados para a probabilidade de que seja um inadimplente: N√£o possu√≠mos valores negativos na fun√ß√£o estimada (curva azul), e indiv√≠duos com maior saldo de cart√£o tem maior probabilidade de serem classificados como inadimplentes. Para modelar a $Pr(Y_i = 1 | X)$ na regress√£o log√≠stica, ou $p(X)$ para encurtar, precisamos da **Fun√ß√£o Log√≠stica**, uma das fun√ß√µes que permitem um *output* entre zero e um.


### Fun√ß√£o Log√≠stica

Lembrando que $p(x)$ √© equivalente √† $Pr(Y_i = 1 | X)$, podemos estimar a regress√£o log√≠stica utilizando a seguinte **Fun√ß√£o Log√≠stica**

$$
p(X) = \frac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}}.
$$

Os par√¢metros $\beta_0$ e $\beta_1$ tamb√©m s√£o estimados, assim como na regress√£o linear. A diferen√ßa est√° em como √© feito. Na regress√£o linear, utilizamos o m√©todo de m√≠nimos quadrados ordin√°rios (ou *OLS* em ingl√™s) para estimar os par√¢metros da equa√ß√£o. Aqui, utilizaremos o m√©todo da M√°xima Verossimilihan√ßa, ou *Maximum Likelihood*, que veremos na pr√≥xima subse√ß√£o (e coloquei um v√≠deo complementar para quem tiver interesse). Com um pouco de manipula√ß√£o (segundo os autores, n√£o eu), chegamos em:

$$
\frac{p(X)}{1 - p(X)} = e^{\beta_0 + \beta_1 X}.
$$

O lado esquerdo da equa√ß√£o ($\frac{p(X)}{1 - p(X)}$) √© conhecido por *odds*, e podem ter qualquer valor entre 0 e $\infty$, e quanto maior, maior a probabilidade de Inadimpl√™ncia (no nosso exemplo anterior), e vice-versa. Tirando o logaritmo de ambos os lados, chegamos em.


$$
\log\!\left(\frac{p(X)}{1 - p(X)}\right) = \beta_0 + \beta_1 X.
$$

Que √© o *log odds* ou *logit*, este √∫ltimo que √© muitas vezes usado como sin√¥nimo de regress√£o log√≠stica. Em um modelo de regress√£o log√≠stica, aumentar $X_1$ em uma unidade altera o valor de *log odds* por $\beta_1$. No entanto a rela√ß√£o entre $p(X)$ e $X$ n√£o √© linear, e o quanto $p(x)$ muda com a mudan√ßa de $X$ depende do valor atual de $X$.


Em resumo, a regress√£o log√≠stica transforma a probabilidade de um evento em uma escala que pode ser modelada linearmente, utilizando o logit (ou log odds) como liga√ß√£o entre as vari√°veis explicativas e o resultado. O modelo estima a rela√ß√£o entre os preditores e a chance de ocorr√™ncia de um evento, garantindo que as previs√µes estejam sempre entre 0 e 1. Essa abordagem √© especialmente √∫til para problemas de classifica√ß√£o bin√°ria, pois permite interpretar diretamente o impacto de cada vari√°vel sobre a probabilidade do evento e evita problemas comuns da regress√£o linear, como previs√µes fora do intervalo v√°lido de probabilidades.



### M√©todo de M√°xima Verossimilhan√ßa

Para estimar os par√¢metros $\beta_0$ e $\beta_1$ na equa√ß√£o
 
$$
p(X) = \frac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}}.
$$

√© utilizado o m√©todo de m√°xima verossimilhan√ßa. A intui√ß√£o por tr√°s desse m√©todo √© a de ele procura estimar os par√¢metros $\beta_0$, $\beta_1$,..., $\beta_p$ (para o caso com mais vari√°veis preditoras) tal que a probabilidade $\hat{p}(X_i)$ para cada indiv√≠duo corresponda, da melhor maneira poss√≠vel, √† probabilidade observada $p(x_i)$. Ou seja,


$$
\frac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}} = \hat{p}(X_i) \approx p(X_i) 
$$

Para isso, √© utilizada a seguinte fun√ß√£o de verossimilhan√ßa.

$$
\ell(\beta_0, \beta_1)
     = \prod_{i:\,y_i = 1} p(x_i)\;
       \prod_{i':\,y_{i'} = 0} \!\bigl(1 - p(x_{i'})\bigr).
$$

Em palavras simples, a equa√ß√£o afirma: ‚ÄúPara um conjunto de par√¢metros ($\beta_0, \beta_1$), a verossimilhan√ßa √© o produto da probabilidade prevista do evento em todos os casos que de fato ocorreram ($y = 1$) multiplicado pela probabilidade prevista do n√£o-evento em todos os casos que n√£o ocorreram ($y = 0$).‚Äù


Quando as observa√ß√µes s√£o independentes, a verossimilhan√ßa de um modelo √© obtida multiplicando as probabilidades individuais atribu√≠das a cada dado observado. Aqui, p(x·µ¢) representa a probabilidade calculada pelo modelo (por exemplo, a sa√≠da da regress√£o log√≠stica) de que o i-√©simo indiv√≠duo tenha y = 1. Para cada y·µ¢ = 1, inclu√≠mos p(x·µ¢) no produto; para cada y·µ¢ = 0, inclu√≠mos 1 ‚àí p(x·µ¢). Dessa forma, par√¢metros que atribuem alta probabilidade aos resultados realmente vistos tornam o produto ‚Äì e, portanto, a verossimilhan√ßa ‚Äì maior.


[^1]: **Classificadores** s√£o modelos de aprendizado de m√°quina supervisionado projetados para atribuir exemplos a categorias ou classes distintas com base em suas caracter√≠sticas. Eles s√£o utilizados quando a vari√°vel resposta √© categ√≥rica, como na identifica√ß√£o de sentimentos em textos, classifica√ß√£o de imagens ou detec√ß√£o de spam em e-mails.