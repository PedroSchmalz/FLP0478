# Modelos de L√≠ngua, N-Grams e Classifica√ß√£o com Texto

Na √∫ltima aula, vimos que o problema de classifica√ß√£o exige m√©todos espec√≠ficos quando a vari√°vel resposta √© categ√≥rica, como sentimentos, diagn√≥sticos ou posicionamentos. Discutimos por que a regress√£o linear n√£o √© adequada para esse tipo de tarefa e exploramos alternativas como a regress√£o log√≠stica, que modela probabilidades de forma apropriada. Aprendemos sobre extens√µes da regress√£o log√≠stica para m√∫ltiplos preditores e m√∫ltiplas classes, al√©m de conhecer os modelos generativos, como LDA, QDA e Naive Bayes, que modelam o processo de gera√ß√£o dos dados e utilizam o Teorema de Bayes para estimar probabilidades de pertencimento √†s classes. Por fim, destacamos as vantagens, limita√ß√µes e pressupostos de cada abordagem, refor√ßando a import√¢ncia de escolher o m√©todo mais adequado ao contexto do problema.

Na aula de hoje

## Do N√∫mero para a L√≠ngua, da L√≠ngua para o N√∫mero

Na aula anterior, discutimos como aplicar classificadores (modelos de aprendizado supervisionado para classifica√ß√£o) no contexto de vari√°veis quantitativas num√©ricas: tinhamos caracter√≠sticas quantitativas de indiv√≠duos (renda, saldo do cart√£o, etc.) e quer√≠amos classificar esses indiv√≠duos como potenciais devedores ou n√£o; Ou t√≠nhamos informa√ß√µes de sa√∫de (tamb√©m quantitativas) e quer√≠amos classificar os indiv√≠duos como pessoas com diabetes ou n√£o. No entanto, o objetivo do curso √© ensin√°-los a classificar observa√ß√µes em categorias (favor√°vel/desfavor√°vel, positivo/negativo) de acordo com o conte√∫do textual (conte√∫do de uma publica√ß√£o no *Twitter*). Para isso, precisamos entender como representar a l√≠ngua de maneira computacional. Da√≠, surge a seguinte pergunta: Qual a unidade m√≠nima quando tratamos, computacionalmente, a l√≠ngua? Para Caseli e Nunes (2024, {cite}`caseli_nunes_pln_2024`.), isso depende do seu crit√©rio e finalidade: Na fonologia, ser√° o fonema; na morfologia, o morfema; podemos separar a l√≠ngua em palavras, caract√©res, e assim por diante. Para os prop√≥sitos dessa disciplina, focaremos em uma unidade: O *Token*.



```{admonition} üí¨ Com a palavra, os autores:
:class: quote
"Ao fazer o processamento computacional de textos escritos, a defini√ß√£o de que tipo de unidade de processamento se quer buscar/estudar parece estar atrelada √†s necessidades da tarefa ou trabalho pretendidos. Geralmente, considera-se que uma palavra √©, simplesmente, uma unidade grafol√≥gica delimitada, nas l√≠nguas europeias, entre espa√ßos em branco na representa√ß√£o gr√°fica, ou entre um espa√ßo em branco e um sinal de pontua√ß√£o. Essa √© uma defini√ß√£o bastante concreta, e bastante pr√°tica. No entanto, ao pensarmos em nossos modelos computacionais e suas aplica√ß√µes no mundo, √© importante nos aprofundarmos um pouco mais na conceitua√ß√£o do que √© uma palavra e nas possibilidades de processamento e implica√ß√µes das decis√µes tomadas no pr√©-processamento dos corpora."
({cite}`caseli_nunes_pln_2024`., p. 68, tradu√ß√£o nossa)
```


### Token e Type

*Token* √© um termo que significa qualquer sequ√™ncia de caract√©res √† qual se atribui um valor. Nas l√≠nguas europ√©ias, a sequ√™ncia consiste em caract√©res delimitados por espa√ßos gr√°ficos, e a tokeniza√ß√£o √© ajustada para lidar com sinais de pontua√ß√£o. No entanto, isso n√£o √© verdade para todas as l√≠nguas. Mas, com essa defini√ß√£o, podemos associar o *token* √† palavra escrita. E o *type* seriam os tokens/palavras √∫nicos encontrados em uma frase ou texto. Vamos retomar o que foi discutido na se√ß√£o "*Bag-of-words* e o modelo multinomial" da terceira aula do curso. Vimos que o modelo *bag-of-words* tem como ideia principal a de representar cada documento pelo n√∫mero de vezes que cada palavra aparece nele. No exemplo, t√≠nhamos:

1. O cachorro ama o osso;
2. O Osso ama o cachorro;

Ignorando o artigo "O", ter√≠amos a seguinte matriz *Document-feature*, ou o seguinte BOW (*Bag-of-words*):


<div align="center">

| Documento | Cachorro | ama | Osso |
|-----------|------|-------|----------|
| Doc1      | 1    | 1     | 1        |
| Doc2      | 1    | 1     | 1        |

</div>

Essa matriz √© chamada assim por que cada linha (ou observa√ß√£o) cont√©m um documento/senten√ßa/frase e cada coluna cont√©m a palavra em considera√ß√£o. Por exemplo, a coluna "Cachorro" mostra quantas vezes o termo Cachorro aparece em cada documento. Neste caso, os dois documentos possuem as mesmas palavras, e seriam tratados como "iguais". Isso se deve ao fato de que, nesse tipo de representa√ß√£o, a ordem das palavras e o contexto n√£o s√£o considerados; por isso, embora as duas frases expressem rela√ß√µes diferentes, a Bag-of-Words as representa de forma id√™ntica, pois cont√™m as mesmas palavras com as mesmas contagens. 

"Tokenizar", ou transformar em *tokens* faz com que cada documento (ou observa√ß√£o na matriz *Document-Feature*) ser√° quebrado em suas partes individuais, as palavras. Esse √© o primeiro passo para a cria√ß√£o do saco de palavras (BOW). O exemplo mais comum √© o do tokenizar em n-grams de 1. A frase "Diga n√£o √† vacina√ß√£o obrigat√≥ria!" pode ser quebrada da seguinte forma:


<div align="center">

["Diga","n√£o","√†","vacina√ß√£o","obrigat√≥ria", "!"]

</div>


Seguindo a mesma frase e a mesma etapa de tokeniza√ß√£o em palavras, podemos formar bi-grams, que s√£o pares de palavras consecutivas no texto. Bi-grams ajudam a capturar um pouco de contexto local que o unigram (n=1) n√£o representa, como express√µes fixas, nega√ß√µes e rela√ß√µes imediatas entre termos.

<div align="center">

["Diga_n√£o", "n√£o_√†", "√†_vacina√ß√£o", "vacina√ß√£o_obrigat√≥ria", "obrigat√≥ria_!"]

</div>

Se o objetivo √© capturar resist√™ncia √† vacina√ß√£o, ou hesita√ß√£o vacinal, talvez quebrar o texto em bigramas seja mais informativo do que quebr√°-lo apenas em suas palavras individuais. Com isso, se d√° mais contexto ao modelo de aprendizado de m√°quina, ao custo de adicionar mais combina√ß√µes raras que podem n√£o aparecer tanto em seu banco de dados. üí¨ "Usar ordens maiores de n-gramas pode aumentar substancialmente o n√∫mero de tipos √∫nicos, mas pode ajudar nossa an√°lise textual ao reter mais informa√ß√µes" ({cite}`grimmer2022text`, p. 99, tradu√ß√£o nossa). Novamente, isso n√£o √© uma escolha trivial: Assim como todos os passos e princ√≠pios discutidos nas √∫ltimas aulas, a forma de processar e representar numericamente o texto altera substancialmente os resultados. O pesquisador deve tentar sempre estar consciente dessas escolhas e relat√°-las aos leitores quando necess√°rio. 

### Processamento Morfol√≥gico

Para desenvolver qualquer aplica√ß√£o de PLN, √© necess√°rio realizar fases/etapas que convencionamos chamar de pr√©-processamento do texto. No pr√©-processamento, algumas tarefas usuais s√£o: Segmenta√ß√£o do texto em senten√ßas (Sentencia√ß√£o); Separa√ß√£o de Palavras (tokeniza√ß√£o); tokeniza√ß√£o em subpalavras; normaliza√ß√£o de palavras (lematiza√ß√£o, radicaliza√ß√£o), entre outras. Como as tarefas mais usuais foram discutidas na se√ß√£o "*Bag-of-words* e o modelo multinomial", n√£o iremos repetir o conte√∫do, partindo para os modelos de linguagem.

## Modelos de Linguagem/L√≠ngua

Modelos de linguagem s√£o sistemas matem√°ticos ou computacionais desenvolvidos para representar e analisar padr√µes presentes em textos, fala ou outras formas de comunica√ß√£o. Eles buscam capturar as regularidades estat√≠sticas da l√≠ngua, como a frequ√™ncia de palavras, a probabilidade de sequ√™ncias de termos e as rela√ß√µes contextuais entre diferentes elementos do texto. Esses modelos podem ser aplicados tanto √† linguagem natural quanto a linguagens formais, como c√≥digos ou express√µes matem√°ticas. Em PLN, modelos de linguagem s√£o fundamentais para tarefas como previs√£o da pr√≥xima palavra, an√°lise de sentimentos, tradu√ß√£o autom√°tica e gera√ß√£o de texto, pois permitem transformar a linguagem em representa√ß√µes num√©ricas que podem ser processadas por algoritmos de aprendizado de m√°quina.

Um **Modelo de Linguagem** √© um modelo de aprendizado de m√°quina que faz uma previs√£o sobre as pr√≥ximas palavras. Formalmente, um modelo de linguagem atribui uma probabilidade para cada pr√≥xima palavra poss√≠vel, podendo atribuir probabilidades para frases inteiras. Por exemplo, um modelo de linguagem pode dizer que a seguinte frase possui alta probabilidade:

- "Do nada, percebi tr√™s homens na cal√ßada"

E atribuir√° baixa probabilidade para a seguinte frase:

- "Na cal√ßada tr√™s nada do percebi homens"

Para que precisar√≠amos prever a pr√≥xima palavra? *LLMs* s√£o constru√≠das s√≥ sendo treinadas para prever palavras, e hoje s√£o muito presentes e possuem diversas aplica√ß√µes poss√≠veis. O modelo mais simples de l√≠ngua √© o ***N-Gram***, que √© uma sequ√™ncia de n palavras. Por exemplo, um bigrama (ou *2-gram*) poderia ser:

1. "A √°gua"
2. "O copo"
3. "A vacina"

E trigramas:

1. "Copo de √°gua"
2. "Vacina da Covid"
3. "Presidente do Brasil"

Mas um *N-gram* tamb√©m √© um modelo de probabilidade[^1] que estima a probabilidade de uma palavra dada as n-1 palavras que vem anteriormente.

### *N-Grams*

Come√ßaremos com a tarefa de estimar a $Pr(p|h)$, a probabilidade da palavra $p$ dado o hist√≥rico $h$. Suponha que o hist√≥rico $h$ seja "A praia de Copacabana √© t√£o" e queremos saber a probabilidade de que a pr√≥xima palavra seja "azul". Portanto, queremos estimar:

$$
Pr(Azul | \text{A praia de Copacabana √© t√£o})
$$

Uma forma de estimar essa probabilidade √© por meio da contagem de frequ√™ncias: Dado um c√≥rpus[^2], quantas vezes a frase "A praia de Copacabana √© t√£o" √© seguida por "Azul".

$$
 Pr(\text{blue} | \text{A praia de Copacabana √© t√£o}) \;=\;
\frac{C(\text{A praia de Copacabana √© t√£o Azul})}
     {C(\text{A praia de Copacabana √© t√£o})} 
$$

No entanto, nenhum c√≥rpus ser√° t√£o grande a ponto de nos dar boas estimativas para essa probabilidade. Isso se deve ao fato da L√≠ngua e a Linguagem serem criativas, e novas frases s√£o criadas o tempo todo. Por isso, outra forma de estimar a probabilidade √© necess√°ria. Uma forma de estimar essa probabilidade √© por meio da *Chain Rule of Probability* (Ou Regra Geral do Produto/Cadeia, em portugu√™s). Aplicando ela para palavras ($p$), temos:

$$
Pr(P_1, ..., P_n) = Pr(P_1) P(P_2|P_1) P(X_3|X_{1:2}) ... P(X_n|X_{1:n-1}) 
$$

De forma geral:

$$
\prod_{k=1}^{n} P\bigl(p_k \,\bigl|\, p_{1{:}k-1}\bigr)
$$

Ou seja, podemos estimar a probabilidade conjunta de uma frase inteira por meio da multiplica√ß√£o das probabilidades condicionais que a comp√µem. Dito de outra forma, a regra geral do produto diz que podemos calcular a probabilidade de uma frase multiplicando as probabilidades de cada palavra aparecer, considerando as palavras anteriores. Assim, mesmo sem ter todas as frases no nosso banco de dados, conseguimos estimar a chance de uma sequ√™ncia de palavras acontecer. No entanto, como calcular cada probabilidade condicional (e.g. $Pr(P_2|P_1)$)?

### A Suposi√ß√£o de Markov

A intui√ß√£o por tr√°s do modelo *N-gram* √© de que, ao inv√©s de computar a $Pr(p|h)$, podemos aproximar o hist√≥rico $h$ s√≥ com as √∫ltimas palavras.  O modelo de bigrama, por exemplo, aproxima a probabilidade de uma palavra dada todas as palavras anteriores $Pr(p_n|p_{1:n-1})$ usando a probabilidade condicional da palavra anterior $Pr(p_n|p_{n-1})$. Ou seja, no lugar de estimar


$$
Pr(Azul | \text{A praia de Copacabana √© t√£o})
$$


Ele aproxima $h$ por meio da probabilidade:

$$
Pr(Azul | t√£o)
$$

De maneira geral, a seguinte aproxima√ß√£o √© feita

$$
Pr(p_n|p_{1:n-1}) \approx Pr(p_n|p_{n-1})
$$

Esse pressuposto, ou suposi√ß√£o, de que a probabilidade de uma palavra depende apenas da palavra anteiror √© chamado de **Suposi√ß√£o de Markov**. Modelos de Markov s√£o uma classe de modelos probabil√≠sticos que assumem que podemos prever a probabilidade de uma unidade futura sem olhar muito distante no passado. Portanto, a probabilidade de uma frase inteira pode ser estimada por

$$
Pr(p_{1:n}) \approx \prod_{k=1}^{n} Pr(p_k|p_{k-1})
$$

### Como estimar as probabilidades?

Uma forma de estimar essas probabilidades que utilizam a suposi√ß√£o de Markov √© chamada de *Maximum Likelihood Estimation*, ou **M√©todo de Estima√ß√£o de M√°xima Verossimilhan√ßa**. Em texto, conseguiremos as estimativas em um modelo *n-gram* pegando contagens de um c√≥rpus, e normalizando[^3] essas contagens para que fiquem entre 0 e 1. Por exemplo, para computar a probabilidade de um bigrama de uma palavra $p_n$ dada uma palavra anterior $p_{n-1}$, se computa a contagem de um bigrama $C(p_{n-1} p_{n})$ e normalizar essa contagem pela soma de todo os bigramas que compartilham a primeira palavra $p_{n-1}$:


$$
P\bigl(p_n \,\bigl|\, p_{n-1}\bigr)
   \;=\;
   \frac{C\!\bigl(p_{n-1}p_n\bigr)}
        {\displaystyle\sum_{p} C\!\bigl(p_{n-1}p\bigr)}
$$

Vamos trabalhar alguns exemplos de c√°lculos de probabilidade s√≥ para entender como funciona (<f> e </f> indicam o come√ßo e o fim de uma frase):
 
$$
<f> \text{Eu sou Jo√£o} </f>
$$

$$
<f> \text{Jo√£o sou eu} </f>
$$

$$
<f> \text{Eu n√£o gosto de sopa} </f>
$$


Se esse fosse nosso c√≥rpus inteiro, poder√≠amos computar as seguinte probabilidades para os bigramas:

$$
Pr (Eu | <f>) = 2/3 \approx 67%
$$

Aqui, queremos a probabilidade de que a frase come√ße com "Eu". Em dois casos (1 e 3 frases), isso ocorre. Obtemos, ent√£o, uma probabilidade de aproximadamente 67%. Para a frase abaixo

$$
Pr (Jo√£o | <f>) = 1/3
$$

Queremos a probabilidade de que a frase come√ße com "Jo√£o". S√≥ uma frase das tr√™s come√ßa, portanto a probabilidade √© de 1/3, ou aproximadamente $33\%$. No caso geral, a estima√ß√£o de param√™tros no *MLE* (Maximum Likelihood Estimation) em n-grams fica:

$$
P\bigl(w_n \,\bigl|\, w_{\,n-N+1{:}n-1}\bigr)
   \;=\;
   \frac{C\!\bigl(w_{\,n-N+1{:}n-1}\,w_n\bigr)}
        {C\!\bigl(w_{\,n-N+1{:}n-1}\bigr)}
$$

Os modelos probabil√≠sticos, como os n-grams, permitem calcular a probabilidade de diferentes sequ√™ncias de palavras em uma l√≠ngua, atribuindo valores a cada poss√≠vel continua√ß√£o de uma frase com base nas ocorr√™ncias observadas em um c√≥rpus. Esse c√°lculo √© fundamental para tarefas de gera√ß√£o autom√°tica de texto, pois possibilita escolher, a cada etapa, a palavra mais prov√°vel para continuar a senten√ßa. O processo de busca gulosa utiliza exatamente essa ideia: a cada passo da gera√ß√£o, seleciona a palavra com maior probabilidade condicional, formando senten√ßas que refletem os padr√µes estat√≠sticos aprendidos pelo modelo. Assim, a busca gulosa √© uma estrat√©gia pr√°tica que conecta diretamente os c√°lculos probabil√≠sticos dos modelos de linguagem √† produ√ß√£o de frases coerentes e naturais. A {numref}`Figura {number} <figgulosa>` ilustra o processo de gera√ß√£o de senten√ßas em um processo de "Busca Gulosa", um dos poss√≠veis dentre v√°rios.


```{figure} ../aula7/images/fig17.1.png
---
width: 100%
name: figgulosa
align: center
---
Exemplo de gera√ß√£o de senten√ßa em um processo de "Busca Gulosa". Fonte Caseli e Nunes (p.369, {cite}`caseli_nunes_pln_2024`.)
```

Compreender os modelos de linguagem n-gram e os modelos probabil√≠sticos apresentados nesta aula √© fundamental para realizar tarefas de classifica√ß√£o com texto em Processamento de Linguagem Natural. Esses modelos permitem transformar textos em representa√ß√µes num√©ricas que capturam padr√µes de frequ√™ncia, contexto e depend√™ncia entre palavras, tornando poss√≠vel aplicar algoritmos de aprendizado de m√°quina para identificar categorias, sentimentos ou t√≥picos em documentos.

O modelo n-gram, ao considerar sequ√™ncias de palavras, vai al√©m da simples contagem individual de termos, incorporando informa√ß√µes sobre o contexto imediato e rela√ß√µes locais entre palavras. Isso √© especialmente √∫til para distinguir nuances de significado, identificar express√µes fixas e melhorar a precis√£o dos classificadores. J√° os modelos probabil√≠sticos, ao estimar a probabilidade de ocorr√™ncia de sequ√™ncias de palavras, fornecem uma base estat√≠stica s√≥lida para a tomada de decis√£o em tarefas de classifica√ß√£o, seja para prever a pr√≥xima palavra, identificar o sentimento de um texto ou categorizar documentos.

Ao dominar esses conceitos, o pesquisador consegue construir representa√ß√µes mais informativas dos textos, escolher as melhores estrat√©gias de pr√©-processamento e selecionar modelos adequados para diferentes problemas de classifica√ß√£o. Dessa forma, o entendimento dos modelos de linguagem n-gram e probabil√≠sticos √© um passo essencial para o desenvolvimento de solu√ß√µes eficazes e interpret√°veis em PLN.

### Pr√≥ximos Passos e Modelos Neurais

Os modelos probabil√≠sticos de linguagem, como os n-grams, inauguraram a ideia de atribuir uma probabilidade expl√≠cita a cada sequ√™ncia de palavras por meio da regra da cadeia e da suposi√ß√£o de Markov. Esse enquadramento mostrou que textos podiam ser convertidos em contagens normalizadas e treinados por m√°xima verossimilhan√ßa, introduzindo m√©tricas como perplexidade para avaliar a qualidade do modelo. Quando surgiram os modelos neurais, eles mantiveram o mesmo objetivo de estimar $Pr(p|h)$, mas trocaram tabelas esparsas por vetores densos e par√¢metros aprendidos, conseguindo generalizar para contextos mais longos e lidar melhor com dados raros. Entender essa heran√ßa probabil√≠stica nos faz perceber que mesmo as arquiteturas mais recentes continuam, no essencial, sendo m√°quinas de previs√£o de sequ√™ncias. Um princ√≠pio que permanece nos grandes modelos de linguagem atuais, como os *LLMs*.



## Regress√£o Log√≠stica e Classifica√ß√£o com Texto

Discutimos ao longo do curso que a tarefa de classifica√ß√£o envolve pegar preditores $X$ (ou *features*) e utiliz√°-los para tentar prever √† que categoria $Y$, nosso *target* pertence. Para a classifica√ß√£o com texto, a principal mudan√ßa √© a de que n√£o utilizaremos mais dados num√©ricos (Saldo do cart√£o, vari√°veis de sa√∫de), e sim **Texto**, pr√©-processado e representado numericamente. Um exemplo cl√°ssico dentro da classifica√ß√£o com texto √© a de verificar se uma avalia√ß√£o de um produto √© positiva ou negativa. Isso √© uma tarefa dentro da √°rea de An√°lise de Sentimentos, e as categorias podem variar um pouco. Nessa tarefa, pegamos o texto da avalia√ß√£o {"O produto √© muito bom!", "N√£o gostei nem um pouco"} e tentaremos classificar eles como positivos ou negativos. Como vimos antes, um classificador b√°sico √© a regress√£o log√≠stica, e ela servir√° de base para entendermos o mecanismo por tr√°s dos modelos de aprendizado supervisionado com texto.


### Regress√£o Log√≠stica

No ISL ({cite}`james2023introduction`.), os autores explicam a Regress√£o Log√≠stica por uma linguamge mais estat√≠stica. J√° Jurafsky e Martin ({cite}`jurafsky2024speech`.) v√£o por uma linha mais do Aprendizado de M√°quina. Por isso, n√£o estranhem a mudan√ßa de jarg√£o e de termos para se referir √† Regress√£o Log√≠stica. 

#### Fun√ß√£o Sigm√≥ide

Como dito antes, o objetivo da regress√£o log√≠stica bin√°ria √© treinar um classificador capaz de tomar uma decis√£o bin√°ria sobre a classe de uma nova observa√ß√£o de entrada. Aqui introduzimos o classificador sigmoide, que nos ajudar√° a tomar essa decis√£o. Em Jurafsky e Martin, a Regress√£o log√≠stica resolve o problema de estimar $Pr(Y=1|X)$ (A probabilidade de que a observa√ß√£o pertence √† classe 1, dado os preditores) estimando um vetor de **pesos** e **termos de vi√©s**. Vamos chamar os pesos de $w$ e o vi√©s de $b$. Cada peso $w_i$ √© um n√∫mero real e est√° associado √† um dos preditores $X_i$. O peso representa qu√£o importante √© cada preditor para a decis√£o de classifica√ß√£o, podendo ser negativo ou positivo. Em uma tarefa de classifica√ß√£o de sentimento, prov√°velmente a palavra "√≥timo" ter√° um peso positivo, e a palavra "horr√≠vel" ter√° um peso negativo. O termo de vi√©s, ou intercepto, √© um n√∫mero real que √© adicionado aos *inputs* ao final do c√°lculo. Para fazer uma decis√£o em uma observa√ß√£o de teste (ap√≥s o treinamento), o classificador log√≠stico vai multiplicar cada preditor $X_i$ pelo seu peso $w_i$, e somar isso com o termo de vi√©s. Formalmente, temos:

$$
z \;=\; \left(\sum_{i=1}^{n} w_i x_i\right) + b
$$

No entanto, os valores de $z$ resultantes dessa equa√ß√£o n√£o est√£o obrigatoriamente entre 0 e 1. Para que possamos calcular a probabilidade de que uma observa√ß√£o √© da classe 0 ou 1, precisamos limitar z para esse intervalo. Para isso, passaremos z pela fun√ß√£o sigm√≥ide $\sigma$, ou fun√ß√£o log√≠stica.

$$
\sigma(z)
   \;=\;
   \frac{1}{1 + e^{-z}}
   \;=\;
   \frac{1}{1 + \exp(-z)}
$$







## Notas

[^1]: O termo **n-gram** pode ser usado em dois sentidos: (1) como uma sequ√™ncia de n itens (palavras, caracteres, etc.) extra√≠da de um texto, e (2) como um modelo de linguagem que estima a probabilidade de uma palavra ou sequ√™ncia com base nas n-1 palavras anteriores. O contexto geralmente indica qual sentido est√° sendo utilizado.

[^2]: O termo **c√≥rpus** refere-se a um conjunto estruturado de textos ou documentos utilizados para an√°lise lingu√≠stica ou treinamento de modelos de linguagem. Em PLN, o c√≥rpus serve como fonte de dados para extrair padr√µes, calcular frequ√™ncias e estimar probabilidades, sendo fundamental para o desenvolvimento e avalia√ß√£o de m√©todos computacionais aplicados √† linguagem. Aqui, estamos indo para al√©m da ideia de um c√≥rpus anotado.

[^3]: **Normalizar** significa ajustar os valores de uma vari√°vel ou conjunto de dados para que fiquem dentro de um intervalo padr√£o, geralmente entre 0 e 1. No contexto de modelos de linguagem, normalizar as contagens transforma frequ√™ncias absolutas em probabilidades, facilitando a compara√ß√£o e o processamento estat√≠stico dos dados.

