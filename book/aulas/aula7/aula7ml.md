# Classifica√ß√£o com texto

Na √∫ltima aula, vimos que o problema de classifica√ß√£o exige m√©todos espec√≠ficos quando a vari√°vel resposta √© categ√≥rica, como sentimentos, diagn√≥sticos ou posicionamentos. Discutimos por que a regress√£o linear n√£o √© adequada para esse tipo de tarefa e exploramos alternativas como a regress√£o log√≠stica, que modela probabilidades de forma apropriada. Aprendemos sobre extens√µes da regress√£o log√≠stica para m√∫ltiplos preditores e m√∫ltiplas classes, al√©m de conhecer os modelos generativos, como LDA, QDA e Naive Bayes, que modelam o processo de gera√ß√£o dos dados e utilizam o Teorema de Bayes para estimar probabilidades de pertencimento √†s classes. Por fim, destacamos as vantagens, limita√ß√µes e pressupostos de cada abordagem, refor√ßando a import√¢ncia de escolher o m√©todo mais adequado ao contexto do problema.

Na aula de hoje, iremos discutir como utilizar esses modelos para a classifica√ß√£o supervisionada **com texto**. Para isso, precisamos entender como representar a l√≠ngua de maneira computacional, e qual ser√° nossa unidade m√≠nima de an√°lise neste caso. Veremos o que s√£o modelos de linguagem e como nos ajudam para esta tarefa, discutindo o uso de *n-grams*, como prever a probabilidade da pr√≥xima palavra em uma frase, etc.


## Do N√∫mero para a L√≠ngua, da L√≠ngua para o N√∫mero

Na aula anterior, discutimos como aplicar classificadores (modelos de aprendizado supervisionado para classifica√ß√£o) no contexto de vari√°veis quantitativas num√©ricas: tinhamos caracter√≠sticas quantitativas de indiv√≠duos (renda, saldo do cart√£o, etc.) e quer√≠amos classificar esses indiv√≠duos como potenciais devedores ou n√£o; Ou t√≠nhamos informa√ß√µes de sa√∫de (tamb√©m quantitativas) e quer√≠amos classificar os indiv√≠duos como pessoas com diabetes ou n√£o. No entanto, o objetivo do curso √© ensin√°-los a classificar observa√ß√µes em categorias (favor√°vel/desfavor√°vel, positivo/negativo) de acordo com o conte√∫do textual (conte√∫do de uma publica√ß√£o no *Twitter*). Para isso, precisamos entender como representar a l√≠ngua de maneira computacional. Da√≠, surge a seguinte pergunta: Qual a unidade m√≠nima quando tratamos, computacionalmente, a l√≠ngua? Para Caseli e Nunes (2024, {cite}`caseli_nunes_pln_2024`.), depende do seu crit√©rio e finalidade: Na fonologia, ser√° o fonema; na morfologia, o morfema; podemos separar a l√≠ngua em palavras, caract√©res, e assim por diante. Para os prop√≥sitos dessa disciplina, focaremos em uma unidade: O *Token*.



```{admonition} üí¨ Com a palavra, os autores:
:class: quote
"Ao fazer o processamento computacional de textos escritos, a defini√ß√£o de que tipo de unidade de processamento se quer buscar/estudar parece estar atrelada √†s necessidades da tarefa ou trabalho pretendidos. Geralmente, considera-se que uma palavra √©, simplesmente, uma unidade grafol√≥gica delimitada, nas l√≠nguas europeias, entre espa√ßos em branco na representa√ß√£o gr√°fica, ou entre um espa√ßo em branco e um sinal de pontua√ß√£o. Essa √© uma defini√ß√£o bastante concreta, e bastante pr√°tica. No entanto, ao pensarmos em nossos modelos computacionais e suas aplica√ß√µes no mundo, √© importante nos aprofundarmos um pouco mais na conceitua√ß√£o do que √© uma palavra e nas possibilidades de processamento e implica√ß√µes das decis√µes tomadas no pr√©-processamento dos corpora."
({cite}`caseli_nunes_pln_2024`., p. 68, tradu√ß√£o nossa)
```


### Token e Type

*Token* √© um termo que significa qualquer sequ√™ncia de caract√©res √† qual se atribui um valor. Nas l√≠nguas europ√©ias, a sequ√™ncia consiste em caract√©res delimitados por espa√ßos gr√°ficos, e a tokeniza√ß√£o √© ajustada para lidar com sinais de pontua√ß√£o. No entanto, isso n√£o √© verdade para todas as l√≠nguas. Mas, com essa defini√ß√£o, podemos associar o *token* √† palavra escrita. E o *type* seriam os tokens/palavras √∫nicos encontrados em uma frase ou texto. Vamos retomar o que foi discutido na se√ß√£o "*Bag-of-words* e o modelo multinomial" da terceira aula do curso. Vimos que o modelo *bag-of-words* tem como ideia principal a de representar cada documento pelo n√∫mero de vezes que cada palavra aparece nele. No exemplo, t√≠nhamos:

1. O cachorro ama o osso;
2. O Osso ama o cachorro;

Ignorando o artigo "O", ter√≠amos a seguinte matriz *Document-feature*, ou o seguinte BOW (*Bag-of-words*):


<div align="center">

| Documento | Cachorro | ama | Osso |
|-----------|------|-------|----------|
| Doc1      | 1    | 1     | 1        |
| Doc2      | 1    | 1     | 1        |

</div>

Essa matriz √© chamada assim por que cada linha (ou observa√ß√£o) cont√©m um documento/senten√ßa/frase e cada coluna cont√©m a palavra em considera√ß√£o. Por exemplo, a coluna "Cachorro" mostra quantas vezes o termo Cachorro aparece em cada documento. Neste caso, os dois documentos possuem as mesmas palavras, e seriam tratados como "iguais". Isso se deve ao fato de que, nesse tipo de representa√ß√£o, a ordem das palavras e o contexto n√£o s√£o considerados; por isso, embora as duas frases expressem rela√ß√µes diferentes, a Bag-of-Words as representa de forma id√™ntica, pois cont√™m as mesmas palavras com as mesmas contagens. 

"Tokenizar", ou transformar em *tokens* faz com que cada documento (ou observa√ß√£o na matriz *Document-Feature*) ser√° quebrado em suas partes individuais, as palavras. Esse √© o primeiro passo para a cria√ß√£o do saco de palavras (BOW). O exemplo mais comum √© o do tokenizar em n-grams de 1. A frase "Diga n√£o √† vacina√ß√£o obrigat√≥ria!" pode ser quebrada da seguinte forma:


<div align="center">

["Diga","n√£o","√†","vacina√ß√£o","obrigat√≥ria", "!"]

</div>


Seguindo a mesma frase e a mesma etapa de tokeniza√ß√£o em palavras, podemos formar bi-grams, que s√£o pares de palavras consecutivas no texto. Bi-grams ajudam a capturar um pouco de contexto local que o unigram (n=1) n√£o representa, como express√µes fixas, nega√ß√µes e rela√ß√µes imediatas entre termos.

<div align="center">

["Diga_n√£o", "n√£o_√†", "√†_vacina√ß√£o", "vacina√ß√£o_obrigat√≥ria", "obrigat√≥ria_!"]

</div>

Se o objetivo √© capturar resist√™ncia √† vacina√ß√£o, ou hesita√ß√£o vacinal, talvez quebrar o texto em bigramas seja mais informativo do que quebr√°-lo apenas em suas palavras individuais. Com isso, se d√° mais contexto ao modelo de aprendizado de m√°quina, ao custo de adicionar mais combina√ß√µes raras que podem n√£o aparecer tanto em seu banco de dados. üí¨ "Usar ordens maiores de n-gramas pode aumentar substancialmente o n√∫mero de tipos √∫nicos, mas pode ajudar nossa an√°lise textual ao reter mais informa√ß√µes" ({cite}`grimmer2022text`, p. 99, tradu√ß√£o nossa). Novamente, isso n√£o √© uma escolha trivial: Assim como todos os passos e princ√≠pios discutidos nas √∫ltimas aulas, a forma de processar e representar numericamente o texto altera substancialmente os resultados. O pesquisador deve tentar sempre estar consciente dessas escolhas e relat√°-las aos leitores quando necess√°rio. 

### Processamento Morfol√≥gico em PLN

Para desenvolver qualquer aplica√ß√£o de PLN, √© necess√°rio realizar fases/etapas que convencionamos chamar de pr√©-processamento do texto. No pr√©-processamento, algumas tarefas usuais s√£o: Segmenta√ß√£o do texto em senten√ßas (Sentencia√ß√£o); Separa√ß√£o de Palavras (tokeniza√ß√£o); tokeniza√ß√£o em subpalavras; normaliza√ß√£o de palavras (lematiza√ß√£o, radicaliza√ß√£o), entre outras. Como as tarefas mais usuais foram discutidas na se√ß√£o "*Bag-of-words* e o modelo multinomial", n√£o iremos repetir o conte√∫do, partindo para os modelos de linguagem.



