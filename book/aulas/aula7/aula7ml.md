# Modelos de L√≠ngua, N-Grams e Classifica√ß√£o com Texto

Na √∫ltima aula, vimos que o problema de classifica√ß√£o exige m√©todos espec√≠ficos quando a vari√°vel resposta √© categ√≥rica, como sentimentos, diagn√≥sticos ou posicionamentos. Discutimos por que a regress√£o linear n√£o √© adequada para esse tipo de tarefa e exploramos alternativas como a regress√£o log√≠stica, que modela probabilidades de forma apropriada. Aprendemos sobre extens√µes da regress√£o log√≠stica para m√∫ltiplos preditores e m√∫ltiplas classes, al√©m de conhecer os modelos generativos, como LDA, QDA e Naive Bayes, que modelam o processo de gera√ß√£o dos dados e utilizam o Teorema de Bayes para estimar probabilidades de pertencimento √†s classes. Por fim, destacamos as vantagens, limita√ß√µes e pressupostos de cada abordagem, refor√ßando a import√¢ncia de escolher o m√©todo mais adequado ao contexto do problema.

Na aula de hoje

## Do N√∫mero para a L√≠ngua, da L√≠ngua para o N√∫mero

Na aula anterior, discutimos como aplicar classificadores (modelos de aprendizado supervisionado para classifica√ß√£o) no contexto de vari√°veis quantitativas num√©ricas: tinhamos caracter√≠sticas quantitativas de indiv√≠duos (renda, saldo do cart√£o, etc.) e quer√≠amos classificar esses indiv√≠duos como potenciais devedores ou n√£o; Ou t√≠nhamos informa√ß√µes de sa√∫de (tamb√©m quantitativas) e quer√≠amos classificar os indiv√≠duos como pessoas com diabetes ou n√£o. No entanto, o objetivo do curso √© ensin√°-los a classificar observa√ß√µes em categorias (favor√°vel/desfavor√°vel, positivo/negativo) de acordo com o conte√∫do textual (conte√∫do de uma publica√ß√£o no *Twitter*). Para isso, precisamos entender como representar a l√≠ngua de maneira computacional. Da√≠, surge a seguinte pergunta: Qual a unidade m√≠nima quando tratamos, computacionalmente, a l√≠ngua? Para Caseli e Nunes (2024, {cite}`caseli_nunes_pln_2024`.), isso depende do seu crit√©rio e finalidade: Na fonologia, ser√° o fonema; na morfologia, o morfema; podemos separar a l√≠ngua em palavras, caract√©res, e assim por diante. Para os prop√≥sitos dessa disciplina, focaremos em uma unidade: O *Token*.



```{admonition} üí¨ Com a palavra, os autores:
:class: quote
"Ao fazer o processamento computacional de textos escritos, a defini√ß√£o de que tipo de unidade de processamento se quer buscar/estudar parece estar atrelada √†s necessidades da tarefa ou trabalho pretendidos. Geralmente, considera-se que uma palavra √©, simplesmente, uma unidade grafol√≥gica delimitada, nas l√≠nguas europeias, entre espa√ßos em branco na representa√ß√£o gr√°fica, ou entre um espa√ßo em branco e um sinal de pontua√ß√£o. Essa √© uma defini√ß√£o bastante concreta, e bastante pr√°tica. No entanto, ao pensarmos em nossos modelos computacionais e suas aplica√ß√µes no mundo, √© importante nos aprofundarmos um pouco mais na conceitua√ß√£o do que √© uma palavra e nas possibilidades de processamento e implica√ß√µes das decis√µes tomadas no pr√©-processamento dos corpora."
({cite}`caseli_nunes_pln_2024`., p. 68, tradu√ß√£o nossa)
```


### Token e Type

*Token* √© um termo que significa qualquer sequ√™ncia de caract√©res √† qual se atribui um valor. Nas l√≠nguas europ√©ias, a sequ√™ncia consiste em caract√©res delimitados por espa√ßos gr√°ficos, e a tokeniza√ß√£o √© ajustada para lidar com sinais de pontua√ß√£o. No entanto, isso n√£o √© verdade para todas as l√≠nguas. Mas, com essa defini√ß√£o, podemos associar o *token* √† palavra escrita. E o *type* seriam os tokens/palavras √∫nicos encontrados em uma frase ou texto. Vamos retomar o que foi discutido na se√ß√£o "*Bag-of-words* e o modelo multinomial" da terceira aula do curso. Vimos que o modelo *bag-of-words* tem como ideia principal a de representar cada documento pelo n√∫mero de vezes que cada palavra aparece nele. No exemplo, t√≠nhamos:

1. O cachorro ama o osso;
2. O Osso ama o cachorro;

Ignorando o artigo "O", ter√≠amos a seguinte matriz *Document-feature*, ou o seguinte BOW (*Bag-of-words*):


<div align="center">

| Documento | Cachorro | ama | Osso |
|-----------|------|-------|----------|
| Doc1      | 1    | 1     | 1        |
| Doc2      | 1    | 1     | 1        |

</div>

Essa matriz √© chamada assim por que cada linha (ou observa√ß√£o) cont√©m um documento/senten√ßa/frase e cada coluna cont√©m a palavra em considera√ß√£o. Por exemplo, a coluna "Cachorro" mostra quantas vezes o termo Cachorro aparece em cada documento. Neste caso, os dois documentos possuem as mesmas palavras, e seriam tratados como "iguais". Isso se deve ao fato de que, nesse tipo de representa√ß√£o, a ordem das palavras e o contexto n√£o s√£o considerados; por isso, embora as duas frases expressem rela√ß√µes diferentes, a Bag-of-Words as representa de forma id√™ntica, pois cont√™m as mesmas palavras com as mesmas contagens. 

"Tokenizar", ou transformar em *tokens* faz com que cada documento (ou observa√ß√£o na matriz *Document-Feature*) ser√° quebrado em suas partes individuais, as palavras. Esse √© o primeiro passo para a cria√ß√£o do saco de palavras (BOW). O exemplo mais comum √© o do tokenizar em n-grams de 1. A frase "Diga n√£o √† vacina√ß√£o obrigat√≥ria!" pode ser quebrada da seguinte forma:


<div align="center">

["Diga","n√£o","√†","vacina√ß√£o","obrigat√≥ria", "!"]

</div>


Seguindo a mesma frase e a mesma etapa de tokeniza√ß√£o em palavras, podemos formar bi-grams, que s√£o pares de palavras consecutivas no texto. Bi-grams ajudam a capturar um pouco de contexto local que o unigram (n=1) n√£o representa, como express√µes fixas, nega√ß√µes e rela√ß√µes imediatas entre termos.

<div align="center">

["Diga_n√£o", "n√£o_√†", "√†_vacina√ß√£o", "vacina√ß√£o_obrigat√≥ria", "obrigat√≥ria_!"]

</div>

Se o objetivo √© capturar resist√™ncia √† vacina√ß√£o, ou hesita√ß√£o vacinal, talvez quebrar o texto em bigramas seja mais informativo do que quebr√°-lo apenas em suas palavras individuais. Com isso, se d√° mais contexto ao modelo de aprendizado de m√°quina, ao custo de adicionar mais combina√ß√µes raras que podem n√£o aparecer tanto em seu banco de dados. üí¨ "Usar ordens maiores de n-gramas pode aumentar substancialmente o n√∫mero de tipos √∫nicos, mas pode ajudar nossa an√°lise textual ao reter mais informa√ß√µes" ({cite}`grimmer2022text`, p. 99, tradu√ß√£o nossa). Novamente, isso n√£o √© uma escolha trivial: Assim como todos os passos e princ√≠pios discutidos nas √∫ltimas aulas, a forma de processar e representar numericamente o texto altera substancialmente os resultados. O pesquisador deve tentar sempre estar consciente dessas escolhas e relat√°-las aos leitores quando necess√°rio. 

### Processamento Morfol√≥gico

Para desenvolver qualquer aplica√ß√£o de PLN, √© necess√°rio realizar fases/etapas que convencionamos chamar de pr√©-processamento do texto. No pr√©-processamento, algumas tarefas usuais s√£o: Segmenta√ß√£o do texto em senten√ßas (Sentencia√ß√£o); Separa√ß√£o de Palavras (tokeniza√ß√£o); tokeniza√ß√£o em subpalavras; normaliza√ß√£o de palavras (lematiza√ß√£o, radicaliza√ß√£o), entre outras. Como as tarefas mais usuais foram discutidas na se√ß√£o "*Bag-of-words* e o modelo multinomial", n√£o iremos repetir o conte√∫do, partindo para os modelos de linguagem.

## Modelos de Linguagem/L√≠ngua

Modelos de linguagem s√£o sistemas matem√°ticos ou computacionais desenvolvidos para representar e analisar padr√µes presentes em textos, fala ou outras formas de comunica√ß√£o. Eles buscam capturar as regularidades estat√≠sticas da l√≠ngua, como a frequ√™ncia de palavras, a probabilidade de sequ√™ncias de termos e as rela√ß√µes contextuais entre diferentes elementos do texto. Esses modelos podem ser aplicados tanto √† linguagem natural quanto a linguagens formais, como c√≥digos ou express√µes matem√°ticas. Em PLN, modelos de linguagem s√£o fundamentais para tarefas como previs√£o da pr√≥xima palavra, an√°lise de sentimentos, tradu√ß√£o autom√°tica e gera√ß√£o de texto, pois permitem transformar a linguagem em representa√ß√µes num√©ricas que podem ser processadas por algoritmos de aprendizado de m√°quina.

Um **Modelo de Linguagem** √© um modelo de aprendizado de m√°quina que faz uma previs√£o sobre as pr√≥ximas palavras. Formalmente, um modelo de linguagem atribui uma probabilidade para cada pr√≥xima palavra poss√≠vel, podendo atribuir probabilidades para frases inteiras. Por exemplo, um modelo de linguagem pode dizer que a seguinte frase possui alta probabilidade:

- "Do nada, percebi tr√™s homens na cal√ßada"

E atribuir√° baixa probabilidade para a seguinte frase:

- "Na cal√ßada tr√™s nada do percebi homens"

Para que precisar√≠amos prever a pr√≥xima palavra? *LLMs* s√£o constru√≠das s√≥ sendo treinadas para prever palavras, e hoje s√£o muito presentes e possuem diversas aplica√ß√µes poss√≠veis. O modelo mais simples de l√≠ngua √© o ***N-Gram***, que √© uma sequ√™ncia de n palavras. Por exemplo, um bigrama (ou *2-gram*) poderia ser:

1. "A √°gua"
2. "O copo"
3. "A vacina"

E trigramas:

1. "Copo de √°gua"
2. "Vacina da Covid"
3. "Presidente do Brasil"

Mas um *N-gram* tamb√©m √© um modelo de probabilidade[^1] que estima a probabilidade de uma palavra dada as n-1 palavras que vem anteriormente.

### *N-Grams*

Come√ßaremos com a tarefa de estimar a $Pr(p|h)$, a probabilidade da palavra $p$ dado o hist√≥rico $h$. Suponha que o hist√≥rico $h$ seja "A praia de Copacabana √© t√£o" e queremos saber a probabilidade de que a pr√≥xima palavra seja "azul". Portanto, queremos estimar:

$$
Pr(Azul | \text{A praia de Copacabana √© t√£o})
$$

Uma forma de estimar essa probabilidade √© por meio da contagem de frequ√™ncias: Dado um c√≥rpus[^2], quantas vezes a frase "A praia de Copacabana √© t√£o" √© seguida por "Azul".

$$
 Pr(\text{blue} | \text{A praia de Copacabana √© t√£o}) \;=\;
\frac{C(\text{A praia de Copacabana √© t√£o Azul})}
     {C(\text{A praia de Copacabana √© t√£o})} 
$$

No entanto, nenhum c√≥rpus ser√° t√£o grande a ponto de nos dar boas estimativas para essa probabilidade. Isso se deve ao fato da L√≠ngua e a Linguagem serem criativas, e novas frases s√£o criadas o tempo todo. Por isso, outra forma de estimar a probabilidade √© necess√°ria. Uma forma de estimar essa probabilidade √© por meio da *Chain Rule of Probability* (Ou Regra Geral do Produto/Cadeia, em portugu√™s). Aplicando ela para palavras ($p$), temos:

$$
Pr(P_1, ..., P_n) = Pr(P_1) P(P_2|P_1) P(X_3|X_{1:2}) ... P(X_n|X_{1:n-1}) 
$$

De forma geral:

$$
\prod_{k=1}^{n} P\bigl(p_k \,\bigl|\, p_{1{:}k-1}\bigr)
$$

Ou seja, podemos estimar a probabilidade conjunta de uma frase inteira por meio da multiplica√ß√£o das probabilidades condicionais que a comp√µem. Dito de outra forma, a regra geral do produto diz que podemos calcular a probabilidade de uma frase multiplicando as probabilidades de cada palavra aparecer, considerando as palavras anteriores. Assim, mesmo sem ter todas as frases no nosso banco de dados, conseguimos estimar a chance de uma sequ√™ncia de palavras acontecer. No entanto, como calcular cada probabilidade condicional (e.g. $Pr(P_2|P_1)$)?

### A Suposi√ß√£o de Markov

A intui√ß√£o por tr√°s do modelo *N-gram* √© de que, ao inv√©s de computar a $Pr(p|h)$, podemos aproximar o hist√≥rico $h$ s√≥ com as √∫ltimas palavras.  O modelo de bigrama, por exemplo, aproxima a probabilidade de uma palavra dada todas as palavras anteriores $Pr(p_n|p_{1:n-1})$ usando a probabilidade condicional da palavra anterior $Pr(p_n|p_{n-1})$. Ou seja, no lugar de estimar


$$
Pr(Azul | \text{A praia de Copacabana √© t√£o})
$$


Ele aproxima $h$ por meio da probabilidade:

$$
Pr(Azul | t√£o)
$$

De maneira geral, a seguinte aproxima√ß√£o √© feita

$$
Pr(p_n|p_{1:n-1}) \approx Pr(p_n|p_{n-1})
$$

Esse pressuposto, ou suposi√ß√£o, de que a probabilidade de uma palavra depende apenas da palavra anteiror √© chamado de **Suposi√ß√£o de Markov**. Modelos de Markov s√£o uma classe de modelos probabil√≠sticos que assumem que podemos prever a probabilidade de uma unidade futura sem olhar muito distante no passado. Portanto, a probabilidade de uma frase inteira pode ser estimada por

$$
Pr(p_{1:n}) \approx \prod_{k=1}^{n} Pr(p_k|p_{k-1})
$$

### Como estimar as probabilidades?

Uma forma de estimar essas probabilidades que utilizam a suposi√ß√£o de Markov √© chamada de *Maximum Likelihood Estimation*, ou **M√©todo de Estima√ß√£o de M√°xima Verossimilhan√ßa**. Em texto, conseguiremos as estimativas em um modelo *n-gram* pegando contagens de um c√≥rpus, e normalizando[^3] essas contagens para que fiquem entre 0 e 1. Por exemplo, para computar a probabilidade de um bigrama de uma palavra $p_n$ dada uma palavra anterior $p_{n-1}$, se computa a contagem de um bigrama $C(p_{n-1} p_{n})$ e normalizar essa contagem pela soma de todo os bigramas que compartilham a primeira palavra $p_{n-1}$:


$$
P\bigl(p_n \,\bigl|\, p_{n-1}\bigr)
   \;=\;
   \frac{C\!\bigl(p_{n-1}p_n\bigr)}
        {\displaystyle\sum_{p} C\!\bigl(p_{n-1}p\bigr)}
$$

Vamos trabalhar alguns exemplos de c√°lculos de probabilidade s√≥ para entender como funciona (<f> e </f> indicam o come√ßo e o fim de uma frase):
 
$$
<f> \text{Eu sou Jo√£o} </f>
$$

$$
<f> \text{Jo√£o sou eu} </f>
$$

$$
<f> \text{Eu n√£o gosto de sopa} </f>
$$


Se esse fosse nosso c√≥rpus inteiro, poder√≠amos computar as seguinte probabilidades para os bigramas:

$$
Pr (Eu | <f>) = 2/3 \approx 67%
$$

Aqui, queremos a probabilidade de que a frase come√ße com "Eu". Em dois casos (1 e 3 frases), isso ocorre. Obtemos, ent√£o, uma probabilidade de aproximadamente 67%. Para a frase abaixo

$$
Pr (Jo√£o | <f>) = 1/3
$$

Queremos a probabilidade de que a frase come√ße com "Jo√£o". S√≥ uma frase das tr√™s come√ßa, portanto a probabilidade √© de 1/3, ou aproximadamente $33\%$. No caso geral, a estima√ß√£o de param√™tros no *MLE* (Maximum Likelihood Estimation) em n-grams fica:

$$
P\bigl(w_n \,\bigl|\, w_{\,n-N+1{:}n-1}\bigr)
   \;=\;
   \frac{C\!\bigl(w_{\,n-N+1{:}n-1}\,w_n\bigr)}
        {C\!\bigl(w_{\,n-N+1{:}n-1}\bigr)}
$$



## Notas

[^1]: O termo **n-gram** pode ser usado em dois sentidos: (1) como uma sequ√™ncia de n itens (palavras, caracteres, etc.) extra√≠da de um texto, e (2) como um modelo de linguagem que estima a probabilidade de uma palavra ou sequ√™ncia com base nas n-1 palavras anteriores. O contexto geralmente indica qual sentido est√° sendo utilizado.

[^2]: O termo **c√≥rpus** refere-se a um conjunto estruturado de textos ou documentos utilizados para an√°lise lingu√≠stica ou treinamento de modelos de linguagem. Em PLN, o c√≥rpus serve como fonte de dados para extrair padr√µes, calcular frequ√™ncias e estimar probabilidades, sendo fundamental para o desenvolvimento e avalia√ß√£o de m√©todos computacionais aplicados √† linguagem. Aqui, estamos indo para al√©m da ideia de um c√≥rpus anotado.

[^3]: **Normalizar** significa ajustar os valores de uma vari√°vel ou conjunto de dados para que fiquem dentro de um intervalo padr√£o, geralmente entre 0 e 1. No contexto de modelos de linguagem, normalizar as contagens transforma frequ√™ncias absolutas em probabilidades, facilitando a compara√ß√£o e o processamento estat√≠stico dos dados.