# M√°quinas de Suporte de Vetores

Na se√ß√£o anterior, trabalhamos o funcionamento do modelo de √°rvores de decis√£o. Agora, veremos o segundo modelo da aula, o modelo de **M√°quinas de Suporte de Vetores**, ou *Support Vector Machines*

## *SVM*

Os m√©todos baseados em √°rvores consistem em estratificar e segmentar o espa√ßo de preditores em um n√∫mero de regi√µes. Na aula de hoje veremos o primeiro m√©todo, que serve de base para os outros, as **√Årvores de Decis√£o** (*Decision Trees*)[^1]. Esses m√©todos fazem previs√µes para uma determinada observa√ß√£o usando o valor m√©dio, ou resposta modal (de moda), das observa√ß√µes de treinamento para a regi√£o a que ela pertence. M√©todos desse tipo possuem a principal vantagem de serem f√°ceis de interpretar, mas n√£o s√£o muito competitivos em termos de performance, especialmente em compara√ß√£o com o *deep learning*.

### √Årvores de Decis√£o



```{video} https://www.youtube.com/embed/_L39rN6gz7Y?si=wh_tmj_6hKx8GVtp
```

---


Vamos come√ßar primeiro  com um exemplo de √°rvore de decis√£o no contexto da regress√£o. Ou seja, no contexto de um *outcome* num√©rico.


```{figure} ../aula10/images/islfig8.3.1.png
---
width: 100%
name: dtreg
align: center
---
Ilustra√ß√£o da √Årvore de Decis√£o no contexto de Regress√£o. Fonte: James et al. ({cite}`james2023introduction`., p. 335)
```

A {numref}`Figura {number} <dtreg>` mostra o processo decis√≥rio em um modelo de √°rvore de decis√£o. Na figura, temos duas vari√°veis preditoras, $X_1$ e $X_2$, e a √°rvore vai se dividindo de acordo com os valores das duas. No come√ßo da √°rvore, tamb√©m conhecido como **N√≥ raiz** ou s√≥ **raiz**, a primeira decis√£o √© com base no corte $t_1$: Valores menores que $t_1$ em $x_1$ jogam as observa√ß√µes para o lado esquerdo da √°rvore, e valores maiores v√£o para o lado direito. Do lado esquerdo da figura, a segunda decis√£o vem com base em $X_2$, com valores menores que o ponto de corte $t_2$ caindo para a primeira regi√£o $R_1$, e valores maiores que $t_2$ caindo na segunda regi√£o $R_2$. Cada observa√ß√£o vai passar por esses n√≥s decis√≥rios, chegando nos n√≥s terminais que v√£o dar a previs√£o final ($R_1$, $R_2$, etc.). No caso das √°rvores de regress√£o, o valor previsto ser√° a m√©dia das observa√ß√µes dentro dessa regi√£o.



```{figure} ../aula10/images/islfig8.3.2.png
---
width: 100%
name: dtregvar
align: center
---
Ilustra√ß√£o da √Årvore de Decis√£o no contexto de Regress√£o. Fonte: James et al. ({cite}`james2023introduction`., p. 335)
```

A {numref}`Figura {number} <dtregvar>` mostra como fica a divis√£o das observa√ß√µes com base nas regi√µes, dentro do espa√ßo de preditores. Com essa ilustra√ß√£o, fica mais f√°cil de ver como cada observa√ß√£o vai ser categorizada, e qual valor predito ser√° utilizado. No entanto, quando temos mais vari√°veis, essa divis√£o n√£o √© t√£o clara assim. No contexto pr√°tico da aplica√ß√£o do modelo em Python, voc√™ veria ele assim:


```{figure} ../aula10/images/geronfig6.4.png
---
width: 100%
name: dtgeronreg
align: center
---
√Årvore de Regress√£o no Python. Fonte: G√©ron ({cite}`geron2022hands`.)
```

A {numref}`Figura {number} <dtgeronreg>` ilustra uma √°rvore de decis√£o de regress√£o que prediz valores num√©ricos cont√≠nuos, demonstrando na pr√°tica o processo de divis√£o bin√°ria recursiva baseado na minimiza√ß√£o do RSS. A estrutura come√ßa com um n√≥ raiz no topo contendo todas as 200 amostras e se ramifica hierarquicamente atrav√©s de n√≥s de decis√£o internos que testam a vari√°vel $x1$ em diferentes pontos de corte, at√© chegar aos n√≥s folha (terminais) coloridos que apresentam as predi√ß√µes finais ‚Äî valores num√©ricos indicados por "value". Em cada n√≥, o MSE (Mean Squared Error) mede o erro quadr√°tico m√©dio naquela regi√£o, e quanto menor esse valor, mais homog√™neos s√£o os dados ‚Äî observe como os n√≥s folha apresentam MSE menores que os n√≥s internos, indicando regi√µes mais puras. O algoritmo selecionou recursivamente em cada etapa o ponto de corte da vari√°vel $x1$ que mais reduziu o RSS/MSE, construindo uma estrutura hier√°rquica que particiona o espa√ßo de preditores em regi√µes retangulares onde a predi√ß√£o √© simplesmente a m√©dia dos valores observados naquela regi√£o.

### Como a √°rvore √© constru√≠da?

As previs√µes da √°rvore de decis√£o s√£o feitas com base na estratifica√ß√£o do espa√ßo de preditores. Esse processo √© dividido em dois passos:

1. O espa√ßo de preditores (valores poss√≠veis de $X_1, X_2, ..., X_n$) √© divido em $J$ regi√µes distintas e sem sobreposi√ß√£o, chamadas de $R_j$. 

2. Para cada observa√ß√£o que cai na regi√£o $R_j$ √© feita a mesma previs√£o, que √© a m√©dia dos valores de treinamento que caem naquela determinada regi√£o. No contexto de classifica√ß√£o, √© a moda dos valores de treinamento.

### Como as regi√µes $R_j$ s√£o constru√≠das?

As regi√µes $R_1,R_2,...,R_j$ n√£o precisam ser retangulares como ilustrados na figura. O objetivo principal do modelo √© encontrar as regi√µes $R_j$ que ir√£o minimizar o erro das previs√µes, ou minimizar a fun√ß√£o custo. Na quarta aula do curso, vimos que uma forma geral de ilustrar a fun√ß√£o custo era por meio do seguinte mapeamento:


$$
L : (y, \hat{y}) \;\longrightarrow\; \mathbb{R}_{\ge 0}
$$

Vimos tamb√©m que a fun√ß√£o custo da regress√£o linear era o *Mean Squared Error*, ou Erro Quadr√°tico M√©dio:


$$
MSE (X) = E[(y- f(x)¬≤| x)]
$$

Para as √°rvores de decis√£o, usamos o **RSS** (*Residual Sum of Squares*, ou Soma dos Quadrados dos Res√≠duos). De maneira geral, ele √© calculado da seguinte forma:


$$
\text{RSS} = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2
$$

Onde:

- $y_i$ √© o valor observado/real da i-√©sima observa√ß√£o
- $\hat{y}_i$ √© o valor previsto pelo modelo para a i-√©sima observa√ß√£o
- $n$ √© o n√∫mero total de observa√ß√µes

Mas adaptado para as regi√µes da √°rvore de decis√£o:

$$
\text{RSS} = \sum_{j=1}^{J}\sum_{i \in R_j}(y_i - \hat{y}_{R_j})^2
$$

Onde:

- $J$ √© o n√∫mero de regi√µes/caixas na √°rvore
- $R_j$ √© a j-√©sima regi√£o
- $\hat{y}_{R_j}$ √© a predi√ß√£o para a regi√£o $R_j$ (m√©dia dos valores naquela regi√£o)
- $i \in R_j$ indica todas as observa√ß√µes que pertencem √† regi√£o $R_j$

Essa √∫ltima equa√ß√£o soma todos os erros de previs√£o ao quadrado em todas as regi√µes criadas pela √°rvore. O somat√≥rio externo $\sum_{j=1}^{J}$ percorre cada uma das $J$ regi√µes em que o espa√ßo de preditores est√° dividido, e o somat√≥rio interno $\sum_{i \in R_j}$ percorrre cada observa√ß√£o dentro das caixas $R_j$. Ou seja, para cada observa√ß√£o √© calculada a diferen√ßa entre o valor real $y_i$ e o valor predito $\hat{y}$, ou $f(x)$, dentro daquela regi√£o. Essa diferen√ßa √© ent√£o elevada ao quadrado, e depois soma o erro de todas as regi√µes, gerando um valor final do *RSS*. Quanto menor esse valor, melhor o resultado de treinamento do modelo.

### Divis√£o Bin√°ria Recursiva

No entanto, o c√°lculo do RSS dessa forma √© computacionalmente inexequ√≠vel, pois exigiria testar todas as parti√ß√µes poss√≠veis em $J$ regi√µes. Portanto, as regi√µes s√£o constru√≠das usando a **Divis√£o Bin√°ria Recursiva**, ou *Recursive Binary Splitting*. Esse algoritmo √© *top-down* e *greedy* (guloso). Ele √© *top-down* pois come√ßa pelo topo da √°rvore (raiz da √°rvore) e sucessivamente divide o espa√ßo de preditores. Cada divis√£o separa a √°rvore em dois novos galhos. O processo √© *greedy* por que a "melhor" divis√£o √© decidida dentro daquele etapa (ou *split*), e n√£o com base no passo anterior ou no pr√≥ximo passo. Ou seja, em cada etapa, o algoritmo testa todos os preditores poss√≠veis $X_i$, e todos os pontos de cortes $t$, criando duas regi√µes: uma onde $x_i < t$ e outra onde $x_i >= t$. O algoritmo ent√£o escolhe a combina√ß√£o que gera o menor RSS naquela etapa (por isso *greedy*) e parte para a pr√≥xima divis√£o. 

### Podando a √Årvore

Deixar a √°rvore crescer sem controle pode gerar boas previs√µes no banco de treinamento, mas pode gerar *overfitting* e baixa capacidade de generaliza√ß√£o para outros dados. Para evitar esse problema, podemos aumentar o vi√©s do modelo, reduzindo sua vari√¢ncia, ao limitar o crescimento da √°rvore, o que ira melhorar sua capacidade de generaliza√ß√£o. Para fazer isso, uma estrat√©gia √© deixar a √°rvore crescer primeiro e depois ir podando ela, gerando sub-√°rvores. A pergunta passa a ser ent√£o "Como escolher a melhor sub-√°rvore"?

#### *Cost-Complexity Pruning*

*Cost-Complexity Pruning*, ou **Poda de Custo-Complexidade**, √© uma das maneiras de escolher a melhor sub-√°rvore, reduzindo a vari√¢ncia do modelo. A f√≥rmula do **Cost-Complexity Pruning (Poda de Custo-Complexidade)** √© a seguinte:


$$
R_\alpha(T) = R(T) + \alpha|T|
$$

Onde:

- $R_\alpha(T)$ √© a medida de custo-complexidade da √°rvore $T$
- $R(T)$ √© o erro total da √°rvore (RSS para regress√£o ou impureza total para classifica√ß√£o)
- $\alpha \geq 0$ √© o par√¢metro de complexidade (hiperpar√¢metro de penaliza√ß√£o)
- $|T|$ √© o n√∫mero de n√≥s terminais (folhas) da √°rvore

Expandindo para incluir as regi√µes, temos:

$$
R_\alpha(T) = \sum_{m=1}^{|T|}\sum_{i \in R_m}(y_i - \hat{y}_{R_m})^2 + \alpha|T|
$$

A equal√£o √© a fun√ß√£o de custo-complexidade que balanceia o erro de previs√£o da √°rvore (o RSS ou impureza do n√≥) com sua complexidade. A primeira parte da equa√ß√£o √© exatamente o RSS que vimos antes. A segunda parte $\alpha|T|$ √© a penalidade de complexidade, onde o $|T|$ conta quantas folhas a √°rvore tem e $\alpha$ √© um par√¢metro de ajuste que controla o quanto voc√™ quer penalizar √°rvores grandes. Quando $\alpha = 0$, a equa√ß√£o √© igual ao RSS, e n√£o h√° nenhuma penalidade para √°rvores grandes demais.

**Interpreta√ß√£o:**

- Quando $\alpha = 0$, a f√≥rmula se reduz apenas ao RSS e voc√™ mant√©m a √°rvore completa
- Quando $\alpha$ aumenta, a penalidade por ter muitos n√≥s terminais cresce, for√ßando uma √°rvore mais simples (podada)
- O objetivo √© encontrar o valor de $\alpha$ que minimiza $R_\alpha(T)$, balanceando erro de predi√ß√£o e complexidade do modelo

### √Årvores de classifica√ß√£o

As √°rvores de classifica√ß√£o aplicam a mesma estrat√©gia das √°rvores de regress√£o ‚Äî particionar recursivamente o espa√ßo de preditores atrav√©s da **divis√£o bin√°ria recursiva** ‚Äî mas com o objetivo de separar observa√ß√µes em classes discretas. Enquanto as √°rvores de regress√£o utilizam o RSS (Residual Sum of Squares) como crit√©rio de divis√£o, as √°rvores de classifica√ß√£o empregam **medidas de impureza** para avaliar a qualidade das separa√ß√µes.


```{figure} ../aula10/images/islfig8.6.png
---
width: 100%
name: dtclass
align: center
---
Ilustra√ß√£o da √Årvore de Classifica√ß√£o. Fonte: James et al. ({cite}`james2023introduction`., p. 340)
```

A {numref}`Figura {number} <dtclass>` ilustra uma √°rvore de decis√£o de classifica√ß√£o bin√°ria que prediz a presen√ßa ou aus√™ncia de doen√ßa card√≠aca (classes "Yes" e "No"), demonstrando na pr√°tica o processo de divis√£o bin√°ria recursiva que discutimos anteriormente. A estrutura come√ßa com um n√≥ raiz no topo que avalia uma caracter√≠stica cl√≠nica (Thal:a) e se ramifica hierarquicamente em n√≥s de decis√£o internos que testam outras vari√°veis m√©dicas, como frequ√™ncia card√≠aca m√°xima (MaxHR) e tipo de dor no peito (ChestPain), at√© chegar aos n√≥s folha (terminais) coloridos que apresentam as classifica√ß√µes finais. Cada n√≥ interno mostra estimativas de probabilidade das classes (indicadas por "Ca + 0.5"), refletindo a propor√ß√£o de observa√ß√µes de cada categoria naquela regi√£o. O algoritmo guloso e top-down selecionou em cada etapa a vari√°vel e o ponto de corte que mais reduziram a impureza.

Em cada **n√≥ terminal** (ou n√≥ folha), a previs√£o pode ser feita de duas formas: (1) atribuindo a **classe modal** (mais frequente) entre as observa√ß√µes da folha; ou (2) fornecendo uma **estimativa de probabilidade** para cada classe, baseada nas propor√ß√µes observadas na regi√£o.

**Estimativa de probabilidade na folha:**

$$
\hat{p}_k = \frac{n_k}{n}
$$

onde $n_k$ √© o n√∫mero de observa√ß√µes da classe $k$ na folha e $n$ √© o total de observa√ß√µes na folha.

#### Crit√©rios de Impureza

A **impureza** mede o grau de mistura de classes em um n√≥: um n√≥ **puro** (impureza zero) cont√©m apenas observa√ß√µes de uma √∫nica classe, enquanto um n√≥ **impuro** cont√©m uma mistura de classes. O algoritmo busca divis√µes que reduzem a impureza, tornando os n√≥s filhos mais homog√™neos. As tr√™s principais m√©tricas s√£o:

**√çndice de Gini** (mais utilizado):

$$
G = 1 - \sum_{k} \hat{p}_k^2
$$

Mede a probabilidade de classifica√ß√£o incorreta aleat√≥ria. Varia de 0 (puro) a aproximadamente 0.5 (m√°xima impureza em problemas bin√°rios).

**Entropia** (baseada na teoria da informa√ß√£o):

$$
H = -\sum_{k} \hat{p}_k \log(\hat{p}_k)
$$

Quantifica a incerteza ou desordem no n√≥. Quanto maior a entropia, maior a mistura de classes.

**Erro de classifica√ß√£o** (misclassification error):

$$
E = 1 - \max_k \hat{p}_k
$$

Propor√ß√£o de observa√ß√µes que n√£o pertencem √† classe majorit√°ria. √â menos sens√≠vel a mudan√ßas na distribui√ß√£o das classes.

Ao avaliar cada poss√≠vel divis√£o durante a **divis√£o bin√°ria recursiva**, a √°rvore calcula a **impureza ponderada** das duas regi√µes geradas (esquerda e direita) e escolhe o split que **mais reduz** a impureza m√©dia :

$$
I_{\text{split}} = \frac{n_{L}}{n} I(L) + \frac{n_{R}}{n} I(R)
$$

onde $I(\cdot)$ √© a medida de impureza escolhida (Gini, Entropia ou Erro), $n_L$ e $n_R$ s√£o os tamanhos das parti√ß√µes esquerda e direita, e $n$ √© o total de observa√ß√µes antes da divis√£o. O algoritmo testa todos os preditores e todos os pontos de corte poss√≠veis, selecionando aquele que minimiza $I_{\text{split}}$ em cada etapa, de forma gulosa e top-down.


```{figure} ../aula10/images/geronfig6.1.png
---
width: 100%
name: dtgeron
align: center
---
√Årvore de decis√£o no Python. Fonte: G√©ron ({cite}`geron2022hands`.)
```

A {numref}`Figura {number} <dtgeron>` ilustra uma √°rvore de decis√£o de classifica√ß√£o treinada no conjunto de dados Iris, que classifica flores em tr√™s esp√©cies (setosa, versicolor e virginica) atrav√©s de uma estrutura hier√°rquica de decis√µes. O n√≥ raiz no topo avalia se o comprimento da p√©tala √© menor ou igual a 2.45 cm e, quando verdadeiro, leva diretamente a um n√≥ folha laranja completamente puro (Gini = 0.0) contendo todas as 50 amostras da classe setosa, enquanto o ramo falso conduz a um segundo n√≥ de divis√£o que avalia a largura da p√©tala (‚â§ 1.75 cm) para separar as 100 amostras restantes. Esse segundo n√≥ de divis√£o gera dois n√≥s folha: um verde classificando 54 amostras como versicolor (com pequena impureza de Gini = 0.168 devido a 5 virginicas misturadas) e um roxo classificando 46 amostras como virginica (com Gini = 0.043, quase puro exceto por 1 versicolor). A estrutura demonstra como a √°rvore utiliza apenas duas caracter√≠sticas (comprimento e largura da p√©tala) e dois pontos de corte para separar eficientemente as tr√™s classes, com os valores de Gini em cada n√≥ indicando a pureza da classifica√ß√£o e o n√∫mero de amostras mostrando a distribui√ß√£o dos dados em cada divis√£o.



```{admonition} üí¨ Com a palavra, os autores:
:class: quote
"Ent√£o, voc√™ deveria usar impureza Gini ou entropia? A verdade √© que, na maioria das vezes, isso n√£o faz uma grande diferen√ßa: eles levam a √°rvores semelhantes. A impureza Gini √© ligeiramente mais r√°pida de calcular, ent√£o √© uma boa escolha padr√£o. No entanto, quando eles diferem, a impureza Gini tende a isolar a classe mais frequente em seu pr√≥prio ramo da √°rvore, enquanto a entropia tende a produzir √°rvores ligeiramente mais balanceadas."
({cite}`geron2022hands`., Cap√≠tulo 6, tradu√ß√£o nossa)
```

## Conclus√£o


Nesta aula exploramos as √°rvores de decis√£o, um dos m√©todos fundamentais de machine learning que serve de base para algoritmos mais sofisticados como Random Forests e Gradient Boosting. Aprendemos que tanto √°rvores de regress√£o quanto de classifica√ß√£o compartilham a mesma estrat√©gia central: particionar recursivamente o espa√ßo de preditores em regi√µes distintas atrav√©s do algoritmo guloso e top-down de divis√£o bin√°ria recursiva, onde cada divis√£o busca localmente a melhor separa√ß√£o dos dados sem considerar o impacto global. Vimos que as √°rvores de regress√£o minimizam o RSS (Residual Sum of Squares) para encontrar as melhores divis√µes, enquanto as √°rvores de classifica√ß√£o utilizam medidas de impureza como o √≠ndice Gini ou entropia para avaliar a qualidade das separa√ß√µes, buscando criar n√≥s filhos mais homog√™neos e puros. Um conceito crucial que abordamos foi a poda de custo-complexidade (Cost-Complexity Pruning), que introduz o hiperpar√¢metro 
Œ±
Œ± para balancear erro de predi√ß√£o e complexidade do modelo, conectando-se diretamente com os conceitos de ajuste de hiperpar√¢metros da aula anterior e demonstrando como controlar o trade-off entre vi√©s e vari√¢ncia para evitar overfitting. A principal vantagem das √°rvores de decis√£o √© sua excepcional interpretabilidade: a estrutura hier√°rquica de regras "se-ent√£o" permite que profissionais de diversas √°reas compreendam facilmente como o modelo toma decis√µes, tornando-as ideais para contextos onde explicabilidade √© crucial, como diagn√≥sticos m√©dicos ou decis√µes de cr√©dito. No entanto, como mencionado, √°rvores individuais geralmente n√£o s√£o t√£o competitivas em termos de performance pura quando comparadas a m√©todos mais modernos, tendendo a ter alta vari√¢ncia e sendo sens√≠veis a pequenas mudan√ßas nos dados de treinamento. Na pr√≥xima se√ß√£o, exploraremos o Support Vector Machine (SVM), um algoritmo com abordagem completamente diferente que busca maximizar margens de separa√ß√£o entre classes, e introduziremos o TF-IDF, uma t√©cnica de pondera√ß√£o de texto que ser√° essencial para aplicar SVMs em problemas de classifica√ß√£o textual.


## Notas

[^1]: Outros m√©todos baseados em √°rvores incluem: Random Forests ‚Äî ensembles de √°rvores constru√≠das por amostragem bootstrap que reduzem a vari√¢ncia; Bagging (Bootstrap Aggregating) ‚Äî agrega√ß√£o de v√°rias √°rvores independentes; Extra-Trees (Extremely Randomized Trees) ‚Äî similar a Random Forest com divis√£o mais aleat√≥ria; Boosting ‚Äî m√©todos sequenciais que corrigem erros (ex.: AdaBoost); Gradient Boosting Machines (GBM) ‚Äî otimiza√ß√£o por gradiente de √°rvores fracas; implementa√ß√µes populares e otimizadas: XGBoost, LightGBM e CatBoost; Isolation Forest ‚Äî uso de √°rvores para detec√ß√£o de anomalias; e abordagens mais especializadas como Conditional Inference Trees e Bayesian Additive Regression Trees (BART). Cada fam√≠lia tem trade-offs distintos entre vi√©s, vari√¢ncia, interpretabilidade e velocidade.

