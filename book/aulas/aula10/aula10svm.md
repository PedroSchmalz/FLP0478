# M√°quinas de Vetores de Suporte

Na se√ß√£o anterior, trabalhamos o funcionamento do modelo de √°rvores de decis√£o. Agora, veremos o segundo modelo da aula, o modelo de **M√°quinas de Vetores de Suporte**, ou *Support Vector Machines*


```{video} https://www.youtube.com/embed/efR1C6CvhmE?si=u_uVRbYz4LlsV6PG
```

---

O *Support Vector Machine*, ou M√°quina de Vetores de Suporte, √© um modelo que surge como generaliza√ß√£o de um classificador mais simples, o *maximal margin classifier* (Classificador de margem m√°xima). Este classificador exigia que houvesse uma separa√ß√£o por meio de um limite linear no espa√ßo de preditores, o que n√£o √© poss√≠vel em muitas situa√ß√µes. Por isso, novos classificadores com base nele surgiram, at√© chegarmos no *SVM* de fato. Vamos primeiro entender o que √© o Classificador de Margem M√°xima e como ele funciona.


## Classificador de Margem M√°xima

Para entender esse classificador, primeiro precisamos entender o que √© um hiperplano.

### O hiperplano

Em um espa√ßo com $p$ dimens√µes, um hiperplano √© um subespa√ßo afim plano de dimens√£o $p-1$. Por exemplo, em duas dimens√µes o hiperplano √© um subespa√ßo plano unidimensional. Ou seja, uma reta. Em tr√™s dimens√µes, um hiperplano √© um subespa√ßo plano bidimensional, um plano. Em $p >3$, a coisa se complica um pouco e fica mais dif√≠cil de visualizar, mas a ideia se mant√©m. Podemos pensar no hiperplano como dividindo um espa√ßo de dimens√£o $p$ em duas metades.



```{figure} ../aula10/images/islfig9.1.png
---
width: 100%
name: hiperplano
align: center
---
Um hiperplano dividindo o espa√ßo dos preditores $X_1$ e $X_2$ na metade. Fonte: James et al. ({cite}`james2023introduction`., p. 369)
```

A {numref}`Figura {number} <hiperplano>` mostra o hiperplano de equa√ß√£o $ 1 + 2X_1 + 3X_2 = 0$. Valores em que essa equa√ß√£o s√£o maiores que zero est√£o coloridos de azul, e valores menores que zero est√£o em roxo/rosa. Como estamos em um espa√ßo de dimens√£o $p=2$, o hiperplano √© uma reta. 



### Classifica√ß√£o usando um hiperplano

√â poss√≠vel pensar a constru√ß√£o de um hiperplano que separa as observa√ß√µes de treinamento de acordo com suas classes. Como visto na figura acima, j√° temos, s√≥ com o hiperplano, a separa√ß√£o entre observa√ß√µes "azuis" e observa√ß√µes "roxas". 

```{figure} ../aula10/images/islfig9.2.1.png
---
width: 100%
name: hiperplano2
align: center
---
Poss√≠veis hiperplanos dividindo o espa√ßo dos preditores $X_1$ e $X_2$ na metade. Fonte: James et al. ({cite}`james2023introduction`., p. 370)
```

A {numref}`Figura {number} <hiperplano2>` mostra algumas retas (hiperplanos) poss√≠veis na separa√ß√£o do espa√ßo de preditores em duas metades. Se existe um hiperplano (nem sempre existe), podemos us√°-lo como um classificador "natural": A observa√ß√£o de teste ser√° classificada com base em qual lado do hiperplano ela est√°, como mostra a figura abaixo:

```{figure} ../aula10/images/islfig9.2.2.png
---
width: 100%
name: hiperplano3
align: center
---
Hiperplano dividindo as observa√ß√µes entre roxos e azuis. Fonte: James et al. ({cite}`james2023introduction`., p. 370)
```


### Qual o melhor hiperplano?

Nem sempre existe um hiperplano que separa as observa√ß√µes. Quando existe, tem mais de uma possibilidade. Precisamos, ent√£o, decidir qual hiperplano iremos utilizar para classificar as observa√ß√µes. A escolha mais natural (segundo os autores) √© a do **hiperplano de margem m√°xima**, ou o **hiperplano √≥timo de separa√ß√£o**, que √© o hiperplano mais distante das observa√ß√µes de treinamento em ambas as classes. Ou seja, podemos calcular a dist√¢ncia perpendicular de cada observa√ß√£o de treinamento at√© um dado hiperplano de separa√ß√£o: a menor dessas dist√¢ncias √© a dist√¢ncia m√≠nima das observa√ß√µes at√© o hiperplano; e √© conhecida como **margem**. O hiperplano de margem m√°xima √© o hiperplano de separa√ß√£o para o qual a margem √© a maior poss√≠vel. Isto √©, o hiperplano que ter a maior dist√¢ncia m√≠nima das observa√ß√µes de treinamento.

```{figure} ../aula10/images/islfig9.3.png
---
width: 100%
name: hiperplano3
align: center
---
Hiperplano de margem m√°xima. Fonte: James et al. ({cite}`james2023introduction`., p. 371)
```

Olhando para a {numref}`Figura {number} <hiperplano3>`, vemos que o hiperplano tem uma margem delimitada pela linha pontilhada. Nesse caso, essa √© a maior margem poss√≠vel entre as observa√ß√µes de cada classe que foi encontrada com base nessas vari√°veis. Podemos ver tamb√©m que tr√™s observa√ß√µes de treinamento s√£o equidistantes do hiperplano (marcadas pelas setas). Essas observa√ß√µes s√£o conhecidas como os **vetores de suporte**, dado que s√£o vetores em um espa√ßo de $p$ dimens√µes e d√£o "suporte" ao hiperplano. Isto √©, se as observa√ß√µes mudassem, o hiperplano de margem m√°xima tamb√©m mudaria. Por isso, ele √© muito sens√≠vel √†s observa√ß√µes pr√≥ximas da divis√£o, e pouco/nada sens√≠vel √†s observa√ß√µes distantes.


## Classificadores de Vetores de Suporte (*SVC*)

Nem sempre √© poss√≠vel separar as observa√ß√µes com um hiperplano. E mesmo quando √© poss√≠vel, talvez n√£o seja desej√°vel usar limites t√£o r√≠gidos quanto os necess√°rios para a defini√ß√£o do hiperplano de margem m√°xima, que separa perfeitamente as observa√ß√µes entre as classes. Por isso, talvez seja √∫til relaxar essa restri√ß√£o, aumentando a robustez do modelo √†s observa√ß√µes dos vetores de suporte, e garantindo maior generaliza√ß√£o dos resultados para dados n√£o vistos. Essa √© a ideia por tr√°s dos **Classificadores de Vetores de Suporte**, ou *Support Vector Classifiers*, que usam uma margem suave, permitindo que algumas observa√ß√µes estejam dentro da margem, ou do lado contr√°rio dela.


```{figure} ../aula10/images/islfig9.6.png
---
width: 100%
name: hiperplano4
align: center
---
Classificadores de Vetores de Suporte de margem "suave". Fonte: James et al. ({cite}`james2023introduction`., p. 371)
```

Na {numref}`Figura {number} <hiperplano4>`, o hiperplano admite que algumas observa√ß√µes estejam dentro da margem. Al√©m disso, tamb√©m permite que algumas observa√ß√µes estejam na "arquibancada da torcida rival", como √© o caso das observa√ß√µes azuis 1 e 12, e a observa√ß√£o roxa de n√∫mero 11. Com isso, aumentamos um pouco o vi√©s do modelo, mas garantimos menor vari√¢ncia e menos varia√ß√£o com base nas observa√ß√µes de suporte. 

O qu√£o "suave" essa margem √©, √© definida com com base em um **Hiperpar√¢metro**, o $C$, que define quanta viola√ß√£o da margem ser√° tolerada. Quanto menor o C, menos vi√©s o modelo ter√° (menos flex√≠vel). Quanto maior o valor desse par√¢metro, maior a flexibilidade.


```{admonition} üí¨ Com a palavra, os autores:
:class: quote
"Agora consideramos o papel do par√¢metro de ajuste C. Em (9.15), C limita a soma dos Œµ·µ¢, e portanto determina o n√∫mero e a severidade das viola√ß√µes √† margem (e ao hiperplano) que toleraremos. Podemos pensar em C como um **or√ßamento** para a quantidade de viola√ß√£o da margem que pode ser cometida pelas n observa√ß√µes. Se C = 0, ent√£o n√£o h√° or√ßamento para viola√ß√µes √† margem, e deve ser o caso que Œµ‚ÇÅ = ¬∑¬∑¬∑ = Œµ‚Çô = 0, caso no qual (9.12)‚Äì(9.15) simplesmente se reduz ao problema de otimiza√ß√£o do hiperplano de margem m√°xima (9.9)‚Äì(9.11). (√â claro, um hiperplano de margem m√°xima existe apenas se as duas classes forem separ√°veis.) Para C > 0, n√£o mais que C observa√ß√µes podem estar no lado errado do hiperplano, porque se uma observa√ß√£o est√° no lado errado do hiperplano ent√£o Œµ·µ¢ > 1, e (9.15) requer que Œ£‚Åø·µ¢‚Çå‚ÇÅ Œµ·µ¢ ‚â§ C. √Ä medida que o or√ßamento C aumenta, nos tornamos mais tolerantes √†s viola√ß√µes da margem, e assim a margem ir√° alargar. Inversamente, √† medida que C diminui, nos tornamos menos tolerantes √†s viola√ß√µes da margem e assim a margem se estreita.
"
({cite}`james2023introduction`., p. 378, tradu√ß√£o nossa)
```

## M√°quinas de Vetores de Suporte (*SVM*)


```{video} https://www.youtube.com/embed/Toet3EiSFcM?si=Ef0hDB6E76GuRvoH
```

---





E quando a separa√ß√£o do espa√ßo de preditores n√£o √© linear? 


```{figure} ../aula10/images/islfig9.8.1.png
---
width: 100%
name: svm
align: center
---
Classificadores de Vetores de Suporte de margem "suave". Fonte: James et al. ({cite}`james2023introduction`., p. 371)
```

Na {numref}`Figura {number} <svm>`, n√£o √© poss√≠vel estabelecer um hiperplano que corta exatamente as classes em duas metades. Por isso, utilizamos as M√°quinas de Vetores de Suporte, ou *Support Vector Machines*, que lidam com a n√£o linearidade de forma autom√°tica, sem precisar saturar o modelo colocandos os polin√¥mios das vari√°veis preditoras (e.g. $X_1^2$, $X_1^3$, $X_1^4$, e assim por diante). O *SVM* √© uma extens√£o do *SVC* que resulta da satura√ß√£o do espa√ßo de preditores utilizando de *Kernels* para lidar com a n√£o linearidade.

O kernel (ou n√∫cleo) √© uma fun√ß√£o matem√°tica que permite ao SVM realizar um "truque" elegante: ao inv√©s de voc√™ manualmente criar todas as vari√°veis polinomiais poss√≠veis para capturar rela√ß√µes n√£o-lineares nos dados originais, o kernel automaticamente transforma os dados para um espa√ßo de dimens√£o superior onde eles se tornam linearmente separ√°veis. Imagine que voc√™ tem pontos distribu√≠dos em c√≠rculos conc√™ntricos em 2D ‚Äî imposs√≠veis de separar com uma linha reta. O kernel RBF (Radial Basis Function), por exemplo, "projeta" esses pontos para um espa√ßo 3D onde eles podem ser separados por um plano. O mais impressionante √© que essa transforma√ß√£o acontece de forma impl√≠cita: o algoritmo nunca calcula explicitamente as coordenadas no novo espa√ßo de alta dimens√£o, apenas calcula produtos internos atrav√©s da fun√ß√£o kernel, tornando o processo computacionalmente eficiente. Os kernels mais comuns s√£o o linear (para dados j√° separ√°veis), polinomial (para rela√ß√µes polinomiais), RBF/Gaussiano (para fronteiras complexas e curvas), e sigmoide (similar a redes neurais). A escolha do kernel e seus par√¢metros (como o grau do polin√¥mio ou o gamma do RBF) s√£o hiperpar√¢metros cruciais que devem ser ajustados usando t√©cnicas como grid search ou random search para otimizar a performance do modelo.

```{figure} ../aula10/images/islfig9.9.png
---
width: 100%
name: svmkernel
align: center
---
Esquerda: Um SVM com kernel polinomial de grau 3 √© aplicado aos dados n√£o lineares da Figura 9.8, resultando em uma regra de decis√£o muito mais apropriada. Direita: Um SVM com kernel radial √© aplicado. Neste exemplo, qualquer um dos kernels √© capaz de capturar a fronteira de decis√£o. Fonte: James et al. ({cite}`james2023introduction`., p. 371)
```

na {numref}`Figura {number} <svmkernel>` temos os mesmos dados da figura anterior. Na figura da esquerda, utiliza-se um kernel polinomial de grau 3 para ajustar melhor √†s observa√ß√µes, comportando a n√£o linearidade. Na figura da direita, tamb√©m lidamos com a n√£o linearidade, mas usando de um kernel radial. Lembre-se de que isso sempre cai no trade-off de flexibilidade: Modelos mais flex√≠veis s√£o mais propensos ao *overfitting*. A escolha do *kernel* em si se torna um hiperpar√¢metro, que deve ser escolhido com base em valida√ß√£o cruzada.

## Conclus√£o


Nesta se√ß√£o exploramos as M√°quinas de Vetores de Suporte (SVMs), um dos algoritmos mais robustos e vers√°teis do aprendizado de m√°quina, desenvolvido por Vladimir Vapnik e seus colegas na d√©cada de 1990. Compreendemos que os SVMs surgem como uma generaliza√ß√£o progressiva de classificadores mais simples: come√ßando pelo Classificador de Margem M√°xima, que encontra o hiperplano que maximiza a dist√¢ncia (margem) entre as classes quando os dados s√£o perfeitamente separ√°veis linearmente, passando pelo Classificador de Vetores de Suporte (SVC), que introduz a margem "suave" atrav√©s do hiperpar√¢metro C para permitir viola√ß√µes e aumentar a robustez do modelo, at√© chegar finalmente ao SVM completo, que utiliza o kernel trick para lidar elegantemente com rela√ß√µes n√£o-lineares sem a necessidade de criar manualmente termos polinomiais. O conceito de vetores de suporte √© central nesse algoritmo: apenas os pontos de dados mais pr√≥ximos do hiperplano (os que "tocam" as fronteiras da margem) definem a solu√ß√£o, tornando o m√©todo eficiente em termos de mem√≥ria e computacionalmente elegante. O hiperpar√¢metro C funciona como um or√ßamento que controla o trade-off entre maximizar a margem e minimizar erros de classifica√ß√£o: valores pequenos de C resultam em margens mais largas mas mais tolerantes a erros (maior vi√©s, menor vari√¢ncia), enquanto valores grandes de C buscam classificar corretamente o m√°ximo de pontos, estreitando a margem (menor vi√©s, maior vari√¢ncia). Os kernels ‚Äî linear, polinomial, RBF/Gaussiano e sigmoide ‚Äî s√£o fun√ß√µes que permitem ao SVM transformar implicitamente os dados para espa√ßos de dimens√£o superior onde se tornam linearmente separ√°veis, realizando essa proje√ß√£o de forma computacionalmente eficiente atrav√©s do c√°lculo de produtos internos, sem nunca calcular explicitamente as coordenadas no novo espa√ßo. 

As principais vantagens dos SVMs incluem sua efic√°cia em espa√ßos de alta dimensionalidade (especialmente quando o n√∫mero de features supera o n√∫mero de amostras), robustez contra overfitting devido √† maximiza√ß√£o da margem, flexibilidade proporcionada pelos diferentes kernels, e uso eficiente de mem√≥ria j√° que apenas os vetores de suporte s√£o necess√°rios para definir a solu√ß√£o. Por outro lado, as desvantagens envolvem a alta complexidade computacional para grandes conjuntos de dados (o treinamento pode ser intensivo em tempo e recursos), a necessidade de sele√ß√£o cuidadosa do kernel e dos hiperpar√¢metros (C e gamma para RBF) atrav√©s de valida√ß√£o cruzada, sensibilidade a dados desbalanceados, e menor interpretabilidade quando comparado a modelos como √°rvores de decis√£o ou regress√£o log√≠stica. 

Como discutido na conex√£o com a aula anterior sobre ajuste de hiperpar√¢metros, a escolha do kernel e o ajuste de seus par√¢metros s√£o etapas cruciais que devem ser realizadas atrav√©s de t√©cnicas como grid search, random search ou otimiza√ß√£o bayesiana, sempre utilizando valida√ß√£o cruzada k-fold para garantir que o modelo generalize bem para dados n√£o vistos. Na pr√≥xima se√ß√£o, introduziremos o TF-IDF (Term Frequency-Inverse Document Frequency), uma t√©cnica de pondera√ß√£o de texto essencial para aplicar SVMs em problemas de classifica√ß√£o textual, criando representa√ß√µes num√©ricas de documentos que capturam a import√¢ncia relativa das palavras.
